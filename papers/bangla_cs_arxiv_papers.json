{"id": "http://arxiv.org/abs/1204.1198v1", "guidislink": true, "updated": "2012-04-05T12:28:11Z", "updated_parsed": [2012, 4, 5, 12, 28, 11, 3, 96, 0], "published": "2012-04-05T12:28:11Z", "published_parsed": [2012, 4, 5, 12, 28, 11, 3, 96, 0], "title": "A Complete Workflow for Development of Bangla OCR", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=0&max_results=10&sortBy=relevance&sortOrder=descending", "value": "A Complete Workflow for Development of Bangla OCR"}, "summary": "Developing a Bangla OCR requires bunch of algorithm and methods. There were\nmany effort went on for developing a Bangla OCR. But all of them failed to\nprovide an error free Bangla OCR. Each of them has some lacking. We discussed\nabout the problem scope of currently existing Bangla OCR's. In this paper, we\npresent the basic steps required for developing a Bangla OCR and a complete\nworkflow for development of a Bangla OCR with mentioning all the possible\nalgorithms required.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=0&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Developing a Bangla OCR requires bunch of algorithm and methods. There were\nmany effort went on for developing a Bangla OCR. But all of them failed to\nprovide an error free Bangla OCR. Each of them has some lacking. We discussed\nabout the problem scope of currently existing Bangla OCR's. In this paper, we\npresent the basic steps required for developing a Bangla OCR and a complete\nworkflow for development of a Bangla OCR with mentioning all the possible\nalgorithms required."}, "authors": ["Farjana Yeasmin Omee", "Shiam Shabbir Himel", "Md. Abu Naser Bikas"], "author_detail": {"name": "Md. Abu Naser Bikas"}, "author": "Md. Abu Naser Bikas", "links": [{"href": "http://arxiv.org/abs/1204.1198v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1204.1198v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1204.1198v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1204.1198v1", "arxiv_comment": null, "journal_reference": "International Journal of Computer Applications, Volume 21, No.9,\n  May 2011", "doi": null}
{"id": "http://arxiv.org/abs/1009.4586v1", "guidislink": true, "updated": "2010-09-23T11:42:41Z", "updated_parsed": [2010, 9, 23, 11, 42, 41, 3, 266, 0], "published": "2010-09-23T11:42:41Z", "published_parsed": [2010, 9, 23, 11, 42, 41, 3, 266, 0], "title": "Optimal Bangla Keyboard Layout using Association Rule of Data Mining", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=0&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Optimal Bangla Keyboard Layout using Association Rule of Data Mining"}, "summary": "In this paper we present an optimal Bangla Keyboard Layout, which distributes\nthe load equally on both hands so that maximizing the ease and minimizing the\neffort. Bangla alphabet has a large number of letters, for this it is difficult\nto type faster using Bangla keyboard. Our proposed keyboard will maximize the\nspeed of operator as they can type with both hands parallel. Here we use the\nassociation rule of data mining to distribute the Bangla characters in the\nkeyboard. First, we analyze the frequencies of data consisting of monograph,\ndigraph and trigraph, which are derived from data wire-house, and then used\nassociation rule of data mining to distribute the Bangla characters in the\nlayout. Finally, we propose a Bangla Keyboard Layout. Experimental results on\nseveral keyboard layout shows the effectiveness of the proposed approach with\nbetter performance.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=0&max_results=10&sortBy=relevance&sortOrder=descending", "value": "In this paper we present an optimal Bangla Keyboard Layout, which distributes\nthe load equally on both hands so that maximizing the ease and minimizing the\neffort. Bangla alphabet has a large number of letters, for this it is difficult\nto type faster using Bangla keyboard. Our proposed keyboard will maximize the\nspeed of operator as they can type with both hands parallel. Here we use the\nassociation rule of data mining to distribute the Bangla characters in the\nkeyboard. First, we analyze the frequencies of data consisting of monograph,\ndigraph and trigraph, which are derived from data wire-house, and then used\nassociation rule of data mining to distribute the Bangla characters in the\nlayout. Finally, we propose a Bangla Keyboard Layout. Experimental results on\nseveral keyboard layout shows the effectiveness of the proposed approach with\nbetter performance."}, "authors": ["Md. Hijbul Alam", "Abdul Kadar Muhammad Masum", "Mohammad Mahadi Hassan", "S. M. Kamruzzaman"], "author_detail": {"name": "S. M. Kamruzzaman"}, "author": "S. M. Kamruzzaman", "arxiv_comment": "3 Pages, International Conference", "links": [{"href": "http://arxiv.org/abs/1009.4586v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1009.4586v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1009.4586v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1009.4586v1", "journal_reference": "Proc. 7th International Conference on Computer and Information\n  Technology (ICCIT 2004), Dhaka, Bangladesh, pp. 679-681, Dec. 2004", "doi": null}
{"id": "http://arxiv.org/abs/1703.10661v1", "guidislink": true, "updated": "2017-02-22T07:57:14Z", "updated_parsed": [2017, 2, 22, 7, 57, 14, 2, 53, 0], "published": "2017-02-22T07:57:14Z", "published_parsed": [2017, 2, 22, 7, 57, 14, 2, 53, 0], "title": "BanglaLekha-Isolated: A Comprehensive Bangla Handwritten Character\n  Dataset", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=0&max_results=10&sortBy=relevance&sortOrder=descending", "value": "BanglaLekha-Isolated: A Comprehensive Bangla Handwritten Character\n  Dataset"}, "summary": "Bangla handwriting recognition is becoming a very important issue nowadays.\nIt is potentially a very important task specially for Bangla speaking\npopulation of Bangladesh and West Bengal. By keeping that in our mind we are\nintroducing a comprehensive Bangla handwritten character dataset named\nBanglaLekha-Isolated. This dataset contains Bangla handwritten numerals, basic\ncharacters and compound characters. This dataset was collected from multiple\ngeographical location within Bangladesh and includes sample collected from a\nvariety of aged groups. This dataset can also be used for other classification\nproblems i.e: gender, age, district. This is the largest dataset on Bangla\nhandwritten characters yet.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=0&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Bangla handwriting recognition is becoming a very important issue nowadays.\nIt is potentially a very important task specially for Bangla speaking\npopulation of Bangladesh and West Bengal. By keeping that in our mind we are\nintroducing a comprehensive Bangla handwritten character dataset named\nBanglaLekha-Isolated. This dataset contains Bangla handwritten numerals, basic\ncharacters and compound characters. This dataset was collected from multiple\ngeographical location within Bangladesh and includes sample collected from a\nvariety of aged groups. This dataset can also be used for other classification\nproblems i.e: gender, age, district. This is the largest dataset on Bangla\nhandwritten characters yet."}, "authors": ["Mithun Biswas", "Rafiqul Islam", "Gautam Kumar Shom", "Md Shopon", "Nabeel Mohammed", "Sifat Momen", "Md Anowarul Abedin"], "author_detail": {"name": "Md Anowarul Abedin"}, "author": "Md Anowarul Abedin", "arxiv_comment": "Bangla Handwriting Dataset, OCR", "links": [{"href": "http://arxiv.org/abs/1703.10661v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1703.10661v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1703.10661v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1703.10661v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1701.07955v1", "guidislink": true, "updated": "2017-01-27T06:30:21Z", "updated_parsed": [2017, 1, 27, 6, 30, 21, 4, 27, 0], "published": "2017-01-27T06:30:21Z", "published_parsed": [2017, 1, 27, 6, 30, 21, 4, 27, 0], "title": "Statistical Analysis on Bangla Newspaper Data to Extract Trending Topic\n  and Visualize Its Change Over Time", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=0&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Statistical Analysis on Bangla Newspaper Data to Extract Trending Topic\n  and Visualize Its Change Over Time"}, "summary": "Trending topic of newspapers is an indicator to understand the situation of a\ncountry and also a way to evaluate the particular newspaper. This paper\nrepresents a model describing few techniques to select trending topics from\nBangla Newspaper. Topics that are discussed more frequently than other in\nBangla newspaper will be marked and how a very famous topic loses its\nimportance with the change of time and another topic takes its place will be\ndemonstrated. Data from two popular Bangla Newspaper with date and time were\ncollected. Statistical analysis was performed after on these data after\npreprocessing. Popular and most used keywords were extracted from the stream of\nBangla keyword with this analysis. This model can also cluster category wise\nnews trend or a list of news trend in daily or weekly basis with enough data. A\npattern can be found on their news trend too. Comparison among past news trend\nof Bangla newspapers will give a visualization of the situation of Bangladesh.\nThis visualization will be helpful to predict future trending topics of Bangla\nNewspaper.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=0&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Trending topic of newspapers is an indicator to understand the situation of a\ncountry and also a way to evaluate the particular newspaper. This paper\nrepresents a model describing few techniques to select trending topics from\nBangla Newspaper. Topics that are discussed more frequently than other in\nBangla newspaper will be marked and how a very famous topic loses its\nimportance with the change of time and another topic takes its place will be\ndemonstrated. Data from two popular Bangla Newspaper with date and time were\ncollected. Statistical analysis was performed after on these data after\npreprocessing. Popular and most used keywords were extracted from the stream of\nBangla keyword with this analysis. This model can also cluster category wise\nnews trend or a list of news trend in daily or weekly basis with enough data. A\npattern can be found on their news trend too. Comparison among past news trend\nof Bangla newspapers will give a visualization of the situation of Bangladesh.\nThis visualization will be helpful to predict future trending topics of Bangla\nNewspaper."}, "authors": ["Syed Mehedi Hasan Nirob", "Md. Kazi Nayeem", "Md. Saiful Islam"], "author_detail": {"name": "Md. Saiful Islam"}, "author": "Md. Saiful Islam", "arxiv_comment": "8 pages", "links": [{"href": "http://arxiv.org/abs/1701.07955v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1701.07955v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.IR", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.IR", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1701.07955v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1701.07955v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/2005.02155v2", "guidislink": true, "updated": "2020-05-06T07:59:45Z", "updated_parsed": [2020, 5, 6, 7, 59, 45, 2, 127, 0], "published": "2020-04-29T06:38:12Z", "published_parsed": [2020, 4, 29, 6, 38, 12, 2, 120, 0], "title": "MatriVasha: A Multipurpose Comprehensive Database for Bangla Handwritten\n  Compound Characters", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=0&max_results=10&sortBy=relevance&sortOrder=descending", "value": "MatriVasha: A Multipurpose Comprehensive Database for Bangla Handwritten\n  Compound Characters"}, "summary": "At present, recognition of the Bangla handwriting compound character has been\nan essential issue for many years. In recent years there have been\napplication-based researches in machine learning, and deep learning, which is\ngained interest, and most notably is handwriting recognition because it has a\ntremendous application such as Bangla OCR. MatrriVasha, the project which can\nrecognize Bangla, handwritten several compound characters. Currently, compound\ncharacter recognition is an important topic due to its variant application, and\nhelps to create old forms, and information digitization with reliability. But\nunfortunately, there is a lack of a comprehensive dataset that can categorize\nall types of Bangla compound characters. MatrriVasha is an attempt to align\ncompound character, and it's challenging because each person has a unique style\nof writing shapes. After all, MatrriVasha has proposed a dataset that intends\nto recognize Bangla 120(one hundred twenty) compound characters that consist of\n2552(two thousand five hundred fifty-two) isolated handwritten characters\nwritten unique writers which were collected from within Bangladesh. This\ndataset faced problems in terms of the district, age, and gender-based written\nrelated research because the samples were collected that includes a verity of\nthe district, age group, and the equal number of males, and females. As of now,\nour proposed dataset is so far the most extensive dataset for Bangla compound\ncharacters. It is intended to frame the acknowledgment technique for\nhandwritten Bangla compound character. In the future, this dataset will be made\npublicly available to help to widen the research.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=0&max_results=10&sortBy=relevance&sortOrder=descending", "value": "At present, recognition of the Bangla handwriting compound character has been\nan essential issue for many years. In recent years there have been\napplication-based researches in machine learning, and deep learning, which is\ngained interest, and most notably is handwriting recognition because it has a\ntremendous application such as Bangla OCR. MatrriVasha, the project which can\nrecognize Bangla, handwritten several compound characters. Currently, compound\ncharacter recognition is an important topic due to its variant application, and\nhelps to create old forms, and information digitization with reliability. But\nunfortunately, there is a lack of a comprehensive dataset that can categorize\nall types of Bangla compound characters. MatrriVasha is an attempt to align\ncompound character, and it's challenging because each person has a unique style\nof writing shapes. After all, MatrriVasha has proposed a dataset that intends\nto recognize Bangla 120(one hundred twenty) compound characters that consist of\n2552(two thousand five hundred fifty-two) isolated handwritten characters\nwritten unique writers which were collected from within Bangladesh. This\ndataset faced problems in terms of the district, age, and gender-based written\nrelated research because the samples were collected that includes a verity of\nthe district, age group, and the equal number of males, and females. As of now,\nour proposed dataset is so far the most extensive dataset for Bangla compound\ncharacters. It is intended to frame the acknowledgment technique for\nhandwritten Bangla compound character. In the future, this dataset will be made\npublicly available to help to widen the research."}, "authors": ["Jannatul Ferdous", "Suvrajit Karmaker", "A K M Shahariar Azad Rabby", "Syed Akhter Hossain"], "author_detail": {"name": "Syed Akhter Hossain"}, "author": "Syed Akhter Hossain", "arxiv_comment": "19 fig, 2 table", "links": [{"href": "http://arxiv.org/abs/2005.02155v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2005.02155v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2005.02155v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2005.02155v2", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1009.4982v1", "guidislink": true, "updated": "2010-09-25T06:55:27Z", "updated_parsed": [2010, 9, 25, 6, 55, 27, 5, 268, 0], "published": "2010-09-25T06:55:27Z", "published_parsed": [2010, 9, 25, 6, 55, 27, 5, 268, 0], "title": "Optimal Bangla Keyboard Layout using Data Mining Technique", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=0&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Optimal Bangla Keyboard Layout using Data Mining Technique"}, "summary": "This paper presents an optimal Bangla Keyboard Layout, which distributes the\nload equally on both hands so that maximizing the ease and minimizing the\neffort. Bangla alphabet has a large number of letters, for this it is difficult\nto type faster using Bangla keyboard. Our proposed keyboard will maximize the\nspeed of operator as they can type with both hands parallel. Here we use the\nassociation rule of data mining to distribute the Bangla characters in the\nkeyboard. First, we analyze the frequencies of data consisting of monograph,\ndigraph and trigraph, which are derived from data wire-house, and then used\nassociation rule of data mining to distribute the Bangla characters in the\nlayout. Experimental results on several data show the effectiveness of the\nproposed approach with better performance.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=0&max_results=10&sortBy=relevance&sortOrder=descending", "value": "This paper presents an optimal Bangla Keyboard Layout, which distributes the\nload equally on both hands so that maximizing the ease and minimizing the\neffort. Bangla alphabet has a large number of letters, for this it is difficult\nto type faster using Bangla keyboard. Our proposed keyboard will maximize the\nspeed of operator as they can type with both hands parallel. Here we use the\nassociation rule of data mining to distribute the Bangla characters in the\nkeyboard. First, we analyze the frequencies of data consisting of monograph,\ndigraph and trigraph, which are derived from data wire-house, and then used\nassociation rule of data mining to distribute the Bangla characters in the\nlayout. Experimental results on several data show the effectiveness of the\nproposed approach with better performance."}, "authors": ["S. M. Kamruzzaman", "Md. Hijbul Alam", "Abdul Kadar Muhammad Masum", "Md. Mahadi Hassan"], "author_detail": {"name": "Md. Mahadi Hassan"}, "author": "Md. Mahadi Hassan", "arxiv_comment": "9 Pages, International Conference", "links": [{"href": "http://arxiv.org/abs/1009.4982v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1009.4982v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1009.4982v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1009.4982v1", "journal_reference": "Proc. International Conference on Information and Communication\n  Technology in Management (ICTM 2005), Multimedia University, Malaysia, May\n  2005", "doi": null}
{"id": "http://arxiv.org/abs/1009.5048v1", "guidislink": true, "updated": "2010-09-26T02:09:41Z", "updated_parsed": [2010, 9, 26, 2, 9, 41, 6, 269, 0], "published": "2010-09-26T02:09:41Z", "published_parsed": [2010, 9, 26, 2, 9, 41, 6, 269, 0], "title": "The Most Advantageous Bangla Keyboard Layout Using Data Mining Technique", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=0&max_results=10&sortBy=relevance&sortOrder=descending", "value": "The Most Advantageous Bangla Keyboard Layout Using Data Mining Technique"}, "summary": "Bangla alphabet has a large number of letters, for this it is complicated to\ntype faster using Bangla keyboard. The proposed keyboard will maximize the\nspeed of operator as they can type with both hands parallel. Association rule\nof data mining to distribute the Bangla characters in the keyboard is used\nhere. The frequencies of data consisting of monograph, digraph and trigraph are\nanalyzed, which are derived from data wire-house, and then used association\nrule of data mining to distribute the Bangla characters in the layout.\nExperimental results on several data show the effectiveness of the proposed\napproach with better performance. This paper presents an optimal Bangla\nKeyboard Layout, which distributes the load equally on both hands so that\nmaximizing the ease and minimizing the effort.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=0&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Bangla alphabet has a large number of letters, for this it is complicated to\ntype faster using Bangla keyboard. The proposed keyboard will maximize the\nspeed of operator as they can type with both hands parallel. Association rule\nof data mining to distribute the Bangla characters in the keyboard is used\nhere. The frequencies of data consisting of monograph, digraph and trigraph are\nanalyzed, which are derived from data wire-house, and then used association\nrule of data mining to distribute the Bangla characters in the layout.\nExperimental results on several data show the effectiveness of the proposed\napproach with better performance. This paper presents an optimal Bangla\nKeyboard Layout, which distributes the load equally on both hands so that\nmaximizing the ease and minimizing the effort."}, "authors": ["Abdul Kadar Muhammad Masum", "Mohammad Mahadi Hassan", "S. M. Kamruzzaman"], "author_detail": {"name": "S. M. Kamruzzaman"}, "author": "S. M. Kamruzzaman", "arxiv_comment": "10 Pages, International Journal", "links": [{"href": "http://arxiv.org/abs/1009.5048v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1009.5048v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1009.5048v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1009.5048v1", "journal_reference": "Journal of Computer Science, IBAIS University, Dkhaka, Bangladesh,\n  Vol. 1, No. 2, Dec. 2007", "doi": null}
{"id": "http://arxiv.org/abs/1201.2010v1", "guidislink": true, "updated": "2012-01-10T10:33:18Z", "updated_parsed": [2012, 1, 10, 10, 33, 18, 1, 10, 0], "published": "2012-01-10T10:33:18Z", "published_parsed": [2012, 1, 10, 10, 33, 18, 1, 10, 0], "title": "Recognizing Bangla Grammar using Predictive Parser", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=0&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Recognizing Bangla Grammar using Predictive Parser"}, "summary": "We describe a Context Free Grammar (CFG) for Bangla language and hence we\npropose a Bangla parser based on the grammar. Our approach is very much general\nto apply in Bangla Sentences and the method is well accepted for parsing a\nlanguage of a grammar. The proposed parser is a predictive parser and we\nconstruct the parse table for recognizing Bangla grammar. Using the parse table\nwe recognize syntactical mistakes of Bangla sentences when there is no entry\nfor a terminal in the parse table. If a natural language can be successfully\nparsed then grammar checking from this language becomes possible. The proposed\nscheme is based on Top down parsing method and we have avoided the left\nrecursion of the CFG using the idea of left factoring.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=0&max_results=10&sortBy=relevance&sortOrder=descending", "value": "We describe a Context Free Grammar (CFG) for Bangla language and hence we\npropose a Bangla parser based on the grammar. Our approach is very much general\nto apply in Bangla Sentences and the method is well accepted for parsing a\nlanguage of a grammar. The proposed parser is a predictive parser and we\nconstruct the parse table for recognizing Bangla grammar. Using the parse table\nwe recognize syntactical mistakes of Bangla sentences when there is no entry\nfor a terminal in the parse table. If a natural language can be successfully\nparsed then grammar checking from this language becomes possible. The proposed\nscheme is based on Top down parsing method and we have avoided the left\nrecursion of the CFG using the idea of left factoring."}, "authors": ["K. M. Azharul Hasan", "Al-Mahmud", "Amit Mondal", "Amit Saha"], "author_detail": {"name": "Amit Saha"}, "author": "Amit Saha", "links": [{"title": "doi", "href": "http://dx.doi.org/10.5121/ijcsit.2011.3605", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/1201.2010v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1201.2010v1", "rel": "related", "type": "application/pdf"}], "arxiv_comment": "13 pages, 13 figures", "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1201.2010v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1201.2010v1", "journal_reference": null, "doi": "10.5121/ijcsit.2011.3605"}
{"id": "http://arxiv.org/abs/1203.0876v1", "guidislink": true, "updated": "2012-03-05T12:06:54Z", "updated_parsed": [2012, 3, 5, 12, 6, 54, 0, 65, 0], "published": "2012-03-05T12:06:54Z", "published_parsed": [2012, 3, 5, 12, 6, 54, 0, 65, 0], "title": "An MLP based Approach for Recognition of Handwritten `Bangla' Numerals", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=0&max_results=10&sortBy=relevance&sortOrder=descending", "value": "An MLP based Approach for Recognition of Handwritten `Bangla' Numerals"}, "summary": "The work presented here involves the design of a Multi Layer Perceptron (MLP)\nbased pattern classifier for recognition of handwritten Bangla digits using a\n76 element feature vector. Bangla is the second most popular script and\nlanguage in the Indian subcontinent and the fifth most popular language in the\nworld. The feature set developed for representing handwritten Bangla numerals\nhere includes 24 shadow features, 16 centroid features and 36 longest-run\nfeatures. On experimentation with a database of 6000 samples, the technique\nyields an average recognition rate of 96.67% evaluated after three-fold cross\nvalidation of results. It is useful for applications related to OCR of\nhandwritten Bangla Digit and can also be extended to include OCR of handwritten\ncharacters of Bangla alphabet.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=0&max_results=10&sortBy=relevance&sortOrder=descending", "value": "The work presented here involves the design of a Multi Layer Perceptron (MLP)\nbased pattern classifier for recognition of handwritten Bangla digits using a\n76 element feature vector. Bangla is the second most popular script and\nlanguage in the Indian subcontinent and the fifth most popular language in the\nworld. The feature set developed for representing handwritten Bangla numerals\nhere includes 24 shadow features, 16 centroid features and 36 longest-run\nfeatures. On experimentation with a database of 6000 samples, the technique\nyields an average recognition rate of 96.67% evaluated after three-fold cross\nvalidation of results. It is useful for applications related to OCR of\nhandwritten Bangla Digit and can also be extended to include OCR of handwritten\ncharacters of Bangla alphabet."}, "authors": ["Subhadip Basu", "Nibaran Das", "Ram Sarkar", "Mahantapas Kundu", "Mita Nasipuri", "Dipak Kumar Basu"], "author_detail": {"name": "Dipak Kumar Basu"}, "author": "Dipak Kumar Basu", "links": [{"href": "http://arxiv.org/abs/1203.0876v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1203.0876v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1203.0876v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1203.0876v1", "arxiv_comment": null, "journal_reference": "Proc. 2nd Indian International Conference on Artificial\n  Intelligence, pp. 407-417, Dec. 2005, Pune", "doi": null}
{"id": "http://arxiv.org/abs/1203.0882v1", "guidislink": true, "updated": "2012-03-05T12:22:23Z", "updated_parsed": [2012, 3, 5, 12, 22, 23, 0, 65, 0], "published": "2012-03-05T12:22:23Z", "published_parsed": [2012, 3, 5, 12, 22, 23, 0, 65, 0], "title": "Handwritten Bangla Alphabet Recognition using an MLP Based Classifier", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=0&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Handwritten Bangla Alphabet Recognition using an MLP Based Classifier"}, "summary": "The work presented here involves the design of a Multi Layer Perceptron (MLP)\nbased classifier for recognition of handwritten Bangla alphabet using a 76\nelement feature set Bangla is the second most popular script and language in\nthe Indian subcontinent and the fifth most popular language in the world. The\nfeature set developed for representing handwritten characters of Bangla\nalphabet includes 24 shadow features, 16 centroid features and 36 longest-run\nfeatures. Recognition performances of the MLP designed to work with this\nfeature set are experimentally observed as 86.46% and 75.05% on the samples of\nthe training and the test sets respectively. The work has useful application in\nthe development of a complete OCR system for handwritten Bangla text.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=0&max_results=10&sortBy=relevance&sortOrder=descending", "value": "The work presented here involves the design of a Multi Layer Perceptron (MLP)\nbased classifier for recognition of handwritten Bangla alphabet using a 76\nelement feature set Bangla is the second most popular script and language in\nthe Indian subcontinent and the fifth most popular language in the world. The\nfeature set developed for representing handwritten characters of Bangla\nalphabet includes 24 shadow features, 16 centroid features and 36 longest-run\nfeatures. Recognition performances of the MLP designed to work with this\nfeature set are experimentally observed as 86.46% and 75.05% on the samples of\nthe training and the test sets respectively. The work has useful application in\nthe development of a complete OCR system for handwritten Bangla text."}, "authors": ["Subhadip Basu", "Nibaran Das", "Ram Sarkar", "Mahantapas Kundu", "Mita Nasipuri", "Dipak Kumar Basu"], "author_detail": {"name": "Dipak Kumar Basu"}, "author": "Dipak Kumar Basu", "links": [{"href": "http://arxiv.org/abs/1203.0882v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1203.0882v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1203.0882v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1203.0882v1", "arxiv_comment": null, "journal_reference": "Proc. of the 2nd National Conf. on Computer Processing of Bangla,\n  pp. 285-291, Feb-2005, Dhaka", "doi": null}
{"id": "http://arxiv.org/abs/1410.2045v1", "guidislink": true, "updated": "2014-10-08T10:01:47Z", "updated_parsed": [2014, 10, 8, 10, 1, 47, 2, 281, 0], "published": "2014-10-08T10:01:47Z", "published_parsed": [2014, 10, 8, 10, 1, 47, 2, 281, 0], "title": "Supervised learning Methods for Bangla Web Document Categorization", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=10&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Supervised learning Methods for Bangla Web Document Categorization"}, "summary": "This paper explores the use of machine learning approaches, or more\nspecifically, four supervised learning Methods, namely Decision Tree(C 4.5),\nK-Nearest Neighbour (KNN), Na\\\"ive Bays (NB), and Support Vector Machine (SVM)\nfor categorization of Bangla web documents. This is a task of automatically\nsorting a set of documents into categories from a predefined set. Whereas a\nwide range of methods have been applied to English text categorization,\nrelatively few studies have been conducted on Bangla language text\ncategorization. Hence, we attempt to analyze the efficiency of those four\nmethods for categorization of Bangla documents. In order to validate, Bangla\ncorpus from various websites has been developed and used as examples for the\nexperiment. For Bangla, empirical results support that all four methods produce\nsatisfactory performance with SVM attaining good result in terms of high\ndimensional and relatively noisy document feature vectors.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=10&max_results=10&sortBy=relevance&sortOrder=descending", "value": "This paper explores the use of machine learning approaches, or more\nspecifically, four supervised learning Methods, namely Decision Tree(C 4.5),\nK-Nearest Neighbour (KNN), Na\\\"ive Bays (NB), and Support Vector Machine (SVM)\nfor categorization of Bangla web documents. This is a task of automatically\nsorting a set of documents into categories from a predefined set. Whereas a\nwide range of methods have been applied to English text categorization,\nrelatively few studies have been conducted on Bangla language text\ncategorization. Hence, we attempt to analyze the efficiency of those four\nmethods for categorization of Bangla documents. In order to validate, Bangla\ncorpus from various websites has been developed and used as examples for the\nexperiment. For Bangla, empirical results support that all four methods produce\nsatisfactory performance with SVM attaining good result in terms of high\ndimensional and relatively noisy document feature vectors."}, "authors": ["Ashis Kumar Mandal", "Rikta Sen"], "author_detail": {"name": "Rikta Sen"}, "author": "Rikta Sen", "arxiv_comment": "13 pages, International Journal of Artificial Intelligence &\n  Applications (IJAIA), Vol. 5, No. 5, September 2014", "links": [{"href": "http://arxiv.org/abs/1410.2045v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1410.2045v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1410.2045v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1410.2045v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1002.4040v2", "guidislink": true, "updated": "2010-02-23T06:44:32Z", "updated_parsed": [2010, 2, 23, 6, 44, 32, 1, 54, 0], "published": "2010-02-22T02:58:49Z", "published_parsed": [2010, 2, 22, 2, 58, 49, 0, 53, 0], "title": "Handwritten Bangla Basic and Compound character recognition using MLP\n  and SVM classifier", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=10&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Handwritten Bangla Basic and Compound character recognition using MLP\n  and SVM classifier"}, "summary": "A novel approach for recognition of handwritten compound Bangla characters,\nalong with the Basic characters of Bangla alphabet, is presented here. Compared\nto English like Roman script, one of the major stumbling blocks in Optical\nCharacter Recognition (OCR) of handwritten Bangla script is the large number of\ncomplex shaped character classes of Bangla alphabet. In addition to 50 basic\ncharacter classes, there are nearly 160 complex shaped compound character\nclasses in Bangla alphabet. Dealing with such a large varieties of handwritten\ncharacters with a suitably designed feature set is a challenging problem.\nUncertainty and imprecision are inherent in handwritten script. Moreover, such\na large varieties of complex shaped characters, some of which have close\nresemblance, makes the problem of OCR of handwritten Bangla characters more\ndifficult. Considering the complexity of the problem, the present approach\nmakes an attempt to identify compound character classes from most frequently to\nless frequently occurred ones, i.e., in order of importance. This is to develop\na frame work for incrementally increasing the number of learned classes of\ncompound characters from more frequently occurred ones to less frequently\noccurred ones along with Basic characters. On experimentation, the technique is\nobserved produce an average recognition rate of 79.25 after three fold cross\nvalidation of data with future scope of improvement and extension.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=10&max_results=10&sortBy=relevance&sortOrder=descending", "value": "A novel approach for recognition of handwritten compound Bangla characters,\nalong with the Basic characters of Bangla alphabet, is presented here. Compared\nto English like Roman script, one of the major stumbling blocks in Optical\nCharacter Recognition (OCR) of handwritten Bangla script is the large number of\ncomplex shaped character classes of Bangla alphabet. In addition to 50 basic\ncharacter classes, there are nearly 160 complex shaped compound character\nclasses in Bangla alphabet. Dealing with such a large varieties of handwritten\ncharacters with a suitably designed feature set is a challenging problem.\nUncertainty and imprecision are inherent in handwritten script. Moreover, such\na large varieties of complex shaped characters, some of which have close\nresemblance, makes the problem of OCR of handwritten Bangla characters more\ndifficult. Considering the complexity of the problem, the present approach\nmakes an attempt to identify compound character classes from most frequently to\nless frequently occurred ones, i.e., in order of importance. This is to develop\na frame work for incrementally increasing the number of learned classes of\ncompound characters from more frequently occurred ones to less frequently\noccurred ones along with Basic characters. On experimentation, the technique is\nobserved produce an average recognition rate of 79.25 after three fold cross\nvalidation of data with future scope of improvement and extension."}, "authors": ["Nibaran Das", "Bindaban Das", "Ram Sarkar", "Subhadip Basu", "Mahantapas Kundu", "Mita Nasipuri"], "author_detail": {"name": "Mita Nasipuri"}, "author": "Mita Nasipuri", "links": [{"href": "http://arxiv.org/abs/1002.4040v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1002.4040v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1002.4040v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1002.4040v2", "arxiv_comment": null, "journal_reference": "Journal of Computing, Volume 2, Issue 2, February 2010,\n  https://sites.google.com/site/journalofcomputing/", "doi": null}
{"id": "http://arxiv.org/abs/1410.0478v1", "guidislink": true, "updated": "2014-10-02T08:26:38Z", "updated_parsed": [2014, 10, 2, 8, 26, 38, 3, 275, 0], "published": "2014-10-02T08:26:38Z", "published_parsed": [2014, 10, 2, 8, 26, 38, 3, 275, 0], "title": "Recognition of Handwritten Bangla Basic Characters and Digits using\n  Convex Hull based Feature Set", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=10&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Recognition of Handwritten Bangla Basic Characters and Digits using\n  Convex Hull based Feature Set"}, "summary": "In dealing with the problem of recognition of handwritten character patterns\nof varying shapes and sizes, selection of a proper feature set is important to\nachieve high recognition performance. The current research aims to evaluate the\nperformance of the convex hull based feature set, i.e. 125 features in all\ncomputed over different bays attributes of the convex hull of a pattern, for\neffective recognition of isolated handwritten Bangla basic characters and\ndigits. On experimentation with a database of 10000 samples, the maximum\nrecognition rate of 76.86% is observed for handwritten Bangla characters. For\nBangla numerals the maximum success rate of 99.45%. is achieved on a database\nof 12000 sample. The current work validates the usefulness of a new kind of\nfeature set for recognition of handwritten Bangla basic characters and\nnumerals.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=10&max_results=10&sortBy=relevance&sortOrder=descending", "value": "In dealing with the problem of recognition of handwritten character patterns\nof varying shapes and sizes, selection of a proper feature set is important to\nachieve high recognition performance. The current research aims to evaluate the\nperformance of the convex hull based feature set, i.e. 125 features in all\ncomputed over different bays attributes of the convex hull of a pattern, for\neffective recognition of isolated handwritten Bangla basic characters and\ndigits. On experimentation with a database of 10000 samples, the maximum\nrecognition rate of 76.86% is observed for handwritten Bangla characters. For\nBangla numerals the maximum success rate of 99.45%. is achieved on a database\nof 12000 sample. The current work validates the usefulness of a new kind of\nfeature set for recognition of handwritten Bangla basic characters and\nnumerals."}, "authors": ["Nibaran Das", "Sandip Pramanik", "Subhadip Basu", "Punam Kumar Saha", "Ram Sarkar", "Mahantapas Kundu", "Mita Nasipuri"], "author_detail": {"name": "Mita Nasipuri"}, "author": "Mita Nasipuri", "links": [{"title": "doi", "href": "http://dx.doi.org/10.13140/2.1.3689.4089", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/1410.0478v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1410.0478v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1410.0478v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1410.0478v1", "arxiv_comment": null, "journal_reference": "2009 International Conference on Artificial Intelligence and\n  Pattern Recognition, At Orlando, Florida pp. 380-386", "doi": "10.13140/2.1.3689.4089"}
{"id": "http://arxiv.org/abs/1809.00339v1", "guidislink": true, "updated": "2018-09-02T14:03:30Z", "updated_parsed": [2018, 9, 2, 14, 3, 30, 6, 245, 0], "published": "2018-09-02T14:03:30Z", "published_parsed": [2018, 9, 2, 14, 3, 30, 6, 245, 0], "title": "Chittron: An Automatic Bangla Image Captioning System", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=10&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Chittron: An Automatic Bangla Image Captioning System"}, "summary": "Automatic image caption generation aims to produce an accurate description of\nan image in natural language automatically. However, Bangla, the fifth most\nwidely spoken language in the world, is lagging considerably in the research\nand development of such domain. Besides, while there are many established data\nsets to related to image annotation in English, no such resource exists for\nBangla yet. Hence, this paper outlines the development of \"Chittron\", an\nautomatic image captioning system in Bangla. Moreover, to address the data set\navailability issue, a collection of 16,000 Bangladeshi contextual images has\nbeen accumulated and manually annotated in Bangla. This data set is then used\nto train a model which integrates a pre-trained VGG16 image embedding model\nwith stacked LSTM layers. The model is trained to predict the caption when the\ninput is an image, one word at a time. The results show that the model has\nsuccessfully been able to learn a working language model and to generate\ncaptions of images quite accurately in many cases. The results are evaluated\nmainly qualitatively. However, BLEU scores are also reported. It is expected\nthat a better result can be obtained with a bigger and more varied data set.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=10&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Automatic image caption generation aims to produce an accurate description of\nan image in natural language automatically. However, Bangla, the fifth most\nwidely spoken language in the world, is lagging considerably in the research\nand development of such domain. Besides, while there are many established data\nsets to related to image annotation in English, no such resource exists for\nBangla yet. Hence, this paper outlines the development of \"Chittron\", an\nautomatic image captioning system in Bangla. Moreover, to address the data set\navailability issue, a collection of 16,000 Bangladeshi contextual images has\nbeen accumulated and manually annotated in Bangla. This data set is then used\nto train a model which integrates a pre-trained VGG16 image embedding model\nwith stacked LSTM layers. The model is trained to predict the caption when the\ninput is an image, one word at a time. The results show that the model has\nsuccessfully been able to learn a working language model and to generate\ncaptions of images quite accurately in many cases. The results are evaluated\nmainly qualitatively. However, BLEU scores are also reported. It is expected\nthat a better result can be obtained with a bigger and more varied data set."}, "authors": ["Motiur Rahman", "Nabeel Mohammed", "Nafees Mansoor", "Sifat Momen"], "author_detail": {"name": "Sifat Momen"}, "author": "Sifat Momen", "links": [{"href": "http://arxiv.org/abs/1809.00339v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1809.00339v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1809.00339v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1809.00339v1", "arxiv_comment": null, "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1701.08702v1", "guidislink": true, "updated": "2017-01-27T18:43:31Z", "updated_parsed": [2017, 1, 27, 18, 43, 31, 4, 27, 0], "published": "2017-01-27T18:43:31Z", "published_parsed": [2017, 1, 27, 18, 43, 31, 4, 27, 0], "title": "Bangla Word Clustering Based on Tri-gram, 4-gram and 5-gram Language\n  Model", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=10&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Bangla Word Clustering Based on Tri-gram, 4-gram and 5-gram Language\n  Model"}, "summary": "In this paper, we describe a research method that generates Bangla word\nclusters on the basis of relating to meaning in language and contextual\nsimilarity. The importance of word clustering is in parts of speech (POS)\ntagging, word sense disambiguation, text classification, recommender system,\nspell checker, grammar checker, knowledge discover and for many others Natural\nLanguage Processing (NLP) applications. In the history of word clustering,\nEnglish and some other languages have already implemented some methods on word\nclustering efficiently. But due to lack of the resources, word clustering in\nBangla has not been still implemented efficiently. Presently, its\nimplementation is in the beginning stage. In some research of word clustering\nin English based on preceding and next five words of a key word they found an\nefficient result. Now, we are trying to implement the tri-gram, 4-gram and\n5-gram model of word clustering for Bangla to observe which one is the best\namong them. We have started our research with quite a large corpus of\napproximate 1 lakh Bangla words. We are using a machine learning technique in\nthis research. We will generate word clusters and analyze the clusters by\ntesting some different threshold values.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=10&max_results=10&sortBy=relevance&sortOrder=descending", "value": "In this paper, we describe a research method that generates Bangla word\nclusters on the basis of relating to meaning in language and contextual\nsimilarity. The importance of word clustering is in parts of speech (POS)\ntagging, word sense disambiguation, text classification, recommender system,\nspell checker, grammar checker, knowledge discover and for many others Natural\nLanguage Processing (NLP) applications. In the history of word clustering,\nEnglish and some other languages have already implemented some methods on word\nclustering efficiently. But due to lack of the resources, word clustering in\nBangla has not been still implemented efficiently. Presently, its\nimplementation is in the beginning stage. In some research of word clustering\nin English based on preceding and next five words of a key word they found an\nefficient result. Now, we are trying to implement the tri-gram, 4-gram and\n5-gram model of word clustering for Bangla to observe which one is the best\namong them. We have started our research with quite a large corpus of\napproximate 1 lakh Bangla words. We are using a machine learning technique in\nthis research. We will generate word clusters and analyze the clusters by\ntesting some different threshold values."}, "authors": ["Dipaloke Saha", "Md Saddam Hossain", "MD. Saiful Islam", "Sabir Ismail"], "author_detail": {"name": "Sabir Ismail"}, "author": "Sabir Ismail", "arxiv_comment": "6 pages", "links": [{"href": "http://arxiv.org/abs/1701.08702v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1701.08702v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1701.08702v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1701.08702v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1401.1190v1", "guidislink": true, "updated": "2014-01-06T20:25:26Z", "updated_parsed": [2014, 1, 6, 20, 25, 26, 0, 6, 0], "published": "2014-01-06T20:25:26Z", "published_parsed": [2014, 1, 6, 20, 25, 26, 0, 6, 0], "title": "Bangla Text Recognition from Video Sequence: A New Focus", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=10&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Bangla Text Recognition from Video Sequence: A New Focus"}, "summary": "Extraction and recognition of Bangla text from video frame images is\nchallenging due to complex color background, low-resolution etc. In this paper,\nwe propose an algorithm for extraction and recognition of Bangla text form such\nvideo frames with complex background. Here, a two-step approach has been\nproposed. First, the text line is segmented into words using information based\non line contours. First order gradient value of the text blocks are used to\nfind the word gap. Next, a local binarization technique is applied on each word\nand text line is reconstructed using those words. Secondly, this binarized text\nblock is sent to OCR for recognition purpose.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=10&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Extraction and recognition of Bangla text from video frame images is\nchallenging due to complex color background, low-resolution etc. In this paper,\nwe propose an algorithm for extraction and recognition of Bangla text form such\nvideo frames with complex background. Here, a two-step approach has been\nproposed. First, the text line is segmented into words using information based\non line contours. First order gradient value of the text blocks are used to\nfind the word gap. Next, a local binarization technique is applied on each word\nand text line is reconstructed using those words. Secondly, this binarized text\nblock is sent to OCR for recognition purpose."}, "authors": ["Souvik Bhowmick", "Purnendu Banerjee"], "author_detail": {"name": "Purnendu Banerjee"}, "author": "Purnendu Banerjee", "links": [{"href": "http://arxiv.org/abs/1401.1190v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1401.1190v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1401.1190v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1401.1190v1", "arxiv_comment": null, "journal_reference": "NATIONAL CONFERENCE ON COMPUTING AND SYSTEMS (NaCCS), pp.\n  62-67,2012", "doi": null}
{"id": "http://arxiv.org/abs/1610.00369v2", "guidislink": true, "updated": "2016-11-24T02:13:05Z", "updated_parsed": [2016, 11, 24, 2, 13, 5, 3, 329, 0], "published": "2016-10-02T23:45:23Z", "published_parsed": [2016, 10, 2, 23, 45, 23, 6, 276, 0], "title": "Sentiment Analysis on Bangla and Romanized Bangla Text (BRBT) using Deep\n  Recurrent models", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=10&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Sentiment Analysis on Bangla and Romanized Bangla Text (BRBT) using Deep\n  Recurrent models"}, "summary": "Sentiment Analysis (SA) is an action research area in the digital age. With\nrapid and constant growth of online social media sites and services, and the\nincreasing amount of textual data such as - statuses, comments, reviews etc.\navailable in them, application of automatic SA is on the rise. However, most of\nthe research works on SA in natural language processing (NLP) are based on\nEnglish language. Despite being the sixth most widely spoken language in the\nworld, Bangla still does not have a large and standard dataset. Because of\nthis, recent research works in Bangla have failed to produce results that can\nbe both comparable to works done by others and reusable as stepping stones for\nfuture researchers to progress in this field. Therefore, we first tried to\nprovide a textual dataset - that includes not just Bangla, but Romanized Bangla\ntexts as well, is substantial, post-processed and multiple validated, ready to\nbe used in SA experiments. We tested this dataset in Deep Recurrent model,\nspecifically, Long Short Term Memory (LSTM), using two types of loss functions\n- binary crossentropy and categorical crossentropy, and also did some\nexperimental pre-training by using data from one validation to pre-train the\nother and vice versa. Lastly, we documented the results along with some\nanalysis on them, which were promising.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=10&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Sentiment Analysis (SA) is an action research area in the digital age. With\nrapid and constant growth of online social media sites and services, and the\nincreasing amount of textual data such as - statuses, comments, reviews etc.\navailable in them, application of automatic SA is on the rise. However, most of\nthe research works on SA in natural language processing (NLP) are based on\nEnglish language. Despite being the sixth most widely spoken language in the\nworld, Bangla still does not have a large and standard dataset. Because of\nthis, recent research works in Bangla have failed to produce results that can\nbe both comparable to works done by others and reusable as stepping stones for\nfuture researchers to progress in this field. Therefore, we first tried to\nprovide a textual dataset - that includes not just Bangla, but Romanized Bangla\ntexts as well, is substantial, post-processed and multiple validated, ready to\nbe used in SA experiments. We tested this dataset in Deep Recurrent model,\nspecifically, Long Short Term Memory (LSTM), using two types of loss functions\n- binary crossentropy and categorical crossentropy, and also did some\nexperimental pre-training by using data from one validation to pre-train the\nother and vice versa. Lastly, we documented the results along with some\nanalysis on them, which were promising."}, "authors": ["A. Hassan", "M. R. Amin", "N. Mohammed", "A. K. A. Azad"], "author_detail": {"name": "A. K. A. Azad"}, "author": "A. K. A. Azad", "links": [{"href": "http://arxiv.org/abs/1610.00369v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1610.00369v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.IR", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.NE", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1610.00369v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1610.00369v2", "arxiv_comment": null, "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1705.02680v1", "guidislink": true, "updated": "2017-05-07T18:49:27Z", "updated_parsed": [2017, 5, 7, 18, 49, 27, 6, 127, 0], "published": "2017-05-07T18:49:27Z", "published_parsed": [2017, 5, 7, 18, 49, 27, 6, 127, 0], "title": "Handwritten Bangla Digit Recognition Using Deep Learning", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=10&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Handwritten Bangla Digit Recognition Using Deep Learning"}, "summary": "In spite of the advances in pattern recognition technology, Handwritten\nBangla Character Recognition (HBCR) (such as alpha-numeric and special\ncharacters) remains largely unsolved due to the presence of many perplexing\ncharacters and excessive cursive in Bangla handwriting. Even the best existing\nrecognizers do not lead to satisfactory performance for practical applications.\nTo improve the performance of Handwritten Bangla Digit Recognition (HBDR), we\nherein present a new approach based on deep neural networks which have recently\nshown excellent performance in many pattern recognition and machine learning\napplications, but has not been throughly attempted for HBDR. We introduce\nBangla digit recognition techniques based on Deep Belief Network (DBN),\nConvolutional Neural Networks (CNN), CNN with dropout, CNN with dropout and\nGaussian filters, and CNN with dropout and Gabor filters. These networks have\nthe advantage of extracting and using feature information, improving the\nrecognition of two dimensional shapes with a high degree of invariance to\ntranslation, scaling and other pattern distortions. We systematically evaluated\nthe performance of our method on publicly available Bangla numeral image\ndatabase named CMATERdb 3.1.1. From experiments, we achieved 98.78% recognition\nrate using the proposed method: CNN with Gabor features and dropout, which\noutperforms the state-of-the-art algorithms for HDBR.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=10&max_results=10&sortBy=relevance&sortOrder=descending", "value": "In spite of the advances in pattern recognition technology, Handwritten\nBangla Character Recognition (HBCR) (such as alpha-numeric and special\ncharacters) remains largely unsolved due to the presence of many perplexing\ncharacters and excessive cursive in Bangla handwriting. Even the best existing\nrecognizers do not lead to satisfactory performance for practical applications.\nTo improve the performance of Handwritten Bangla Digit Recognition (HBDR), we\nherein present a new approach based on deep neural networks which have recently\nshown excellent performance in many pattern recognition and machine learning\napplications, but has not been throughly attempted for HBDR. We introduce\nBangla digit recognition techniques based on Deep Belief Network (DBN),\nConvolutional Neural Networks (CNN), CNN with dropout, CNN with dropout and\nGaussian filters, and CNN with dropout and Gabor filters. These networks have\nthe advantage of extracting and using feature information, improving the\nrecognition of two dimensional shapes with a high degree of invariance to\ntranslation, scaling and other pattern distortions. We systematically evaluated\nthe performance of our method on publicly available Bangla numeral image\ndatabase named CMATERdb 3.1.1. From experiments, we achieved 98.78% recognition\nrate using the proposed method: CNN with Gabor features and dropout, which\noutperforms the state-of-the-art algorithms for HDBR."}, "authors": ["Md Zahangir Alom", "Paheding Sidike", "Tarek M. Taha", "Vijayan K. Asari"], "author_detail": {"name": "Vijayan K. Asari"}, "author": "Vijayan K. Asari", "arxiv_comment": "12 pages, 10 figures, 3 tables", "links": [{"href": "http://arxiv.org/abs/1705.02680v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1705.02680v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1705.02680v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1705.02680v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1208.0995v1", "guidislink": true, "updated": "2012-08-05T09:22:06Z", "updated_parsed": [2012, 8, 5, 9, 22, 6, 6, 218, 0], "published": "2012-08-05T09:22:06Z", "published_parsed": [2012, 8, 5, 9, 22, 6, 6, 218, 0], "title": "Design and implementation of a digital clock showing digits in Bangla\n  font using microcontroller AT89C4051", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=10&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Design and implementation of a digital clock showing digits in Bangla\n  font using microcontroller AT89C4051"}, "summary": "In this paper, a digital clock is designed where the microcontroller is used\nfor timing controller and the font of the Bangla digits are designed, and\nprogrammed within the microcontroller. The design is cost effective, simple and\neasy for maintenance.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=10&max_results=10&sortBy=relevance&sortOrder=descending", "value": "In this paper, a digital clock is designed where the microcontroller is used\nfor timing controller and the font of the Bangla digits are designed, and\nprogrammed within the microcontroller. The design is cost effective, simple and\neasy for maintenance."}, "authors": ["Nasif Muslim", "Md. Tanvir Adnan", "Mohammad Zahidul Kabir", "Md. Humayun Kabir", "Sheikh Mominul Islam"], "author_detail": {"name": "Sheikh Mominul Islam"}, "author": "Sheikh Mominul Islam", "links": [{"href": "http://arxiv.org/abs/1208.0995v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1208.0995v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.AR", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.AR", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1208.0995v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1208.0995v1", "arxiv_comment": null, "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1003.5897v1", "guidislink": true, "updated": "2010-03-30T18:54:57Z", "updated_parsed": [2010, 3, 30, 18, 54, 57, 1, 89, 0], "published": "2010-03-30T18:54:57Z", "published_parsed": [2010, 3, 30, 18, 54, 57, 1, 89, 0], "title": "Development of a Multi-User Recognition Engine for Handwritten Bangla\n  Basic Characters and Digits", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=10&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Development of a Multi-User Recognition Engine for Handwritten Bangla\n  Basic Characters and Digits"}, "summary": "The objective of the paper is to recognize handwritten samples of basic\nBangla characters using Tesseract open source Optical Character Recognition\n(OCR) engine under Apache License 2.0. Handwritten data samples containing\nisolated Bangla basic characters and digits were collected from different\nusers. Tesseract is trained with user-specific data samples of document pages\nto generate separate user-models representing a unique language-set. Each such\nlanguage-set recognizes isolated basic Bangla handwritten test samples\ncollected from the designated users. On a three user model, the system is\ntrained with 919, 928 and 648 isolated handwritten character and digit samples\nand the performance is tested on 1527, 14116 and 1279 character and digit\nsamples, collected form the test datasets of the three users respectively. The\nuser specific character/digit recognition accuracies were obtained as 90.66%,\n91.66% and 96.87% respectively. The overall basic character-level and digit\nlevel accuracy of the system is observed as 92.15% and 97.37%. The system fails\nto segment 12.33% characters and 15.96% digits and also erroneously classifies\n7.85% characters and 2.63% on the overall dataset.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=10&max_results=10&sortBy=relevance&sortOrder=descending", "value": "The objective of the paper is to recognize handwritten samples of basic\nBangla characters using Tesseract open source Optical Character Recognition\n(OCR) engine under Apache License 2.0. Handwritten data samples containing\nisolated Bangla basic characters and digits were collected from different\nusers. Tesseract is trained with user-specific data samples of document pages\nto generate separate user-models representing a unique language-set. Each such\nlanguage-set recognizes isolated basic Bangla handwritten test samples\ncollected from the designated users. On a three user model, the system is\ntrained with 919, 928 and 648 isolated handwritten character and digit samples\nand the performance is tested on 1527, 14116 and 1279 character and digit\nsamples, collected form the test datasets of the three users respectively. The\nuser specific character/digit recognition accuracies were obtained as 90.66%,\n91.66% and 96.87% respectively. The overall basic character-level and digit\nlevel accuracy of the system is observed as 92.15% and 97.37%. The system fails\nto segment 12.33% characters and 15.96% digits and also erroneously classifies\n7.85% characters and 2.63% on the overall dataset."}, "authors": ["Sandip Rakshit", "Debkumar Ghosal", "Tanmoy Das", "Subhrajit Dutta", "Subhadip Basu"], "author_detail": {"name": "Subhadip Basu"}, "author": "Subhadip Basu", "arxiv_comment": "Proc. (CD) Int. Conf. on Information Technology and Business\n  Intelligence (2009)", "links": [{"href": "http://arxiv.org/abs/1003.5897v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1003.5897v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1003.5897v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1003.5897v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1308.3785v1", "guidislink": true, "updated": "2013-08-17T14:04:00Z", "updated_parsed": [2013, 8, 17, 14, 4, 0, 5, 229, 0], "published": "2013-08-17T14:04:00Z", "published_parsed": [2013, 8, 17, 14, 4, 0, 5, 229, 0], "title": "Implementation Of Back-Propagation Neural Network For Isolated Bangla\n  Speech Recognition", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=20&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Implementation Of Back-Propagation Neural Network For Isolated Bangla\n  Speech Recognition"}, "summary": "This paper is concerned with the development of Back-propagation Neural\nNetwork for Bangla Speech Recognition. In this paper, ten bangla digits were\nrecorded from ten speakers and have been recognized. The features of these\nspeech digits were extracted by the method of Mel Frequency Cepstral\nCoefficient (MFCC) analysis. The mfcc features of five speakers were used to\ntrain the network with Back propagation algorithm. The mfcc features of ten\nbangla digit speeches, from 0 to 9, of another five speakers were used to test\nthe system. All the methods and algorithms used in this research were\nimplemented using the features of Turbo C and C++ languages. From our\ninvestigation it is seen that the developed system can successfully encode and\nanalyze the mfcc features of the speech signal to recognition. The developed\nsystem achieved recognition rate about 96.332% for known speakers (i.e.,\nspeaker dependent) and 92% for unknown speakers (i.e., speaker independent).", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=20&max_results=10&sortBy=relevance&sortOrder=descending", "value": "This paper is concerned with the development of Back-propagation Neural\nNetwork for Bangla Speech Recognition. In this paper, ten bangla digits were\nrecorded from ten speakers and have been recognized. The features of these\nspeech digits were extracted by the method of Mel Frequency Cepstral\nCoefficient (MFCC) analysis. The mfcc features of five speakers were used to\ntrain the network with Back propagation algorithm. The mfcc features of ten\nbangla digit speeches, from 0 to 9, of another five speakers were used to test\nthe system. All the methods and algorithms used in this research were\nimplemented using the features of Turbo C and C++ languages. From our\ninvestigation it is seen that the developed system can successfully encode and\nanalyze the mfcc features of the speech signal to recognition. The developed\nsystem achieved recognition rate about 96.332% for known speakers (i.e.,\nspeaker dependent) and 92% for unknown speakers (i.e., speaker independent)."}, "authors": ["Md. Ali Hossain", "Md. Mijanur Rahman", "Uzzal Kumar Prodhan", "Md. Farukuzzaman Khan"], "author_detail": {"name": "Md. Farukuzzaman Khan"}, "author": "Md. Farukuzzaman Khan", "links": [{"title": "doi", "href": "http://dx.doi.org/10.5121/ijist.2013.3401", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/1308.3785v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1308.3785v1", "rel": "related", "type": "application/pdf"}], "arxiv_comment": "9 pages, 3 figures, 1 table", "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.NE", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1308.3785v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1308.3785v1", "journal_reference": "International Journal of Information Sciences and Techniques\n  (IJIST) Vol.3, No.4, July 2013", "doi": "10.5121/ijist.2013.3401"}
{"id": "http://arxiv.org/abs/1310.1590v1", "guidislink": true, "updated": "2013-10-06T14:37:05Z", "updated_parsed": [2013, 10, 6, 14, 37, 5, 6, 279, 0], "published": "2013-10-06T14:37:05Z", "published_parsed": [2013, 10, 6, 14, 37, 5, 6, 279, 0], "title": "Evolution of the Modern Phase of Written Bangla: A Statistical Study", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=20&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Evolution of the Modern Phase of Written Bangla: A Statistical Study"}, "summary": "Active languages such as Bangla (or Bengali) evolve over time due to a\nvariety of social, cultural, economic, and political issues. In this paper, we\nanalyze the change in the written form of the modern phase of Bangla\nquantitatively in terms of character-level, syllable-level, morpheme-level and\nword-level features. We collect three different types of corpora---classical,\nnewspapers and blogs---and test whether the differences in their features are\nstatistically significant. Results suggest that there are significant changes\nin the length of a word when measured in terms of characters, but there is not\nmuch difference in usage of different characters, syllables and morphemes in a\nword or of different words in a sentence. To the best of our knowledge, this is\nthe first work on Bangla of this kind.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=20&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Active languages such as Bangla (or Bengali) evolve over time due to a\nvariety of social, cultural, economic, and political issues. In this paper, we\nanalyze the change in the written form of the modern phase of Bangla\nquantitatively in terms of character-level, syllable-level, morpheme-level and\nword-level features. We collect three different types of corpora---classical,\nnewspapers and blogs---and test whether the differences in their features are\nstatistically significant. Results suggest that there are significant changes\nin the length of a word when measured in terms of characters, but there is not\nmuch difference in usage of different characters, syllables and morphemes in a\nword or of different words in a sentence. To the best of our knowledge, this is\nthe first work on Bangla of this kind."}, "authors": ["Paheli Bhattacharya", "Arnab Bhattacharya"], "author_detail": {"name": "Arnab Bhattacharya"}, "author": "Arnab Bhattacharya", "arxiv_comment": "LCC 2013", "links": [{"href": "http://arxiv.org/abs/1310.1590v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1310.1590v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "I.2.7", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1310.1590v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1310.1590v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1602.07803v1", "guidislink": true, "updated": "2016-02-25T05:35:16Z", "updated_parsed": [2016, 2, 25, 5, 35, 16, 3, 56, 0], "published": "2016-02-25T05:35:16Z", "published_parsed": [2016, 2, 25, 5, 35, 16, 3, 56, 0], "title": "Automated Word Prediction in Bangla Language Using Stochastic Language\n  Models", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=20&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Automated Word Prediction in Bangla Language Using Stochastic Language\n  Models"}, "summary": "Word completion and word prediction are two important phenomena in typing\nthat benefit users who type using keyboard or other similar devices. They can\nhave profound impact on the typing of disable people. Our work is based on word\nprediction on Bangla sentence by using stochastic, i.e. N-gram language model\nsuch as unigram, bigram, trigram, deleted Interpolation and backoff models for\nauto completing a sentence by predicting a correct word in a sentence which\nsaves time and keystrokes of typing and also reduces misspelling. We use large\ndata corpus of Bangla language of different word types to predict correct word\nwith the accuracy as much as possible. We have found promising results. We hope\nthat our work will impact on the baseline for automated Bangla typing.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=20&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Word completion and word prediction are two important phenomena in typing\nthat benefit users who type using keyboard or other similar devices. They can\nhave profound impact on the typing of disable people. Our work is based on word\nprediction on Bangla sentence by using stochastic, i.e. N-gram language model\nsuch as unigram, bigram, trigram, deleted Interpolation and backoff models for\nauto completing a sentence by predicting a correct word in a sentence which\nsaves time and keystrokes of typing and also reduces misspelling. We use large\ndata corpus of Bangla language of different word types to predict correct word\nwith the accuracy as much as possible. We have found promising results. We hope\nthat our work will impact on the baseline for automated Bangla typing."}, "authors": ["Md. Masudul Haque", "Md. Tarek Habib", "Md. Mokhlesur Rahman"], "author_detail": {"name": "Md. Mokhlesur Rahman"}, "author": "Md. Mokhlesur Rahman", "links": [{"title": "doi", "href": "http://dx.doi.org/10.5121/ijfcst.2015.5607", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/1602.07803v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1602.07803v1", "rel": "related", "type": "application/pdf"}], "arxiv_comment": "in International Journal in Foundations of Computer Science &\n  Technology (IJFCST) Vol.5, No.6, November 2015", "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1602.07803v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1602.07803v1", "journal_reference": null, "doi": "10.5121/ijfcst.2015.5607"}
{"id": "http://arxiv.org/abs/1809.00905v1", "guidislink": true, "updated": "2018-09-04T11:55:34Z", "updated_parsed": [2018, 9, 4, 11, 55, 34, 1, 247, 0], "published": "2018-09-04T11:55:34Z", "published_parsed": [2018, 9, 4, 11, 55, 34, 1, 247, 0], "title": "Bangla License Plate Recognition Using Convolutional Neural Networks\n  (CNN)", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=20&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Bangla License Plate Recognition Using Convolutional Neural Networks\n  (CNN)"}, "summary": "In the last few years, the deep learning technique in particular\nConvolutional Neural Networks (CNNs) is using massively in the field of\ncomputer vision and machine learning. This deep learning technique provides\nstate-of-the-art accuracy in different classification, segmentation, and\ndetection tasks on different benchmarks such as MNIST, CIFAR-10, CIFAR-100,\nMicrosoft COCO, and ImageNet. However, there are a lot of research has been\nconducted for Bangla License plate recognition with traditional machine\nlearning approaches in last decade. None of them are used to deploy a physical\nsystem for Bangla License Plate Recognition System (BLPRS) due to their poor\nrecognition accuracy. In this paper, we have implemented CNNs based Bangla\nlicense plate recognition system with better accuracy that can be applied for\ndifferent purposes including roadside assistance, automatic parking lot\nmanagement system, vehicle license status detection and so on. Along with that,\nwe have also created and released a very first and standard database for BLPRS.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=20&max_results=10&sortBy=relevance&sortOrder=descending", "value": "In the last few years, the deep learning technique in particular\nConvolutional Neural Networks (CNNs) is using massively in the field of\ncomputer vision and machine learning. This deep learning technique provides\nstate-of-the-art accuracy in different classification, segmentation, and\ndetection tasks on different benchmarks such as MNIST, CIFAR-10, CIFAR-100,\nMicrosoft COCO, and ImageNet. However, there are a lot of research has been\nconducted for Bangla License plate recognition with traditional machine\nlearning approaches in last decade. None of them are used to deploy a physical\nsystem for Bangla License Plate Recognition System (BLPRS) due to their poor\nrecognition accuracy. In this paper, we have implemented CNNs based Bangla\nlicense plate recognition system with better accuracy that can be applied for\ndifferent purposes including roadside assistance, automatic parking lot\nmanagement system, vehicle license status detection and so on. Along with that,\nwe have also created and released a very first and standard database for BLPRS."}, "authors": ["M M Shaifur Rahman", "Mst Shamima Nasrin", "Moin Mostakim", "Md Zahangir Alom"], "author_detail": {"name": "Md Zahangir Alom"}, "author": "Md Zahangir Alom", "arxiv_comment": "6 pages,10 figures", "links": [{"href": "http://arxiv.org/abs/1809.00905v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1809.00905v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1809.00905v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1809.00905v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1911.07613v1", "guidislink": true, "updated": "2019-11-15T08:22:33Z", "updated_parsed": [2019, 11, 15, 8, 22, 33, 4, 319, 0], "published": "2019-11-15T08:22:33Z", "published_parsed": [2019, 11, 15, 8, 22, 33, 4, 319, 0], "title": "A Subword Level Language Model for Bangla Language", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=20&max_results=10&sortBy=relevance&sortOrder=descending", "value": "A Subword Level Language Model for Bangla Language"}, "summary": "Language models are at the core of natural language processing. The ability\nto represent natural language gives rise to its applications in numerous NLP\ntasks including text classification, summarization, and translation. Research\nin this area is very limited in Bangla due to the scarcity of resources, except\nfor some count-based models and very recent neural language models being\nproposed, which are all based on words and limited in practical tasks due to\ntheir high perplexity. This paper attempts to approach this issue of perplexity\nand proposes a subword level neural language model with the AWD-LSTM\narchitecture and various other techniques suitable for training in Bangla\nlanguage. The model is trained on a corpus of Bangla newspaper articles of an\nappreciable size consisting of more than 28.5 million word tokens. The\nperformance comparison with various other models depicts the significant\nreduction in perplexity the proposed model provides, reaching as low as 39.84,\nin just 20 epochs.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=20&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Language models are at the core of natural language processing. The ability\nto represent natural language gives rise to its applications in numerous NLP\ntasks including text classification, summarization, and translation. Research\nin this area is very limited in Bangla due to the scarcity of resources, except\nfor some count-based models and very recent neural language models being\nproposed, which are all based on words and limited in practical tasks due to\ntheir high perplexity. This paper attempts to approach this issue of perplexity\nand proposes a subword level neural language model with the AWD-LSTM\narchitecture and various other techniques suitable for training in Bangla\nlanguage. The model is trained on a corpus of Bangla newspaper articles of an\nappreciable size consisting of more than 28.5 million word tokens. The\nperformance comparison with various other models depicts the significant\nreduction in perplexity the proposed model provides, reaching as low as 39.84,\nin just 20 epochs."}, "authors": ["Aisha Khatun", "Anisur Rahman", "Hemayet Ahmed Chowdhury", "Md. Saiful Islam", "Ayesha Tasnim"], "author_detail": {"name": "Ayesha Tasnim"}, "author": "Ayesha Tasnim", "arxiv_comment": "12 pages, Conference Paper", "links": [{"href": "http://arxiv.org/abs/1911.07613v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1911.07613v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1911.07613v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1911.07613v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/2005.14627v1", "guidislink": true, "updated": "2020-05-29T15:38:54Z", "updated_parsed": [2020, 5, 29, 15, 38, 54, 4, 150, 0], "published": "2020-05-29T15:38:54Z", "published_parsed": [2020, 5, 29, 15, 38, 54, 4, 150, 0], "title": "Detection of Bangla Fake News using MNB and SVM Classifier", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=20&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Detection of Bangla Fake News using MNB and SVM Classifier"}, "summary": "Fake news has been coming into sight in significant numbers for numerous\nbusiness and political reasons and has become frequent in the online world.\nPeople can get contaminated easily by these fake news for its fabricated words\nwhich have enormous effects on the offline community. Thus, interest in\nresearch in this area has risen. Significant research has been conducted on the\ndetection of fake news from English texts and other languages but a few in\nBangla Language. Our work reflects the experimental analysis on the detection\nof Bangla fake news from social media as this field still requires much focus.\nIn this research work, we have used two supervised machine learning algorithms,\nMultinomial Naive Bayes (MNB) and Support Vector Machine (SVM) classifiers to\ndetect Bangla fake news with CountVectorizer and Term Frequency - Inverse\nDocument Frequency Vectorizer as feature extraction. Our proposed framework\ndetects fake news depending on the polarity of the corresponding article.\nFinally, our analysis shows SVM with the linear kernel with an accuracy of\n96.64% outperform MNB with an accuracy of 93.32%.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=20&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Fake news has been coming into sight in significant numbers for numerous\nbusiness and political reasons and has become frequent in the online world.\nPeople can get contaminated easily by these fake news for its fabricated words\nwhich have enormous effects on the offline community. Thus, interest in\nresearch in this area has risen. Significant research has been conducted on the\ndetection of fake news from English texts and other languages but a few in\nBangla Language. Our work reflects the experimental analysis on the detection\nof Bangla fake news from social media as this field still requires much focus.\nIn this research work, we have used two supervised machine learning algorithms,\nMultinomial Naive Bayes (MNB) and Support Vector Machine (SVM) classifiers to\ndetect Bangla fake news with CountVectorizer and Term Frequency - Inverse\nDocument Frequency Vectorizer as feature extraction. Our proposed framework\ndetects fake news depending on the polarity of the corresponding article.\nFinally, our analysis shows SVM with the linear kernel with an accuracy of\n96.64% outperform MNB with an accuracy of 93.32%."}, "authors": ["Md Gulzar Hussain", "Md Rashidul Hasan", "Mahmuda Rahman", "Joy Protim", "Sakib Al Hasan"], "author_detail": {"name": "Sakib Al Hasan"}, "author": "Sakib Al Hasan", "links": [{"href": "http://arxiv.org/abs/2005.14627v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2005.14627v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.IR", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2005.14627v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2005.14627v1", "arxiv_comment": null, "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1712.09872v3", "guidislink": true, "updated": "2018-02-10T18:40:54Z", "updated_parsed": [2018, 2, 10, 18, 40, 54, 5, 41, 0], "published": "2017-12-28T14:31:56Z", "published_parsed": [2017, 12, 28, 14, 31, 56, 3, 362, 0], "title": "Handwritten Bangla Character Recognition Using The State-of-Art Deep\n  Convolutional Neural Networks", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=20&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Handwritten Bangla Character Recognition Using The State-of-Art Deep\n  Convolutional Neural Networks"}, "summary": "In spite of advances in object recognition technology, Handwritten Bangla\nCharacter Recognition (HBCR) remains largely unsolved due to the presence of\nmany ambiguous handwritten characters and excessively cursive Bangla\nhandwritings. Even the best existing recognizers do not lead to satisfactory\nperformance for practical applications related to Bangla character recognition\nand have much lower performance than those developed for English alpha-numeric\ncharacters. To improve the performance of HBCR, we herein present the\napplication of the state-of-the-art Deep Convolutional Neural Networks (DCNN)\nincluding VGG Network, All Convolution Network (All-Conv Net), Network in\nNetwork (NiN), Residual Network, FractalNet, and DenseNet for HBCR. The deep\nlearning approaches have the advantage of extracting and using feature\ninformation, improving the recognition of 2D shapes with a high degree of\ninvariance to translation, scaling and other distortions. We systematically\nevaluated the performance of DCNN models on publicly available Bangla\nhandwritten character dataset called CMATERdb and achieved the superior\nrecognition accuracy when using DCNN models. This improvement would help in\nbuilding an automatic HBCR system for practical applications.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=20&max_results=10&sortBy=relevance&sortOrder=descending", "value": "In spite of advances in object recognition technology, Handwritten Bangla\nCharacter Recognition (HBCR) remains largely unsolved due to the presence of\nmany ambiguous handwritten characters and excessively cursive Bangla\nhandwritings. Even the best existing recognizers do not lead to satisfactory\nperformance for practical applications related to Bangla character recognition\nand have much lower performance than those developed for English alpha-numeric\ncharacters. To improve the performance of HBCR, we herein present the\napplication of the state-of-the-art Deep Convolutional Neural Networks (DCNN)\nincluding VGG Network, All Convolution Network (All-Conv Net), Network in\nNetwork (NiN), Residual Network, FractalNet, and DenseNet for HBCR. The deep\nlearning approaches have the advantage of extracting and using feature\ninformation, improving the recognition of 2D shapes with a high degree of\ninvariance to translation, scaling and other distortions. We systematically\nevaluated the performance of DCNN models on publicly available Bangla\nhandwritten character dataset called CMATERdb and achieved the superior\nrecognition accuracy when using DCNN models. This improvement would help in\nbuilding an automatic HBCR system for practical applications."}, "authors": ["Md Zahangir Alom", "Peheding Sidike", "Mahmudul Hasan", "Tark M. Taha", "Vijayan K. Asari"], "author_detail": {"name": "Vijayan K. Asari"}, "author": "Vijayan K. Asari", "arxiv_comment": "12 pages,22 figures, 5 tables. arXiv admin note: text overlap with\n  arXiv:1705.02680", "links": [{"href": "http://arxiv.org/abs/1712.09872v3", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1712.09872v3", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1712.09872v3", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1712.09872v3", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/2101.00204v1", "guidislink": true, "updated": "2021-01-01T09:28:45Z", "updated_parsed": [2021, 1, 1, 9, 28, 45, 4, 1, 0], "published": "2021-01-01T09:28:45Z", "published_parsed": [2021, 1, 1, 9, 28, 45, 4, 1, 0], "title": "BanglaBERT: Combating Embedding Barrier for Low-Resource Language\n  Understanding", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=20&max_results=10&sortBy=relevance&sortOrder=descending", "value": "BanglaBERT: Combating Embedding Barrier for Low-Resource Language\n  Understanding"}, "summary": "Pre-training language models on large volume of data with self-supervised\nobjectives has become a standard practice in natural language processing.\nHowever, most such state-of-the-art models are available in only English and\nother resource-rich languages. Even in multilingual models, which are trained\non hundreds of languages, low-resource ones still remain underrepresented.\nBangla, the seventh most widely spoken language in the world, is still low in\nterms of resources. Few downstream task datasets for language understanding in\nBangla are publicly available, and there is a clear shortage of good quality\ndata for pre-training. In this work, we build a Bangla natural language\nunderstanding model pre-trained on 18.6 GB data we crawled from top Bangla\nsites on the internet. We introduce a new downstream task dataset and benchmark\non four tasks on sentence classification, document classification, natural\nlanguage understanding, and sequence tagging. Our model outperforms\nmultilingual baselines and previous state-of-the-art results by 1-6%. In the\nprocess, we identify a major shortcoming of multilingual models that hurt\nperformance for low-resource languages that don't share writing scripts with\nany high resource one, which we name the `Embedding Barrier'. We perform\nextensive experiments to study this barrier. We release all our datasets and\npre-trained models to aid future NLP research on Bangla and other low-resource\nlanguages. Our code and data are available at\nhttps://github.com/csebuetnlp/banglabert.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=20&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Pre-training language models on large volume of data with self-supervised\nobjectives has become a standard practice in natural language processing.\nHowever, most such state-of-the-art models are available in only English and\nother resource-rich languages. Even in multilingual models, which are trained\non hundreds of languages, low-resource ones still remain underrepresented.\nBangla, the seventh most widely spoken language in the world, is still low in\nterms of resources. Few downstream task datasets for language understanding in\nBangla are publicly available, and there is a clear shortage of good quality\ndata for pre-training. In this work, we build a Bangla natural language\nunderstanding model pre-trained on 18.6 GB data we crawled from top Bangla\nsites on the internet. We introduce a new downstream task dataset and benchmark\non four tasks on sentence classification, document classification, natural\nlanguage understanding, and sequence tagging. Our model outperforms\nmultilingual baselines and previous state-of-the-art results by 1-6%. In the\nprocess, we identify a major shortcoming of multilingual models that hurt\nperformance for low-resource languages that don't share writing scripts with\nany high resource one, which we name the `Embedding Barrier'. We perform\nextensive experiments to study this barrier. We release all our datasets and\npre-trained models to aid future NLP research on Bangla and other low-resource\nlanguages. Our code and data are available at\nhttps://github.com/csebuetnlp/banglabert."}, "authors": ["Abhik Bhattacharjee", "Tahmid Hasan", "Kazi Samin", "M. Sohel Rahman", "Anindya Iqbal", "Rifat Shahriyar"], "author_detail": {"name": "Rifat Shahriyar"}, "author": "Rifat Shahriyar", "links": [{"href": "http://arxiv.org/abs/2101.00204v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2101.00204v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2101.00204v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2101.00204v1", "arxiv_comment": null, "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/2011.10106v1", "guidislink": true, "updated": "2020-11-19T21:06:28Z", "updated_parsed": [2020, 11, 19, 21, 6, 28, 3, 324, 0], "published": "2020-11-19T21:06:28Z", "published_parsed": [2020, 11, 19, 21, 6, 28, 3, 324, 0], "title": "Sentiment Classification in Bangla Textual Content: A Comparative Study", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=20&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Sentiment Classification in Bangla Textual Content: A Comparative Study"}, "summary": "Sentiment analysis has been widely used to understand our views on social and\npolitical agendas or user experiences over a product. It is one of the cores\nand well-researched areas in NLP. However, for low-resource languages, like\nBangla, one of the prominent challenge is the lack of resources. Another\nimportant limitation, in the current literature for Bangla, is the absence of\ncomparable results due to the lack of a well-defined train/test split. In this\nstudy, we explore several publicly available sentiment labeled datasets and\ndesigned classifiers using both classical and deep learning algorithms. In our\nstudy, the classical algorithms include SVM and Random Forest, and deep\nlearning algorithms include CNN, FastText, and transformer-based models. We\ncompare these models in terms of model performance and time-resource\ncomplexity. Our finding suggests transformer-based models, which have not been\nexplored earlier for Bangla, outperform all other models. Furthermore, we\ncreated a weighted list of lexicon content based on the valence score per\nclass. We then analyzed the content for high significance entries per class, in\nthe datasets. For reproducibility, we make publicly available data splits and\nthe ranked lexicon list. The presented results can be used for future studies\nas a benchmark.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=20&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Sentiment analysis has been widely used to understand our views on social and\npolitical agendas or user experiences over a product. It is one of the cores\nand well-researched areas in NLP. However, for low-resource languages, like\nBangla, one of the prominent challenge is the lack of resources. Another\nimportant limitation, in the current literature for Bangla, is the absence of\ncomparable results due to the lack of a well-defined train/test split. In this\nstudy, we explore several publicly available sentiment labeled datasets and\ndesigned classifiers using both classical and deep learning algorithms. In our\nstudy, the classical algorithms include SVM and Random Forest, and deep\nlearning algorithms include CNN, FastText, and transformer-based models. We\ncompare these models in terms of model performance and time-resource\ncomplexity. Our finding suggests transformer-based models, which have not been\nexplored earlier for Bangla, outperform all other models. Furthermore, we\ncreated a weighted list of lexicon content based on the valence score per\nclass. We then analyzed the content for high significance entries per class, in\nthe datasets. For reproducibility, we make publicly available data splits and\nthe ranked lexicon list. The presented results can be used for future studies\nas a benchmark."}, "authors": ["Md. Arid Hasan", "Jannatul Tajrin", "Shammur Absar Chowdhury", "Firoj Alam"], "author_detail": {"name": "Firoj Alam"}, "author": "Firoj Alam", "arxiv_comment": "Accepted at ICCIT-2020", "links": [{"href": "http://arxiv.org/abs/2011.10106v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2011.10106v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.IR", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "68T50", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "I.2.7", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2011.10106v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2011.10106v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1002.4007v1", "guidislink": true, "updated": "2010-02-21T19:48:16Z", "updated_parsed": [2010, 2, 21, 19, 48, 16, 6, 52, 0], "published": "2010-02-21T19:48:16Z", "published_parsed": [2010, 2, 21, 19, 48, 16, 6, 52, 0], "title": "Word level Script Identification from Bangla and Devanagri Handwritten\n  Texts mixed with Roman Script", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=20&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Word level Script Identification from Bangla and Devanagri Handwritten\n  Texts mixed with Roman Script"}, "summary": "India is a multi-lingual country where Roman script is often used alongside\ndifferent Indic scripts in a text document. To develop a script specific\nhandwritten Optical Character Recognition (OCR) system, it is therefore\nnecessary to identify the scripts of handwritten text correctly. In this paper,\nwe present a system, which automatically separates the scripts of handwritten\nwords from a document, written in Bangla or Devanagri mixed with Roman scripts.\nIn this script separation technique, we first, extract the text lines and words\nfrom document pages using a script independent Neighboring Component Analysis\ntechnique. Then we have designed a Multi Layer Perceptron (MLP) based\nclassifier for script separation, trained with 8 different wordlevel holistic\nfeatures. Two equal sized datasets, one with Bangla and Roman scripts and the\nother with Devanagri and Roman scripts, are prepared for the system evaluation.\nOn respective independent text samples, word-level script identification\naccuracies of 99.29% and 98.43% are achieved.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=20&max_results=10&sortBy=relevance&sortOrder=descending", "value": "India is a multi-lingual country where Roman script is often used alongside\ndifferent Indic scripts in a text document. To develop a script specific\nhandwritten Optical Character Recognition (OCR) system, it is therefore\nnecessary to identify the scripts of handwritten text correctly. In this paper,\nwe present a system, which automatically separates the scripts of handwritten\nwords from a document, written in Bangla or Devanagri mixed with Roman scripts.\nIn this script separation technique, we first, extract the text lines and words\nfrom document pages using a script independent Neighboring Component Analysis\ntechnique. Then we have designed a Multi Layer Perceptron (MLP) based\nclassifier for script separation, trained with 8 different wordlevel holistic\nfeatures. Two equal sized datasets, one with Bangla and Roman scripts and the\nother with Devanagri and Roman scripts, are prepared for the system evaluation.\nOn respective independent text samples, word-level script identification\naccuracies of 99.29% and 98.43% are achieved."}, "authors": ["Ram Sarkar", "Nibaran Das", "Subhadip Basu", "Mahantapas Kundu", "Mita Nasipuri", "Dipak Kumar Basu"], "author_detail": {"name": "Dipak Kumar Basu"}, "author": "Dipak Kumar Basu", "links": [{"href": "http://arxiv.org/abs/1002.4007v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1002.4007v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1002.4007v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1002.4007v1", "arxiv_comment": null, "journal_reference": "Journal of Computing, Volume 2, Issue 2, February 2010,\n  https://sites.google.com/site/journalofcomputing/", "doi": null}
{"id": "http://arxiv.org/abs/1206.0381v1", "guidislink": true, "updated": "2012-06-02T13:23:18Z", "updated_parsed": [2012, 6, 2, 13, 23, 18, 5, 154, 0], "published": "2012-06-02T13:23:18Z", "published_parsed": [2012, 6, 2, 13, 23, 18, 5, 154, 0], "title": "UNL Based Bangla Natural Text Conversion - Predicate Preserving Parser\n  Approach", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=30&max_results=10&sortBy=relevance&sortOrder=descending", "value": "UNL Based Bangla Natural Text Conversion - Predicate Preserving Parser\n  Approach"}, "summary": "Universal Networking Language (UNL) is a declarative formal language that is\nused to represent semantic data extracted from natural language texts. This\npaper presents a novel approach to converting Bangla natural language text into\nUNL using a method known as Predicate Preserving Parser (PPP) technique. PPP\nperforms morphological, syntactic and semantic, and lexical analysis of text\nsynchronously. This analysis produces a semantic-net like structure represented\nusing UNL. We demonstrate how Bangla texts are analyzed following the PPP\ntechnique to produce UNL documents which can then be translated into any other\nsuitable natural language facilitating the opportunity to develop a universal\nlanguage translation method via UNL.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=30&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Universal Networking Language (UNL) is a declarative formal language that is\nused to represent semantic data extracted from natural language texts. This\npaper presents a novel approach to converting Bangla natural language text into\nUNL using a method known as Predicate Preserving Parser (PPP) technique. PPP\nperforms morphological, syntactic and semantic, and lexical analysis of text\nsynchronously. This analysis produces a semantic-net like structure represented\nusing UNL. We demonstrate how Bangla texts are analyzed following the PPP\ntechnique to produce UNL documents which can then be translated into any other\nsuitable natural language facilitating the opportunity to develop a universal\nlanguage translation method via UNL."}, "authors": ["Md. Nawab Yousuf Ali", "Shamim Ripon", "Shaikh Muhammad Allayear"], "author_detail": {"name": "Shaikh Muhammad Allayear"}, "author": "Shaikh Muhammad Allayear", "arxiv_comment": "7 pages, International Journal of Computer Science Issues (IJCSI),\n  Volume 9, Issue 3 May 2012", "links": [{"href": "http://arxiv.org/abs/1206.0381v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1206.0381v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1206.0381v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1206.0381v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1310.1426v1", "guidislink": true, "updated": "2013-10-05T00:39:02Z", "updated_parsed": [2013, 10, 5, 0, 39, 2, 5, 278, 0], "published": "2013-10-05T00:39:02Z", "published_parsed": [2013, 10, 5, 0, 39, 2, 5, 278, 0], "title": "Local Feature or Mel Frequency Cepstral Coefficients - Which One is\n  Better for MLN-Based Bangla Speech Recognition?", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=30&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Local Feature or Mel Frequency Cepstral Coefficients - Which One is\n  Better for MLN-Based Bangla Speech Recognition?"}, "summary": "This paper discusses the dominancy of local features (LFs), as input to the\nmultilayer neural network (MLN), extracted from a Bangla input speech over mel\nfrequency cepstral coefficients (MFCCs). Here, LF-based method comprises three\nstages: (i) LF extraction from input speech, (ii) phoneme probabilities\nextraction using MLN from LF and (iii) the hidden Markov model (HMM) based\nclassifier to obtain more accurate phoneme strings. In the experiments on\nBangla speech corpus prepared by us, it is observed that the LFbased automatic\nspeech recognition (ASR) system provides higher phoneme correct rate than the\nMFCC-based system. Moreover, the proposed system requires fewer mixture\ncomponents in the HMMs.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=30&max_results=10&sortBy=relevance&sortOrder=descending", "value": "This paper discusses the dominancy of local features (LFs), as input to the\nmultilayer neural network (MLN), extracted from a Bangla input speech over mel\nfrequency cepstral coefficients (MFCCs). Here, LF-based method comprises three\nstages: (i) LF extraction from input speech, (ii) phoneme probabilities\nextraction using MLN from LF and (iii) the hidden Markov model (HMM) based\nclassifier to obtain more accurate phoneme strings. In the experiments on\nBangla speech corpus prepared by us, it is observed that the LFbased automatic\nspeech recognition (ASR) system provides higher phoneme correct rate than the\nMFCC-based system. Moreover, the proposed system requires fewer mixture\ncomponents in the HMMs."}, "authors": ["Foyzul Hassan", "Mohammed Rokibul Alam Kotwal", "Md. Mostafizur Rahman", "Mohammad Nasiruddin", "Md. Abdul Latif", "Mohammad Nurul Huda"], "author_detail": {"name": "Mohammad Nurul Huda"}, "author": "Mohammad Nurul Huda", "arxiv_comment": "9 pages Advances in Computing and Communications (ACC) 2011", "links": [{"href": "http://arxiv.org/abs/1310.1426v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1310.1426v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "68T50", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "I.2.7", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1310.1426v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1310.1426v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1911.11062v1", "guidislink": true, "updated": "2019-11-19T20:37:03Z", "updated_parsed": [2019, 11, 19, 20, 37, 3, 1, 323, 0], "published": "2019-11-19T20:37:03Z", "published_parsed": [2019, 11, 19, 20, 37, 3, 1, 323, 0], "title": "Automatic Detection of Satire in Bangla Documents: A CNN Approach Based\n  on Hybrid Feature Extraction Model", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=30&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Automatic Detection of Satire in Bangla Documents: A CNN Approach Based\n  on Hybrid Feature Extraction Model"}, "summary": "Widespread of satirical news in online communities is an ongoing trend. The\nnature of satires is so inherently ambiguous that sometimes it's too hard even\nfor humans to understand whether it's actually satire or not. So, research\ninterest has grown in this field. The purpose of this research is to detect\nBangla satirical news spread in online news portals as well as social media. In\nthis paper, we propose a hybrid technique for extracting features from text\ndocuments combining Word2Vec and TF-IDF. Using our proposed feature extraction\ntechnique, with standard CNN architecture we could detect whether a Bangla text\ndocument is satire or not with an accuracy of more than 96%.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=30&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Widespread of satirical news in online communities is an ongoing trend. The\nnature of satires is so inherently ambiguous that sometimes it's too hard even\nfor humans to understand whether it's actually satire or not. So, research\ninterest has grown in this field. The purpose of this research is to detect\nBangla satirical news spread in online news portals as well as social media. In\nthis paper, we propose a hybrid technique for extracting features from text\ndocuments combining Word2Vec and TF-IDF. Using our proposed feature extraction\ntechnique, with standard CNN architecture we could detect whether a Bangla text\ndocument is satire or not with an accuracy of more than 96%."}, "authors": ["Arnab Sen Sharma", "Maruf Ahmed Mridul", "Md Saiful Islam"], "author_detail": {"name": "Md Saiful Islam"}, "author": "Md Saiful Islam", "arxiv_comment": "5 pages, Conference paper", "links": [{"href": "http://arxiv.org/abs/1911.11062v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1911.11062v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.IR", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.IR", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1911.11062v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1911.11062v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/2001.05316v1", "guidislink": true, "updated": "2020-01-11T14:54:04Z", "updated_parsed": [2020, 1, 11, 14, 54, 4, 5, 11, 0], "published": "2020-01-11T14:54:04Z", "published_parsed": [2020, 1, 11, 14, 54, 4, 5, 11, 0], "title": "Authorship Attribution in Bangla literature using Character-level CNN", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=30&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Authorship Attribution in Bangla literature using Character-level CNN"}, "summary": "Characters are the smallest unit of text that can extract stylometric signals\nto determine the author of a text. In this paper, we investigate the\neffectiveness of character-level signals in Authorship Attribution of Bangla\nLiterature and show that the results are promising but improvable. The time and\nmemory efficiency of the proposed model is much higher than the word level\ncounterparts but accuracy is 2-5% less than the best performing word-level\nmodels. Comparison of various word-based models is performed and shown that the\nproposed model performs increasingly better with larger datasets. We also\nanalyze the effect of pre-training character embedding of diverse Bangla\ncharacter set in authorship attribution. It is seen that the performance is\nimproved by up to 10% on pre-training. We used 2 datasets from 6 to 14 authors,\nbalancing them before training and compare the results.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=30&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Characters are the smallest unit of text that can extract stylometric signals\nto determine the author of a text. In this paper, we investigate the\neffectiveness of character-level signals in Authorship Attribution of Bangla\nLiterature and show that the results are promising but improvable. The time and\nmemory efficiency of the proposed model is much higher than the word level\ncounterparts but accuracy is 2-5% less than the best performing word-level\nmodels. Comparison of various word-based models is performed and shown that the\nproposed model performs increasingly better with larger datasets. We also\nanalyze the effect of pre-training character embedding of diverse Bangla\ncharacter set in authorship attribution. It is seen that the performance is\nimproved by up to 10% on pre-training. We used 2 datasets from 6 to 14 authors,\nbalancing them before training and compare the results."}, "authors": ["Aisha Khatun", "Anisur Rahman", "Md. Saiful Islam", "Marium-E-Jannat"], "author_detail": {"name": "Marium-E-Jannat"}, "author": "Marium-E-Jannat", "arxiv_comment": "5 pages", "links": [{"href": "http://arxiv.org/abs/2001.05316v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2001.05316v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2001.05316v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2001.05316v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/2004.08789v1", "guidislink": true, "updated": "2020-04-19T07:42:22Z", "updated_parsed": [2020, 4, 19, 7, 42, 22, 6, 110, 0], "published": "2020-04-19T07:42:22Z", "published_parsed": [2020, 4, 19, 7, 42, 22, 6, 110, 0], "title": "BanFakeNews: A Dataset for Detecting Fake News in Bangla", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=30&max_results=10&sortBy=relevance&sortOrder=descending", "value": "BanFakeNews: A Dataset for Detecting Fake News in Bangla"}, "summary": "Observing the damages that can be done by the rapid propagation of fake news\nin various sectors like politics and finance, automatic identification of fake\nnews using linguistic analysis has drawn the attention of the research\ncommunity. However, such methods are largely being developed for English where\nlow resource languages remain out of the focus. But the risks spawned by fake\nand manipulative news are not confined by languages. In this work, we propose\nan annotated dataset of ~50K news that can be used for building automated fake\nnews detection systems for a low resource language like Bangla. Additionally,\nwe provide an analysis of the dataset and develop a benchmark system with state\nof the art NLP techniques to identify Bangla fake news. To create this system,\nwe explore traditional linguistic features and neural network based methods. We\nexpect this dataset will be a valuable resource for building technologies to\nprevent the spreading of fake news and contribute in research with low resource\nlanguages.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=30&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Observing the damages that can be done by the rapid propagation of fake news\nin various sectors like politics and finance, automatic identification of fake\nnews using linguistic analysis has drawn the attention of the research\ncommunity. However, such methods are largely being developed for English where\nlow resource languages remain out of the focus. But the risks spawned by fake\nand manipulative news are not confined by languages. In this work, we propose\nan annotated dataset of ~50K news that can be used for building automated fake\nnews detection systems for a low resource language like Bangla. Additionally,\nwe provide an analysis of the dataset and develop a benchmark system with state\nof the art NLP techniques to identify Bangla fake news. To create this system,\nwe explore traditional linguistic features and neural network based methods. We\nexpect this dataset will be a valuable resource for building technologies to\nprevent the spreading of fake news and contribute in research with low resource\nlanguages."}, "authors": ["Md Zobaer Hossain", "Md Ashraful Rahman", "Md Saiful Islam", "Sudipta Kar"], "author_detail": {"name": "Sudipta Kar"}, "author": "Sudipta Kar", "arxiv_comment": "LREC 2020", "links": [{"href": "http://arxiv.org/abs/2004.08789v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2004.08789v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2004.08789v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2004.08789v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/2010.13404v2", "guidislink": true, "updated": "2021-01-12T13:51:27Z", "updated_parsed": [2021, 1, 12, 13, 51, 27, 1, 12, 0], "published": "2020-10-26T08:00:48Z", "published_parsed": [2020, 10, 26, 8, 0, 48, 0, 300, 0], "title": "Robust and Consistent Estimation of Word Embedding for Bangla Language\n  by fine-tuning Word2Vec Model", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=30&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Robust and Consistent Estimation of Word Embedding for Bangla Language\n  by fine-tuning Word2Vec Model"}, "summary": "In recent times, data is growing rapidly in every domain such as news, social\nmedia, banking, education, etc. Due to the excessiveness of data, there is a\nneed for an automatic keyword extractor that can help to summarize the data.\nKeyword extraction is a text analysis technique that consists of automatically\nextracting the most important words and expressions in a text. It helps\nsummarize the content of a text and recognize the main topics which are being\ndiscussed. Earlier works regarding this topic have been done but no significant\nwork was done for the Bangla language. So, we tried to achieve the same things\nwhich could be done with other languages in the Bangla language.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=30&max_results=10&sortBy=relevance&sortOrder=descending", "value": "In recent times, data is growing rapidly in every domain such as news, social\nmedia, banking, education, etc. Due to the excessiveness of data, there is a\nneed for an automatic keyword extractor that can help to summarize the data.\nKeyword extraction is a text analysis technique that consists of automatically\nextracting the most important words and expressions in a text. It helps\nsummarize the content of a text and recognize the main topics which are being\ndiscussed. Earlier works regarding this topic have been done but no significant\nwork was done for the Bangla language. So, we tried to achieve the same things\nwhich could be done with other languages in the Bangla language."}, "authors": ["Rifat Rahman"], "author_detail": {"name": "Rifat Rahman"}, "author": "Rifat Rahman", "arxiv_comment": "I have implemented some approaches that are wrong. Now I am fixing\n  these issues. The methodology used my previous script may be harmful for\n  relevant researchers", "links": [{"href": "http://arxiv.org/abs/2010.13404v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2010.13404v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2010.13404v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2010.13404v2", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/2011.04446v1", "guidislink": true, "updated": "2020-11-09T14:12:07Z", "updated_parsed": [2020, 11, 9, 14, 12, 7, 0, 314, 0], "published": "2020-11-09T14:12:07Z", "published_parsed": [2020, 11, 9, 14, 12, 7, 0, 314, 0], "title": "Bangla Text Classification using Transformers", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=30&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Bangla Text Classification using Transformers"}, "summary": "Text classification has been one of the earliest problems in NLP. Over time\nthe scope of application areas has broadened and the difficulty of dealing with\nnew areas (e.g., noisy social media content) has increased. The problem-solving\nstrategy switched from classical machine learning to deep learning algorithms.\nOne of the recent deep neural network architecture is the Transformer. Models\ndesigned with this type of network and its variants recently showed their\nsuccess in many downstream natural language processing tasks, especially for\nresource-rich languages, e.g., English. However, these models have not been\nexplored fully for Bangla text classification tasks. In this work, we fine-tune\nmultilingual transformer models for Bangla text classification tasks in\ndifferent domains, including sentiment analysis, emotion detection, news\ncategorization, and authorship attribution. We obtain the state of the art\nresults on six benchmark datasets, improving upon the previous results by 5-29%\naccuracy across different tasks.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=30&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Text classification has been one of the earliest problems in NLP. Over time\nthe scope of application areas has broadened and the difficulty of dealing with\nnew areas (e.g., noisy social media content) has increased. The problem-solving\nstrategy switched from classical machine learning to deep learning algorithms.\nOne of the recent deep neural network architecture is the Transformer. Models\ndesigned with this type of network and its variants recently showed their\nsuccess in many downstream natural language processing tasks, especially for\nresource-rich languages, e.g., English. However, these models have not been\nexplored fully for Bangla text classification tasks. In this work, we fine-tune\nmultilingual transformer models for Bangla text classification tasks in\ndifferent domains, including sentiment analysis, emotion detection, news\ncategorization, and authorship attribution. We obtain the state of the art\nresults on six benchmark datasets, improving upon the previous results by 5-29%\naccuracy across different tasks."}, "authors": ["Tanvirul Alam", "Akib Khan", "Firoj Alam"], "author_detail": {"name": "Firoj Alam"}, "author": "Firoj Alam", "links": [{"href": "http://arxiv.org/abs/2011.04446v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2011.04446v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2011.04446v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2011.04446v1", "arxiv_comment": null, "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/2011.07499v2", "guidislink": true, "updated": "2020-12-08T09:30:02Z", "updated_parsed": [2020, 12, 8, 9, 30, 2, 1, 343, 0], "published": "2020-11-15T11:08:53Z", "published_parsed": [2020, 11, 15, 11, 8, 53, 6, 320, 0], "title": "BanglaWriting: A multi-purpose offline Bangla handwriting dataset", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=30&max_results=10&sortBy=relevance&sortOrder=descending", "value": "BanglaWriting: A multi-purpose offline Bangla handwriting dataset"}, "summary": "This article presents a Bangla handwriting dataset named BanglaWriting that\ncontains single-page handwritings of 260 individuals of different personalities\nand ages. Each page includes bounding-boxes that bounds each word, along with\nthe unicode representation of the writing. This dataset contains 21,234 words\nand 32,787 characters in total. Moreover, this dataset includes 5,470 unique\nwords of Bangla vocabulary. Apart from the usual words, the dataset comprises\n261 comprehensible overwriting and 450 handwritten strikes and mistakes. All of\nthe bounding-boxes and word labels are manually-generated. The dataset can be\nused for complex optical character/word recognition, writer identification,\nhandwritten word segmentation, and word generation. Furthermore, this dataset\nis suitable for extracting age-based and gender-based variation of handwriting.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=30&max_results=10&sortBy=relevance&sortOrder=descending", "value": "This article presents a Bangla handwriting dataset named BanglaWriting that\ncontains single-page handwritings of 260 individuals of different personalities\nand ages. Each page includes bounding-boxes that bounds each word, along with\nthe unicode representation of the writing. This dataset contains 21,234 words\nand 32,787 characters in total. Moreover, this dataset includes 5,470 unique\nwords of Bangla vocabulary. Apart from the usual words, the dataset comprises\n261 comprehensible overwriting and 450 handwritten strikes and mistakes. All of\nthe bounding-boxes and word labels are manually-generated. The dataset can be\nused for complex optical character/word recognition, writer identification,\nhandwritten word segmentation, and word generation. Furthermore, this dataset\nis suitable for extracting age-based and gender-based variation of handwriting."}, "authors": ["M. F. Mridha", "Abu Quwsar Ohi", "M. Ameer Ali", "Mazedul Islam Emon", "Muhammad Mohsin Kabir"], "author_detail": {"name": "Muhammad Mohsin Kabir"}, "author": "Muhammad Mohsin Kabir", "arxiv_comment": "Accepted in journal Data in Brief. The dataset is available on\n  https://data.mendeley.com/datasets/r43wkvdk4w/", "links": [{"href": "http://arxiv.org/abs/2011.07499v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2011.07499v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2011.07499v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2011.07499v2", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1901.05613v1", "guidislink": true, "updated": "2019-01-17T04:27:34Z", "updated_parsed": [2019, 1, 17, 4, 27, 34, 3, 17, 0], "published": "2019-01-17T04:27:34Z", "published_parsed": [2019, 1, 17, 4, 27, 34, 3, 17, 0], "title": "Hand Sign to Bangla Speech: A Deep Learning in Vision based system for\n  Recognizing Hand Sign Digits and Generating Bangla Speech", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=30&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Hand Sign to Bangla Speech: A Deep Learning in Vision based system for\n  Recognizing Hand Sign Digits and Generating Bangla Speech"}, "summary": "Recent advancements in the field of computer vision with the help of deep\nneural networks have led us to explore and develop many existing challenges\nthat were once unattended due to the lack of necessary technologies. Hand\nSign/Gesture Recognition is one of the significant areas where the deep neural\nnetwork is making a substantial impact. In the last few years, a large number\nof researches has been conducted to recognize hand signs and hand gestures,\nwhich we aim to extend to our mother-tongue, Bangla (also known as Bengali).\nThe primary goal of our work is to make an automated tool to aid the people who\nare unable to speak. We developed a system that automatically detects hand sign\nbased digits and speaks out the result in Bangla language. According to the\nreport of the World Health Organization (WHO), 15% of people in the world live\nwith some kind of disabilities. Among them, individuals with communication\nimpairment such as speech disabilities experience substantial barrier in social\ninteraction. The proposed system can be invaluable to mitigate such a barrier.\nThe core of the system is built with a deep learning model which is based on\nconvolutional neural networks (CNN). The model classifies hand sign based\ndigits with 92% accuracy over validation data which ensures it a highly\ntrustworthy system. Upon classification of the digits, the resulting output is\nfed to the text to speech engine and the translator unit eventually which\ngenerates audio output in Bangla language. A web application to demonstrate our\ntool is available at http://bit.ly/signdigits2banglaspeech.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=30&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Recent advancements in the field of computer vision with the help of deep\nneural networks have led us to explore and develop many existing challenges\nthat were once unattended due to the lack of necessary technologies. Hand\nSign/Gesture Recognition is one of the significant areas where the deep neural\nnetwork is making a substantial impact. In the last few years, a large number\nof researches has been conducted to recognize hand signs and hand gestures,\nwhich we aim to extend to our mother-tongue, Bangla (also known as Bengali).\nThe primary goal of our work is to make an automated tool to aid the people who\nare unable to speak. We developed a system that automatically detects hand sign\nbased digits and speaks out the result in Bangla language. According to the\nreport of the World Health Organization (WHO), 15% of people in the world live\nwith some kind of disabilities. Among them, individuals with communication\nimpairment such as speech disabilities experience substantial barrier in social\ninteraction. The proposed system can be invaluable to mitigate such a barrier.\nThe core of the system is built with a deep learning model which is based on\nconvolutional neural networks (CNN). The model classifies hand sign based\ndigits with 92% accuracy over validation data which ensures it a highly\ntrustworthy system. Upon classification of the digits, the resulting output is\nfed to the text to speech engine and the translator unit eventually which\ngenerates audio output in Bangla language. A web application to demonstrate our\ntool is available at http://bit.ly/signdigits2banglaspeech."}, "authors": ["Shahjalal Ahmed", "Md. Rafiqul Islam", "Jahid Hassan", "Minhaz Uddin Ahmed", "Bilkis Jamal Ferdosi", "Sanjay Saha", "Md. Shopon"], "author_detail": {"name": "Md. Shopon"}, "author": "Md. Shopon", "links": [{"href": "http://arxiv.org/abs/1901.05613v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1901.05613v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1901.05613v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1901.05613v1", "arxiv_comment": null, "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1701.08706v1", "guidislink": true, "updated": "2017-01-27T12:54:52Z", "updated_parsed": [2017, 1, 27, 12, 54, 52, 4, 27, 0], "published": "2017-01-27T12:54:52Z", "published_parsed": [2017, 1, 27, 12, 54, 52, 4, 27, 0], "title": "Document Decomposition of Bangla Printed Text", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=30&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Document Decomposition of Bangla Printed Text"}, "summary": "Today all kind of information is getting digitized and along with all this\ndigitization, the huge archive of various kinds of documents is being digitized\ntoo. We know that, Optical Character Recognition is the method through which,\nnewspapers and other paper documents convert into digital resources. But, it is\na fact that this method works on texts only. As a result, if we try to process\nany document which contains non-textual zones, then we will get garbage texts\nas output. That is why; in order to digitize documents properly they should be\nprepossessed carefully. And while preprocessing, segmenting document in\ndifferent regions according to the category properly is most important. But,\nthe Optical Character Recognition processes available for Bangla language have\nno such algorithm that can categorize a newspaper/book page fully. So we worked\nto decompose a document into its several parts like headlines, sub headlines,\ncolumns, images etc. And if the input is skewed and rotated, then the input was\nalso deskewed and de-rotated. To decompose any Bangla document we found out the\nedges of the input image. Then we find out the horizontal and vertical area of\nevery pixel where it lies in. Later on the input image was cut according to\nthese areas. Then we pick each and every sub image and found out their\nheight-width ratio, line height. Then according to these values the sub images\nwere categorized. To deskew the image we found out the skew angle and de skewed\nthe image according to this angle. To de-rotate the image we used the line\nheight, matra line, pixel ratio of matra line.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=30&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Today all kind of information is getting digitized and along with all this\ndigitization, the huge archive of various kinds of documents is being digitized\ntoo. We know that, Optical Character Recognition is the method through which,\nnewspapers and other paper documents convert into digital resources. But, it is\na fact that this method works on texts only. As a result, if we try to process\nany document which contains non-textual zones, then we will get garbage texts\nas output. That is why; in order to digitize documents properly they should be\nprepossessed carefully. And while preprocessing, segmenting document in\ndifferent regions according to the category properly is most important. But,\nthe Optical Character Recognition processes available for Bangla language have\nno such algorithm that can categorize a newspaper/book page fully. So we worked\nto decompose a document into its several parts like headlines, sub headlines,\ncolumns, images etc. And if the input is skewed and rotated, then the input was\nalso deskewed and de-rotated. To decompose any Bangla document we found out the\nedges of the input image. Then we find out the horizontal and vertical area of\nevery pixel where it lies in. Later on the input image was cut according to\nthese areas. Then we pick each and every sub image and found out their\nheight-width ratio, line height. Then according to these values the sub images\nwere categorized. To deskew the image we found out the skew angle and de skewed\nthe image according to this angle. To de-rotate the image we used the line\nheight, matra line, pixel ratio of matra line."}, "authors": ["Md. Fahad Hasan", "Tasmin Afroz", "Sabir Ismail", "Md. Saiful Islam"], "author_detail": {"name": "Md. Saiful Islam"}, "author": "Md. Saiful Islam", "arxiv_comment": "6 pages", "links": [{"href": "http://arxiv.org/abs/1701.08706v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1701.08706v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1701.08706v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1701.08706v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1707.08398v1", "guidislink": true, "updated": "2017-07-26T12:03:39Z", "updated_parsed": [2017, 7, 26, 12, 3, 39, 2, 207, 0], "published": "2017-07-26T12:03:39Z", "published_parsed": [2017, 7, 26, 12, 3, 39, 2, 207, 0], "title": "A Harmony Search Based Wrapper Feature Selection Method for Holistic\n  Bangla word Recognition", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=40&max_results=10&sortBy=relevance&sortOrder=descending", "value": "A Harmony Search Based Wrapper Feature Selection Method for Holistic\n  Bangla word Recognition"}, "summary": "A lot of search approaches have been explored for the selection of features\nin pattern classification domain in order to discover significant subset of the\nfeatures which produces better accuracy. In this paper, we introduced a Harmony\nSearch (HS) algorithm based feature selection method for feature dimensionality\nreduction in handwritten Bangla word recognition problem. This algorithm has\nbeen implemented to reduce the feature dimensionality of a technique described\nin one of our previous papers by S. Bhowmik et al.[1]. In the said paper, a set\nof 65 elliptical features were computed for handwritten Bangla word recognition\npurpose and a recognition accuracy of 81.37% was achieved using Multi Layer\nPerceptron (MLP) classifier. In the present work, a subset containing 48\nfeatures (approximately 75% of said feature vector) has been selected by HS\nbased wrapper feature selection method which produces an accuracy rate of\n90.29%. Reasonable outcomes also validates that the introduced algorithm\nutilizes optimal number of features while showing higher classification\naccuracies when compared to two standard evolutionary algorithms like Genetic\nAlgorithm (GA), Particle Swarm Optimization (PSO) and statistical feature\ndimensionality reduction technique like Principal Component Analysis (PCA).\nThis confirms the suitability of HS algorithm to the holistic handwritten word\nrecognition problem.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=40&max_results=10&sortBy=relevance&sortOrder=descending", "value": "A lot of search approaches have been explored for the selection of features\nin pattern classification domain in order to discover significant subset of the\nfeatures which produces better accuracy. In this paper, we introduced a Harmony\nSearch (HS) algorithm based feature selection method for feature dimensionality\nreduction in handwritten Bangla word recognition problem. This algorithm has\nbeen implemented to reduce the feature dimensionality of a technique described\nin one of our previous papers by S. Bhowmik et al.[1]. In the said paper, a set\nof 65 elliptical features were computed for handwritten Bangla word recognition\npurpose and a recognition accuracy of 81.37% was achieved using Multi Layer\nPerceptron (MLP) classifier. In the present work, a subset containing 48\nfeatures (approximately 75% of said feature vector) has been selected by HS\nbased wrapper feature selection method which produces an accuracy rate of\n90.29%. Reasonable outcomes also validates that the introduced algorithm\nutilizes optimal number of features while showing higher classification\naccuracies when compared to two standard evolutionary algorithms like Genetic\nAlgorithm (GA), Particle Swarm Optimization (PSO) and statistical feature\ndimensionality reduction technique like Principal Component Analysis (PCA).\nThis confirms the suitability of HS algorithm to the holistic handwritten word\nrecognition problem."}, "authors": ["Supratim Das", "Pawan Kumar Singh", "Showmik Bhowmik", "Ram Sarkar", "Mita Nasipuri"], "author_detail": {"name": "Mita Nasipuri"}, "author": "Mita Nasipuri", "links": [{"href": "http://arxiv.org/abs/1707.08398v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1707.08398v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "68T10", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1707.08398v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1707.08398v1", "arxiv_comment": null, "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1501.05497v1", "guidislink": true, "updated": "2015-01-22T13:50:25Z", "updated_parsed": [2015, 1, 22, 13, 50, 25, 3, 22, 0], "published": "2015-01-22T13:50:25Z", "published_parsed": [2015, 1, 22, 13, 50, 25, 3, 22, 0], "title": "An Improved Feature Descriptor for Recognition of Handwritten Bangla\n  Alphabet", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=40&max_results=10&sortBy=relevance&sortOrder=descending", "value": "An Improved Feature Descriptor for Recognition of Handwritten Bangla\n  Alphabet"}, "summary": "Appropriate feature set for representation of pattern classes is one of the\nmost important aspects of handwritten character recognition. The effectiveness\nof features depends on the discriminating power of the features chosen to\nrepresent patterns of different classes. However, discriminatory features are\nnot easily measurable. Investigative experimentation is necessary for\nidentifying discriminatory features. In the present work we have identified a\nnew variation of feature set which significantly outperforms on handwritten\nBangla alphabet from the previously used feature set. 132 number of features in\nall viz. modified shadow features, octant and centroid features, distance based\nfeatures, quad tree based longest run features are used here. Using this\nfeature set the recognition performance increases sharply from the 75.05%\nobserved in our previous work [7], to 85.40% on 50 character classes with MLP\nbased classifier on the same dataset.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=40&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Appropriate feature set for representation of pattern classes is one of the\nmost important aspects of handwritten character recognition. The effectiveness\nof features depends on the discriminating power of the features chosen to\nrepresent patterns of different classes. However, discriminatory features are\nnot easily measurable. Investigative experimentation is necessary for\nidentifying discriminatory features. In the present work we have identified a\nnew variation of feature set which significantly outperforms on handwritten\nBangla alphabet from the previously used feature set. 132 number of features in\nall viz. modified shadow features, octant and centroid features, distance based\nfeatures, quad tree based longest run features are used here. Using this\nfeature set the recognition performance increases sharply from the 75.05%\nobserved in our previous work [7], to 85.40% on 50 character classes with MLP\nbased classifier on the same dataset."}, "authors": ["Nibaran Das", "Subhadip Basu", "Ram Sarkar", "Mahantapas Kundu", "Mita Nasipuri", "Dipak kumar Basu"], "author_detail": {"name": "Dipak kumar Basu"}, "author": "Dipak kumar Basu", "arxiv_comment": "In proceedings of ICSIP 2009, pp. 451 to 454, August 2009, Mysore,\n  India. arXiv admin note: substantial text overlap with arXiv:1203.0882,\n  arXiv:1002.4040, arXiv:1410.0478", "links": [{"href": "http://arxiv.org/abs/1501.05497v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1501.05497v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1501.05497v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1501.05497v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1712.01434v1", "guidislink": true, "updated": "2017-12-05T01:12:25Z", "updated_parsed": [2017, 12, 5, 1, 12, 25, 1, 339, 0], "published": "2017-12-05T01:12:25Z", "published_parsed": [2017, 12, 5, 1, 12, 25, 1, 339, 0], "title": "Zone-based Keyword Spotting in Bangla and Devanagari Documents", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=40&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Zone-based Keyword Spotting in Bangla and Devanagari Documents"}, "summary": "In this paper we present a word spotting system in text lines for offline\nIndic scripts such as Bangla (Bengali) and Devanagari. Recently, it was shown\nthat zone-wise recognition method improves the word recognition performance\nthan conventional full word recognition system in Indic scripts. Inspired with\nthis idea we consider the zone segmentation approach and use middle zone\ninformation to improve the traditional word spotting performance. To avoid the\nproblem of zone segmentation using heuristic approach, we propose here an HMM\nbased approach to segment the upper and lower zone components from the text\nline images. The candidate keywords are searched from a line without segmenting\ncharacters or words. Also, we propose a novel feature combining foreground and\nbackground information of text line images for keyword-spotting by character\nfiller models. A significant improvement in performance is noted by using both\nforeground and background information than their individual one. Pyramid\nHistogram of Oriented Gradient (PHOG) feature has been used in our word\nspotting framework. From the experiment, it has been noted that the proposed\nzone-segmentation based system outperforms traditional approaches of word\nspotting.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=40&max_results=10&sortBy=relevance&sortOrder=descending", "value": "In this paper we present a word spotting system in text lines for offline\nIndic scripts such as Bangla (Bengali) and Devanagari. Recently, it was shown\nthat zone-wise recognition method improves the word recognition performance\nthan conventional full word recognition system in Indic scripts. Inspired with\nthis idea we consider the zone segmentation approach and use middle zone\ninformation to improve the traditional word spotting performance. To avoid the\nproblem of zone segmentation using heuristic approach, we propose here an HMM\nbased approach to segment the upper and lower zone components from the text\nline images. The candidate keywords are searched from a line without segmenting\ncharacters or words. Also, we propose a novel feature combining foreground and\nbackground information of text line images for keyword-spotting by character\nfiller models. A significant improvement in performance is noted by using both\nforeground and background information than their individual one. Pyramid\nHistogram of Oriented Gradient (PHOG) feature has been used in our word\nspotting framework. From the experiment, it has been noted that the proposed\nzone-segmentation based system outperforms traditional approaches of word\nspotting."}, "authors": ["Ayan Kumar Bhunia", "Partha Pratim Roy", "Umapada Pal"], "author_detail": {"name": "Umapada Pal"}, "author": "Umapada Pal", "arxiv_comment": "Preprint Submitted", "links": [{"href": "http://arxiv.org/abs/1712.01434v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1712.01434v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1712.01434v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1712.01434v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1802.00671v1", "guidislink": true, "updated": "2018-02-02T13:06:43Z", "updated_parsed": [2018, 2, 2, 13, 6, 43, 4, 33, 0], "published": "2018-02-02T13:06:43Z", "published_parsed": [2018, 2, 2, 13, 6, 43, 4, 33, 0], "title": "Handwritten Isolated Bangla Compound Character Recognition: a new\n  benchmark using a novel deep learning approach", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=40&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Handwritten Isolated Bangla Compound Character Recognition: a new\n  benchmark using a novel deep learning approach"}, "summary": "In this work, a novel deep learning technique for the recognition of\nhandwritten Bangla isolated compound character is presented and a new benchmark\nof recognition accuracy on the CMATERdb 3.1.3.3 dataset is reported. Greedy\nlayer wise training of Deep Neural Network has helped to make significant\nstrides in various pattern recognition problems. We employ layerwise training\nto Deep Convolutional Neural Networks (DCNN) in a supervised fashion and\naugment the training process with the RMSProp algorithm to achieve faster\nconvergence. We compare results with those obtained from standard shallow\nlearning methods with predefined features, as well as standard DCNNs.\nSupervised layerwise trained DCNNs are found to outperform standard shallow\nlearning models such as Support Vector Machines as well as regular DCNNs of\nsimilar architecture by achieving error rate of 9.67% thereby setting a new\nbenchmark on the CMATERdb 3.1.3.3 with recognition accuracy of 90.33%,\nrepresenting an improvement of nearly 10%.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=40&max_results=10&sortBy=relevance&sortOrder=descending", "value": "In this work, a novel deep learning technique for the recognition of\nhandwritten Bangla isolated compound character is presented and a new benchmark\nof recognition accuracy on the CMATERdb 3.1.3.3 dataset is reported. Greedy\nlayer wise training of Deep Neural Network has helped to make significant\nstrides in various pattern recognition problems. We employ layerwise training\nto Deep Convolutional Neural Networks (DCNN) in a supervised fashion and\naugment the training process with the RMSProp algorithm to achieve faster\nconvergence. We compare results with those obtained from standard shallow\nlearning methods with predefined features, as well as standard DCNNs.\nSupervised layerwise trained DCNNs are found to outperform standard shallow\nlearning models such as Support Vector Machines as well as regular DCNNs of\nsimilar architecture by achieving error rate of 9.67% thereby setting a new\nbenchmark on the CMATERdb 3.1.3.3 with recognition accuracy of 90.33%,\nrepresenting an improvement of nearly 10%."}, "authors": ["Saikat Roy", "Nibaran Das", "Mahantapas Kundu", "Mita Nasipuri"], "author_detail": {"name": "Mita Nasipuri"}, "author": "Mita Nasipuri", "links": [{"title": "doi", "href": "http://dx.doi.org/10.1016/j.patrec.2017.03.004", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/1802.00671v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1802.00671v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1802.00671v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1802.00671v1", "arxiv_comment": null, "journal_reference": "Pattern Recognition Letters, Elsevier, Vol. 90, Pages 15-21, 2017", "doi": "10.1016/j.patrec.2017.03.004"}
{"id": "http://arxiv.org/abs/1806.08037v1", "guidislink": true, "updated": "2018-06-21T01:30:30Z", "updated_parsed": [2018, 6, 21, 1, 30, 30, 3, 172, 0], "published": "2018-06-21T01:30:30Z", "published_parsed": [2018, 6, 21, 1, 30, 30, 3, 172, 0], "title": "Pixel-level Reconstruction and Classification for Noisy Handwritten\n  Bangla Characters", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=40&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Pixel-level Reconstruction and Classification for Noisy Handwritten\n  Bangla Characters"}, "summary": "Classification techniques for images of handwritten characters are\nsusceptible to noise. Quadtrees can be an efficient representation for learning\nfrom sparse features. In this paper, we improve the effectiveness of\nprobabilistic quadtrees by using a pixel level classifier to extract the\ncharacter pixels and remove noise from handwritten character images. The pixel\nlevel denoiser (a deep belief network) uses the map responses obtained from a\npretrained CNN as features for reconstructing the characters eliminating noise.\nWe experimentally demonstrate the effectiveness of our approach by\nreconstructing and classifying a noisy version of handwritten Bangla Numeral\nand Basic Character datasets.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=40&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Classification techniques for images of handwritten characters are\nsusceptible to noise. Quadtrees can be an efficient representation for learning\nfrom sparse features. In this paper, we improve the effectiveness of\nprobabilistic quadtrees by using a pixel level classifier to extract the\ncharacter pixels and remove noise from handwritten character images. The pixel\nlevel denoiser (a deep belief network) uses the map responses obtained from a\npretrained CNN as features for reconstructing the characters eliminating noise.\nWe experimentally demonstrate the effectiveness of our approach by\nreconstructing and classifying a noisy version of handwritten Bangla Numeral\nand Basic Character datasets."}, "authors": ["Manohar Karki", "Qun Liu", "Robert DiBiano", "Saikat Basu", "Supratik Mukhopadhyay"], "author_detail": {"name": "Supratik Mukhopadhyay"}, "author": "Supratik Mukhopadhyay", "arxiv_comment": "Paper was accepted at the 16th International Conference on Frontiers\n  in Handwriting Recognition (ICFHR 2018)", "links": [{"href": "http://arxiv.org/abs/1806.08037v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1806.08037v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1806.08037v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1806.08037v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1907.07826v1", "guidislink": true, "updated": "2019-07-18T01:00:42Z", "updated_parsed": [2019, 7, 18, 1, 0, 42, 3, 199, 0], "published": "2019-07-18T01:00:42Z", "published_parsed": [2019, 7, 18, 1, 0, 42, 3, 199, 0], "title": "Comparison of Classical Machine Learning Approaches on Bangla Textual\n  Emotion Analysis", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=40&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Comparison of Classical Machine Learning Approaches on Bangla Textual\n  Emotion Analysis"}, "summary": "Detecting emotions from text is an extension of simple sentiment polarity\ndetection. Instead of considering only positive or negative sentiments,\nemotions are conveyed using more tangible manner; thus, they can be expressed\nas many shades of gray. This paper manifests the results of our experimentation\nfor fine-grained emotion analysis on Bangla text. We gathered and annotated a\ntext corpus consisting of user comments from several Facebook groups regarding\nsocio-economic and political issues, and we made efforts to extract the basic\nemotions (sadness, happiness, disgust, surprise, fear, anger) conveyed through\nthese comments. Finally, we compared the results of the five most popular\nclassical machine learning techniques namely Naive Bayes, Decision Tree,\nk-Nearest Neighbor (k-NN), Support Vector Machine (SVM) and K-Means Clustering\nwith several combinations of features. Our best model (SVM with a non-linear\nradial-basis function (RBF) kernel) achieved an overall average accuracy score\nof 52.98% and an F1 score (macro) of 0.3324", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=40&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Detecting emotions from text is an extension of simple sentiment polarity\ndetection. Instead of considering only positive or negative sentiments,\nemotions are conveyed using more tangible manner; thus, they can be expressed\nas many shades of gray. This paper manifests the results of our experimentation\nfor fine-grained emotion analysis on Bangla text. We gathered and annotated a\ntext corpus consisting of user comments from several Facebook groups regarding\nsocio-economic and political issues, and we made efforts to extract the basic\nemotions (sadness, happiness, disgust, surprise, fear, anger) conveyed through\nthese comments. Finally, we compared the results of the five most popular\nclassical machine learning techniques namely Naive Bayes, Decision Tree,\nk-Nearest Neighbor (k-NN), Support Vector Machine (SVM) and K-Means Clustering\nwith several combinations of features. Our best model (SVM with a non-linear\nradial-basis function (RBF) kernel) achieved an overall average accuracy score\nof 52.98% and an F1 score (macro) of 0.3324"}, "authors": ["Md. Ataur Rahman", "Md. Hanif Seddiqui"], "author_detail": {"name": "Md. Hanif Seddiqui"}, "author": "Md. Hanif Seddiqui", "links": [{"href": "http://arxiv.org/abs/1907.07826v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1907.07826v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1907.07826v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1907.07826v1", "arxiv_comment": null, "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1908.08987v1", "guidislink": true, "updated": "2019-08-11T08:01:58Z", "updated_parsed": [2019, 8, 11, 8, 1, 58, 6, 223, 0], "published": "2019-08-11T08:01:58Z", "published_parsed": [2019, 8, 11, 8, 1, 58, 6, 223, 0], "title": "PCGAN-CHAR: Progressively Trained Classifier Generative Adversarial\n  Networks for Classification of Noisy Handwritten Bangla Characters", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=40&max_results=10&sortBy=relevance&sortOrder=descending", "value": "PCGAN-CHAR: Progressively Trained Classifier Generative Adversarial\n  Networks for Classification of Noisy Handwritten Bangla Characters"}, "summary": "Due to the sparsity of features, noise has proven to be a great inhibitor in\nthe classification of handwritten characters. To combat this, most techniques\nperform denoising of the data before classification. In this paper, we\nconsolidate the approach by training an all-in-one model that is able to\nclassify even noisy characters. For classification, we progressively train a\nclassifier generative adversarial network on the characters from low to high\nresolution. We show that by learning the features at each resolution\nindependently a trained model is able to accurately classify characters even in\nthe presence of noise. We experimentally demonstrate the effectiveness of our\napproach by classifying noisy versions of MNIST, handwritten Bangla Numeral,\nand Basic Character datasets.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=40&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Due to the sparsity of features, noise has proven to be a great inhibitor in\nthe classification of handwritten characters. To combat this, most techniques\nperform denoising of the data before classification. In this paper, we\nconsolidate the approach by training an all-in-one model that is able to\nclassify even noisy characters. For classification, we progressively train a\nclassifier generative adversarial network on the characters from low to high\nresolution. We show that by learning the features at each resolution\nindependently a trained model is able to accurately classify characters even in\nthe presence of noise. We experimentally demonstrate the effectiveness of our\napproach by classifying noisy versions of MNIST, handwritten Bangla Numeral,\nand Basic Character datasets."}, "authors": ["Qun Liu", "Edward Collier", "Supratik Mukhopadhyay"], "author_detail": {"name": "Supratik Mukhopadhyay"}, "author": "Supratik Mukhopadhyay", "arxiv_comment": "Paper was accepted at the 21st International Conference on\n  Asia-Pacific Digital Libraries (ICADL 2019)", "links": [{"href": "http://arxiv.org/abs/1908.08987v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1908.08987v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.IR", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1908.08987v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1908.08987v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1912.11612v1", "guidislink": true, "updated": "2019-12-25T07:31:44Z", "updated_parsed": [2019, 12, 25, 7, 31, 44, 2, 359, 0], "published": "2019-12-25T07:31:44Z", "published_parsed": [2019, 12, 25, 7, 31, 44, 2, 359, 0], "title": "N-gram Statistical Stemmer for Bangla Corpus", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=40&max_results=10&sortBy=relevance&sortOrder=descending", "value": "N-gram Statistical Stemmer for Bangla Corpus"}, "summary": "Stemming is a process that can be utilized to trim inflected words to stem or\nroot form. It is useful for enhancing the retrieval effectiveness, especially\nfor text search in order to solve the mismatch problems. Previous research on\nBangla stemming mostly relied on eliminating multiple suffixes from a solitary\nword through a recursive rule based procedure to recover progressively\napplicable relative root. Our proposed system has enhanced the aforementioned\nexploration by actualizing one of the stemming algorithms called N-gram\nstemming. By utilizing an affiliation measure called dice coefficient, related\nsets of words are clustered depending on their character structure. The\nsmallest word in one cluster may be considered as the stem. We additionally\nanalyzed Affinity Propagation clustering algorithms with coefficient similarity\nas well as with median similarity. Our result indicates N-gram stemming\ntechniques to be effective in general which gave us around 87% accurate\nclusters.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=40&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Stemming is a process that can be utilized to trim inflected words to stem or\nroot form. It is useful for enhancing the retrieval effectiveness, especially\nfor text search in order to solve the mismatch problems. Previous research on\nBangla stemming mostly relied on eliminating multiple suffixes from a solitary\nword through a recursive rule based procedure to recover progressively\napplicable relative root. Our proposed system has enhanced the aforementioned\nexploration by actualizing one of the stemming algorithms called N-gram\nstemming. By utilizing an affiliation measure called dice coefficient, related\nsets of words are clustered depending on their character structure. The\nsmallest word in one cluster may be considered as the stem. We additionally\nanalyzed Affinity Propagation clustering algorithms with coefficient similarity\nas well as with median similarity. Our result indicates N-gram stemming\ntechniques to be effective in general which gave us around 87% accurate\nclusters."}, "authors": ["Rabeya Sadia", "Md Ataur Rahman", "Md Hanif Seddiqui"], "author_detail": {"name": "Md Hanif Seddiqui"}, "author": "Md Hanif Seddiqui", "links": [{"href": "http://arxiv.org/abs/1912.11612v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1912.11612v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.IR", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1912.11612v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1912.11612v1", "arxiv_comment": null, "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/2004.12769v1", "guidislink": true, "updated": "2020-04-27T13:18:58Z", "updated_parsed": [2020, 4, 27, 13, 18, 58, 0, 118, 0], "published": "2020-04-27T13:18:58Z", "published_parsed": [2020, 4, 27, 13, 18, 58, 0, 118, 0], "title": "A Skip-connected Multi-column Network for Isolated Handwritten Bangla\n  Character and Digit recognition", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=40&max_results=10&sortBy=relevance&sortOrder=descending", "value": "A Skip-connected Multi-column Network for Isolated Handwritten Bangla\n  Character and Digit recognition"}, "summary": "Finding local invariant patterns in handwrit-ten characters and/or digits for\noptical character recognition is a difficult task. Variations in writing styles\nfrom one person to another make this task challenging. We have proposed a\nnon-explicit feature extraction method using a multi-scale multi-column skip\nconvolutional neural network in this work. Local and global features extracted\nfrom different layers of the proposed architecture are combined to derive the\nfinal feature descriptor encoding a character or digit image. Our method is\nevaluated on four publicly available datasets of isolated handwritten Bangla\ncharacters and digits. Exhaustive comparative analysis against contemporary\nmethods establishes the efficacy of our proposed approach.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=40&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Finding local invariant patterns in handwrit-ten characters and/or digits for\noptical character recognition is a difficult task. Variations in writing styles\nfrom one person to another make this task challenging. We have proposed a\nnon-explicit feature extraction method using a multi-scale multi-column skip\nconvolutional neural network in this work. Local and global features extracted\nfrom different layers of the proposed architecture are combined to derive the\nfinal feature descriptor encoding a character or digit image. Our method is\nevaluated on four publicly available datasets of isolated handwritten Bangla\ncharacters and digits. Exhaustive comparative analysis against contemporary\nmethods establishes the efficacy of our proposed approach."}, "authors": ["Animesh Singh", "Ritesh Sarkhel", "Nibaran Das", "Mahantapas Kundu", "Mita Nasipuri"], "author_detail": {"name": "Mita Nasipuri"}, "author": "Mita Nasipuri", "links": [{"href": "http://arxiv.org/abs/2004.12769v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2004.12769v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2004.12769v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2004.12769v1", "arxiv_comment": null, "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/2009.08037v1", "guidislink": true, "updated": "2020-09-17T03:14:27Z", "updated_parsed": [2020, 9, 17, 3, 14, 27, 3, 261, 0], "published": "2020-09-17T03:14:27Z", "published_parsed": [2020, 9, 17, 3, 14, 27, 3, 261, 0], "title": "Word Segmentation from Unconstrained Handwritten Bangla Document Images\n  using Distance Transform", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=40&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Word Segmentation from Unconstrained Handwritten Bangla Document Images\n  using Distance Transform"}, "summary": "Segmentation of handwritten document images into text lines and words is one\nof the most significant and challenging tasks in the development of a complete\nOptical Character Recognition (OCR) system. This paper addresses the automatic\nsegmentation of text words directly from unconstrained Bangla handwritten\ndocument images. The popular Distance transform (DT) algorithm is applied for\nlocating the outer boundary of the word images. This technique is free from\ngenerating the over-segmented words. A simple post-processing procedure is\napplied to isolate the under-segmented word images, if any. The proposed\ntechnique is tested on 50 random images taken from CMATERdb1.1.1 database.\nSatisfactory result is achieved with a segmentation accuracy of 91.88% which\nconfirms the robustness of the proposed methodology.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=40&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Segmentation of handwritten document images into text lines and words is one\nof the most significant and challenging tasks in the development of a complete\nOptical Character Recognition (OCR) system. This paper addresses the automatic\nsegmentation of text words directly from unconstrained Bangla handwritten\ndocument images. The popular Distance transform (DT) algorithm is applied for\nlocating the outer boundary of the word images. This technique is free from\ngenerating the over-segmented words. A simple post-processing procedure is\napplied to isolate the under-segmented word images, if any. The proposed\ntechnique is tested on 50 random images taken from CMATERdb1.1.1 database.\nSatisfactory result is achieved with a segmentation accuracy of 91.88% which\nconfirms the robustness of the proposed methodology."}, "authors": ["Pawan Kumar Singh", "Shubham Sinha", "Sagnik Pal Chowdhury", "Ram Sarkar", "Mita Nasipuri"], "author_detail": {"name": "Mita Nasipuri"}, "author": "Mita Nasipuri", "arxiv_comment": "12 pages, 5 figures, conference", "links": [{"href": "http://arxiv.org/abs/2009.08037v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2009.08037v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.MM", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "68U10, 68U15", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2009.08037v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2009.08037v1", "journal_reference": "7th International Conference on Advances in Communication, Network\n  and Computing (CNC),pp. 271-282, 2016", "doi": null}
{"id": "http://arxiv.org/abs/1206.0238v1", "guidislink": true, "updated": "2012-06-01T16:20:41Z", "updated_parsed": [2012, 6, 1, 16, 20, 41, 4, 153, 0], "published": "2012-06-01T16:20:41Z", "published_parsed": [2012, 6, 1, 16, 20, 41, 4, 153, 0], "title": "Rapid Feature Extraction for Optical Character Recognition", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=50&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Rapid Feature Extraction for Optical Character Recognition"}, "summary": "Feature extraction is one of the fundamental problems of character\nrecognition. The performance of character recognition system is depends on\nproper feature extraction and correct classifier selection. In this article, a\nrapid feature extraction method is proposed and named as Celled Projection (CP)\nthat compute the projection of each section formed through partitioning an\nimage. The recognition performance of the proposed method is compared with\nother widely used feature extraction methods that are intensively studied for\nmany different scripts in literature. The experiments have been conducted using\nBangla handwritten numerals along with three different well known classifiers\nwhich demonstrate comparable results including 94.12% recognition accuracy\nusing celled projection.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=50&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Feature extraction is one of the fundamental problems of character\nrecognition. The performance of character recognition system is depends on\nproper feature extraction and correct classifier selection. In this article, a\nrapid feature extraction method is proposed and named as Celled Projection (CP)\nthat compute the projection of each section formed through partitioning an\nimage. The recognition performance of the proposed method is compared with\nother widely used feature extraction methods that are intensively studied for\nmany different scripts in literature. The experiments have been conducted using\nBangla handwritten numerals along with three different well known classifiers\nwhich demonstrate comparable results including 94.12% recognition accuracy\nusing celled projection."}, "authors": ["M. Zahid Hossain", "M. Ashraful Amin", "Hong Yan"], "author_detail": {"name": "Hong Yan"}, "author": "Hong Yan", "arxiv_comment": "5 pages, 1 figure", "links": [{"href": "http://arxiv.org/abs/1206.0238v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1206.0238v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "I.5.2; I.7.5", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1206.0238v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1206.0238v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1701.08156v2", "guidislink": true, "updated": "2018-04-26T18:17:00Z", "updated_parsed": [2018, 4, 26, 18, 17, 0, 3, 116, 0], "published": "2017-01-27T12:38:47Z", "published_parsed": [2017, 1, 27, 12, 38, 47, 4, 27, 0], "title": "A Comprehensive Survey on Bengali Phoneme Recognition", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=50&max_results=10&sortBy=relevance&sortOrder=descending", "value": "A Comprehensive Survey on Bengali Phoneme Recognition"}, "summary": "Hidden Markov model based various phoneme recognition methods for Bengali\nlanguage is reviewed. Automatic phoneme recognition for Bengali language using\nmultilayer neural network is reviewed. Usefulness of multilayer neural network\nover single layer neural network is discussed. Bangla phonetic feature table\nconstruction and enhancement for Bengali speech recognition is also discussed.\nComparison among these methods is discussed.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=50&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Hidden Markov model based various phoneme recognition methods for Bengali\nlanguage is reviewed. Automatic phoneme recognition for Bengali language using\nmultilayer neural network is reviewed. Usefulness of multilayer neural network\nover single layer neural network is discussed. Bangla phonetic feature table\nconstruction and enhancement for Bengali speech recognition is also discussed.\nComparison among these methods is discussed."}, "authors": ["Sadia Tasnim Swarna", "Shamim Ehsan", "Md. Saiful Islam", "Marium E Jannat"], "author_detail": {"name": "Marium E Jannat"}, "author": "Marium E Jannat", "arxiv_comment": "7 pages, reference added in phoneme recognition methods", "links": [{"href": "http://arxiv.org/abs/1701.08156v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1701.08156v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.SD", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.SD", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1701.08156v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1701.08156v2", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/2008.07853v1", "guidislink": true, "updated": "2020-08-18T11:02:25Z", "updated_parsed": [2020, 8, 18, 11, 2, 25, 1, 231, 0], "published": "2020-08-18T11:02:25Z", "published_parsed": [2020, 8, 18, 11, 2, 25, 1, 231, 0], "title": "Image Pre-processing on NumtaDB for Bengali Handwritten Digit\n  Recognition", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=50&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Image Pre-processing on NumtaDB for Bengali Handwritten Digit\n  Recognition"}, "summary": "NumtaDB is by far the largest data-set collection for handwritten digits in\nBengali. This is a diverse dataset containing more than 85000 images. But this\ndiversity also makes this dataset very difficult to work with. The goal of this\npaper is to find the benchmark for pre-processed images which gives good\naccuracy on any machine learning models. The reason being, there are no\navailable pre-processed data for Bengali digit recognition to work with like\nthe English digits for MNIST.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=50&max_results=10&sortBy=relevance&sortOrder=descending", "value": "NumtaDB is by far the largest data-set collection for handwritten digits in\nBengali. This is a diverse dataset containing more than 85000 images. But this\ndiversity also makes this dataset very difficult to work with. The goal of this\npaper is to find the benchmark for pre-processed images which gives good\naccuracy on any machine learning models. The reason being, there are no\navailable pre-processed data for Bengali digit recognition to work with like\nthe English digits for MNIST."}, "authors": ["Ovi Paul"], "author_detail": {"name": "Ovi Paul"}, "author": "Ovi Paul", "links": [{"title": "doi", "href": "http://dx.doi.org/10.1109/ICBSLP.2018.8554910", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/2008.07853v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2008.07853v1", "rel": "related", "type": "application/pdf"}], "arxiv_comment": "5 pages, 8 figures and 4 tables", "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2008.07853v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2008.07853v1", "journal_reference": "2018 International Conference on Bangla Speech and Language\n  Processing (ICBSLP), Sylhet, 2018, pp. 1-6", "doi": "10.1109/ICBSLP.2018.8554910"}
{"id": "http://arxiv.org/abs/1501.05495v1", "guidislink": true, "updated": "2015-01-22T13:46:06Z", "updated_parsed": [2015, 1, 22, 13, 46, 6, 3, 22, 0], "published": "2015-01-22T13:46:06Z", "published_parsed": [2015, 1, 22, 13, 46, 6, 3, 22, 0], "title": "A GA Based approach for selection of local features for recognition of\n  handwritten Bangla numerals", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=50&max_results=10&sortBy=relevance&sortOrder=descending", "value": "A GA Based approach for selection of local features for recognition of\n  handwritten Bangla numerals"}, "summary": "Soft computing approaches are mainly designed to address the real world\nill-defined, imprecisely formulated problems, combining different kind of novel\nmodels of computation, such as neural networks, genetic algorithms (GAs.\nHandwritten digit recognition is a typical example of one such problem. In the\ncurrent work we have developed a two-pass approach where the first pass\nclassifier performs a coarse classification, based on some global features of\nthe input pattern by restricting the possibility of classification decisions\nwithin a group of classes, smaller than the number of classes considered\ninitially. In the second pass, the group specific classifiers concentrate on\nthe features extracted from the selected local regions, and refine the earlier\ndecision by combining the local and the global features for selecting the true\nclass of the input pattern from the group of candidate classes selected in the\nfirst pass. To optimize the selection of local regions a GA based approach has\nbeen developed here. The maximum recognition performance on Bangla digit\nsamples as achieved on the test set, during the first pass of the two pass\napproach is 93.35%. After combining the results of the two stage classifiers,\nan overall success rate of 95.25% is achieved.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=50&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Soft computing approaches are mainly designed to address the real world\nill-defined, imprecisely formulated problems, combining different kind of novel\nmodels of computation, such as neural networks, genetic algorithms (GAs.\nHandwritten digit recognition is a typical example of one such problem. In the\ncurrent work we have developed a two-pass approach where the first pass\nclassifier performs a coarse classification, based on some global features of\nthe input pattern by restricting the possibility of classification decisions\nwithin a group of classes, smaller than the number of classes considered\ninitially. In the second pass, the group specific classifiers concentrate on\nthe features extracted from the selected local regions, and refine the earlier\ndecision by combining the local and the global features for selecting the true\nclass of the input pattern from the group of candidate classes selected in the\nfirst pass. To optimize the selection of local regions a GA based approach has\nbeen developed here. The maximum recognition performance on Bangla digit\nsamples as achieved on the test set, during the first pass of the two pass\napproach is 93.35%. After combining the results of the two stage classifiers,\nan overall success rate of 95.25% is achieved."}, "authors": ["Nibaran Das", "Subhadip Basu", "Punam Kumar Saha", "Ram Sarkar", "Mahantapas Kundu", "Mita Nasipuri"], "author_detail": {"name": "Mita Nasipuri"}, "author": "Mita Nasipuri", "arxiv_comment": "In proceedings of UB NE ASEE 2009 conference, University of\n  Bridgeport, USA", "links": [{"href": "http://arxiv.org/abs/1501.05495v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1501.05495v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1501.05495v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1501.05495v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1605.00420v1", "guidislink": true, "updated": "2016-05-02T10:28:07Z", "updated_parsed": [2016, 5, 2, 10, 28, 7, 0, 123, 0], "published": "2016-05-02T10:28:07Z", "published_parsed": [2016, 5, 2, 10, 28, 7, 0, 123, 0], "title": "An Enhanced Harmony Search Method for Bangla Handwritten Character\n  Recognition Using Region Sampling", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=50&max_results=10&sortBy=relevance&sortOrder=descending", "value": "An Enhanced Harmony Search Method for Bangla Handwritten Character\n  Recognition Using Region Sampling"}, "summary": "Identification of minimum number of local regions of a handwritten character\nimage, containing well-defined discriminating features which are sufficient for\na minimal but complete description of the character is a challenging task. A\nnew region selection technique based on the idea of an enhanced Harmony Search\nmethodology has been proposed here. The powerful framework of Harmony Search\nhas been utilized to search the region space and detect only the most\ninformative regions for correctly recognizing the handwritten character. The\nproposed method has been tested on handwritten samples of Bangla Basic,\nCompound and mixed (Basic and Compound characters) characters separately with\nSVM based classifier using a longest run based feature-set obtained from the\nimage subregions formed by a CG based quad-tree partitioning approach. Applying\nthis methodology on the above mentioned three types of datasets, respectively\n43.75%, 12.5% and 37.5% gains have been achieved in terms of region reduction\nand 2.3%, 0.6% and 1.2% gains have been achieved in terms of recognition\naccuracy. The results show a sizeable reduction in the minimal number of\ndescriptive regions as well a significant increase in recognition accuracy for\nall the datasets using the proposed technique. Thus the time and cost related\nto feature extraction is decreased without dampening the corresponding\nrecognition accuracy.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=50&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Identification of minimum number of local regions of a handwritten character\nimage, containing well-defined discriminating features which are sufficient for\na minimal but complete description of the character is a challenging task. A\nnew region selection technique based on the idea of an enhanced Harmony Search\nmethodology has been proposed here. The powerful framework of Harmony Search\nhas been utilized to search the region space and detect only the most\ninformative regions for correctly recognizing the handwritten character. The\nproposed method has been tested on handwritten samples of Bangla Basic,\nCompound and mixed (Basic and Compound characters) characters separately with\nSVM based classifier using a longest run based feature-set obtained from the\nimage subregions formed by a CG based quad-tree partitioning approach. Applying\nthis methodology on the above mentioned three types of datasets, respectively\n43.75%, 12.5% and 37.5% gains have been achieved in terms of region reduction\nand 2.3%, 0.6% and 1.2% gains have been achieved in terms of recognition\naccuracy. The results show a sizeable reduction in the minimal number of\ndescriptive regions as well a significant increase in recognition accuracy for\nall the datasets using the proposed technique. Thus the time and cost related\nto feature extraction is decreased without dampening the corresponding\nrecognition accuracy."}, "authors": ["Ritesh Sarkhel", "Amit K Saha", "Nibaran Das"], "author_detail": {"name": "Nibaran Das"}, "author": "Nibaran Das", "links": [{"title": "doi", "href": "http://dx.doi.org/10.1109/ReTIS.2015.7232899", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/1605.00420v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1605.00420v1", "rel": "related", "type": "application/pdf"}], "arxiv_comment": "2nd IEEE International Conference on Recent Trends in Information\n  Systems, 2015", "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1605.00420v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1605.00420v1", "journal_reference": null, "doi": "10.1109/ReTIS.2015.7232899"}
{"id": "http://arxiv.org/abs/2101.05081v1", "guidislink": true, "updated": "2020-12-10T15:36:41Z", "updated_parsed": [2020, 12, 10, 15, 36, 41, 3, 345, 0], "published": "2020-12-10T15:36:41Z", "published_parsed": [2020, 12, 10, 15, 36, 41, 3, 345, 0], "title": "Deep Learning Approach Combining Lightweight CNN Architecture with\n  Transfer Learning: An Automatic Approach for the Detection and Recognition of\n  Bangladeshi Banknotes", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=50&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Deep Learning Approach Combining Lightweight CNN Architecture with\n  Transfer Learning: An Automatic Approach for the Detection and Recognition of\n  Bangladeshi Banknotes"}, "summary": "Automatic detection and recognition of banknotes can be a very useful\ntechnology for people with visual difficulties and also for the banks itself by\nproviding efficient management for handling different paper currencies.\nLightweight models can easily be integrated into any handy IoT based\ngadgets/devices. This article presents our experiments on several\nstate-of-the-art deep learning methods based on Lightweight Convolutional\nNeural Network architectures combining with transfer learning. ResNet152v2,\nMobileNet, and NASNetMobile were used as the base models with two different\ndatasets containing Bangladeshi banknote images. The Bangla Currency dataset\nhas 8000 Bangladeshi banknote images where the Bangla Money dataset consists of\n1970 images. The performances of the models were measured using both the\ndatasets and the combination of the two datasets. In order to achieve maximum\nefficiency, we used various augmentations, hyperparameter tuning, and\noptimizations techniques. We have achieved maximum test accuracy of 98.88\\% on\n8000 images dataset using MobileNet, 100\\% on the 1970 images dataset using\nNASNetMobile, and 97.77\\% on the combined dataset (9970 images) using\nMobileNet.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=50&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Automatic detection and recognition of banknotes can be a very useful\ntechnology for people with visual difficulties and also for the banks itself by\nproviding efficient management for handling different paper currencies.\nLightweight models can easily be integrated into any handy IoT based\ngadgets/devices. This article presents our experiments on several\nstate-of-the-art deep learning methods based on Lightweight Convolutional\nNeural Network architectures combining with transfer learning. ResNet152v2,\nMobileNet, and NASNetMobile were used as the base models with two different\ndatasets containing Bangladeshi banknote images. The Bangla Currency dataset\nhas 8000 Bangladeshi banknote images where the Bangla Money dataset consists of\n1970 images. The performances of the models were measured using both the\ndatasets and the combination of the two datasets. In order to achieve maximum\nefficiency, we used various augmentations, hyperparameter tuning, and\noptimizations techniques. We have achieved maximum test accuracy of 98.88\\% on\n8000 images dataset using MobileNet, 100\\% on the 1970 images dataset using\nNASNetMobile, and 97.77\\% on the combined dataset (9970 images) using\nMobileNet."}, "authors": ["Ali Hasan Md. Linkon", "Md. Mahir Labib", "Faisal Haque Bappy", "Soumik Sarker", "Marium-E-Jannat", "Md Saiful Islam"], "author_detail": {"name": "Md Saiful Islam"}, "author": "Md Saiful Islam", "arxiv_comment": "4 pages", "links": [{"href": "http://arxiv.org/abs/2101.05081v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2101.05081v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2101.05081v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2101.05081v1", "journal_reference": "2020 11th International Conference on Electrical and Computer\n  Engineering (ICECE)", "doi": null}
{"id": "http://arxiv.org/abs/1708.00227v1", "guidislink": true, "updated": "2017-08-01T09:52:03Z", "updated_parsed": [2017, 8, 1, 9, 52, 3, 1, 213, 0], "published": "2017-08-01T09:52:03Z", "published_parsed": [2017, 8, 1, 9, 52, 3, 1, 213, 0], "title": "HMM-based Indic Handwritten Word Recognition using Zone Segmentation", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=50&max_results=10&sortBy=relevance&sortOrder=descending", "value": "HMM-based Indic Handwritten Word Recognition using Zone Segmentation"}, "summary": "This paper presents a novel approach towards Indic handwritten word\nrecognition using zone-wise information. Because of complex nature due to\ncompound characters, modifiers, overlapping and touching, etc., character\nsegmentation and recognition is a tedious job in Indic scripts (e.g.\nDevanagari, Bangla, Gurumukhi, and other similar scripts). To avoid character\nsegmentation in such scripts, HMM-based sequence modeling has been used earlier\nin holistic way. This paper proposes an efficient word recognition framework by\nsegmenting the handwritten word images horizontally into three zones (upper,\nmiddle and lower) and recognize the corresponding zones. The main aim of this\nzone segmentation approach is to reduce the number of distinct component\nclasses compared to the total number of classes in Indic scripts. As a result,\nuse of this zone segmentation approach enhances the recognition performance of\nthe system. The components in middle zone where characters are mostly touching\nare recognized using HMM. After the recognition of middle zone, HMM based\nViterbi forced alignment is applied to mark the left and right boundaries of\nthe characters. Next, the residue components, if any, in upper and lower zones\nin their respective boundary are combined to achieve the final word level\nrecognition. Water reservoir feature has been integrated in this framework to\nimprove the zone segmentation and character alignment defects while\nsegmentation. A novel sliding window-based feature, called Pyramid Histogram of\nOriented Gradient (PHOG) is proposed for middle zone recognition. An exhaustive\nexperiment is performed on two Indic scripts namely, Bangla and Devanagari for\nthe performance evaluation. From the experiment, it has been noted that\nproposed zone-wise recognition improves accuracy with respect to the\ntraditional way of Indic word recognition.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=50&max_results=10&sortBy=relevance&sortOrder=descending", "value": "This paper presents a novel approach towards Indic handwritten word\nrecognition using zone-wise information. Because of complex nature due to\ncompound characters, modifiers, overlapping and touching, etc., character\nsegmentation and recognition is a tedious job in Indic scripts (e.g.\nDevanagari, Bangla, Gurumukhi, and other similar scripts). To avoid character\nsegmentation in such scripts, HMM-based sequence modeling has been used earlier\nin holistic way. This paper proposes an efficient word recognition framework by\nsegmenting the handwritten word images horizontally into three zones (upper,\nmiddle and lower) and recognize the corresponding zones. The main aim of this\nzone segmentation approach is to reduce the number of distinct component\nclasses compared to the total number of classes in Indic scripts. As a result,\nuse of this zone segmentation approach enhances the recognition performance of\nthe system. The components in middle zone where characters are mostly touching\nare recognized using HMM. After the recognition of middle zone, HMM based\nViterbi forced alignment is applied to mark the left and right boundaries of\nthe characters. Next, the residue components, if any, in upper and lower zones\nin their respective boundary are combined to achieve the final word level\nrecognition. Water reservoir feature has been integrated in this framework to\nimprove the zone segmentation and character alignment defects while\nsegmentation. A novel sliding window-based feature, called Pyramid Histogram of\nOriented Gradient (PHOG) is proposed for middle zone recognition. An exhaustive\nexperiment is performed on two Indic scripts namely, Bangla and Devanagari for\nthe performance evaluation. From the experiment, it has been noted that\nproposed zone-wise recognition improves accuracy with respect to the\ntraditional way of Indic word recognition."}, "authors": ["Partha Pratim Roy", "Ayan Kumar Bhunia", "Ayan Das", "Prasenjit Dey", "Umapada Pal"], "author_detail": {"name": "Umapada Pal"}, "author": "Umapada Pal", "links": [{"title": "doi", "href": "http://dx.doi.org/10.1016/j.patcog.2016.04.012", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/1708.00227v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1708.00227v1", "rel": "related", "type": "application/pdf"}], "arxiv_comment": "Published in Pattern Recognition(2016)", "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1708.00227v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1708.00227v1", "journal_reference": "Pattern Recognition, Volume 60, December 2016, Pages 1057-1075", "doi": "10.1016/j.patcog.2016.04.012"}
{"id": "http://arxiv.org/abs/1708.05529v6", "guidislink": true, "updated": "2018-07-30T10:41:30Z", "updated_parsed": [2018, 7, 30, 10, 41, 30, 0, 211, 0], "published": "2017-08-18T07:47:05Z", "published_parsed": [2017, 8, 18, 7, 47, 5, 4, 230, 0], "title": "Word Searching in Scene Image and Video Frame in Multi-Script Scenario\n  using Dynamic Shape Coding", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=50&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Word Searching in Scene Image and Video Frame in Multi-Script Scenario\n  using Dynamic Shape Coding"}, "summary": "Retrieval of text information from natural scene images and video frames is a\nchallenging task due to its inherent problems like complex character shapes,\nlow resolution, background noise, etc. Available OCR systems often fail to\nretrieve such information in scene/video frames. Keyword spotting, an\nalternative way to retrieve information, performs efficient text searching in\nsuch scenarios. However, current word spotting techniques in scene/video images\nare script-specific and they are mainly developed for Latin script. This paper\npresents a novel word spotting framework using dynamic shape coding for text\nretrieval in natural scene image and video frames. The framework is designed to\nsearch query keyword from multiple scripts with the help of on-the-fly\nscript-wise keyword generation for the corresponding script. We have used a\ntwo-stage word spotting approach using Hidden Markov Model (HMM) to detect the\ntranslated keyword in a given text line by identifying the script of the line.\nA novel unsupervised dynamic shape coding based scheme has been used to group\nsimilar shape characters to avoid confusion and to improve text alignment.\nNext, the hypotheses locations are verified to improve retrieval performance.\nTo evaluate the proposed system for searching keyword from natural scene image\nand video frames, we have considered two popular Indic scripts such as Bangla\n(Bengali) and Devanagari along with English. Inspired by the zone-wise\nrecognition approach in Indic scripts[1], zone-wise text information has been\nused to improve the traditional word spotting performance in Indic scripts. For\nour experiment, a dataset consisting of images of different scenes and video\nframes of English, Bangla and Devanagari scripts were considered. The results\nobtained showed the effectiveness of our proposed word spotting approach.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=50&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Retrieval of text information from natural scene images and video frames is a\nchallenging task due to its inherent problems like complex character shapes,\nlow resolution, background noise, etc. Available OCR systems often fail to\nretrieve such information in scene/video frames. Keyword spotting, an\nalternative way to retrieve information, performs efficient text searching in\nsuch scenarios. However, current word spotting techniques in scene/video images\nare script-specific and they are mainly developed for Latin script. This paper\npresents a novel word spotting framework using dynamic shape coding for text\nretrieval in natural scene image and video frames. The framework is designed to\nsearch query keyword from multiple scripts with the help of on-the-fly\nscript-wise keyword generation for the corresponding script. We have used a\ntwo-stage word spotting approach using Hidden Markov Model (HMM) to detect the\ntranslated keyword in a given text line by identifying the script of the line.\nA novel unsupervised dynamic shape coding based scheme has been used to group\nsimilar shape characters to avoid confusion and to improve text alignment.\nNext, the hypotheses locations are verified to improve retrieval performance.\nTo evaluate the proposed system for searching keyword from natural scene image\nand video frames, we have considered two popular Indic scripts such as Bangla\n(Bengali) and Devanagari along with English. Inspired by the zone-wise\nrecognition approach in Indic scripts[1], zone-wise text information has been\nused to improve the traditional word spotting performance in Indic scripts. For\nour experiment, a dataset consisting of images of different scenes and video\nframes of English, Bangla and Devanagari scripts were considered. The results\nobtained showed the effectiveness of our proposed word spotting approach."}, "authors": ["Partha Pratim Roy", "Ayan Kumar Bhunia", "Avirup Bhattacharyya", "Umapada Pal"], "author_detail": {"name": "Umapada Pal"}, "author": "Umapada Pal", "arxiv_comment": "Multimedia Tools and Applications, Springer", "links": [{"href": "http://arxiv.org/abs/1708.05529v6", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1708.05529v6", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.IR", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1708.05529v6", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1708.05529v6", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1009.4979v1", "guidislink": true, "updated": "2010-09-25T06:27:49Z", "updated_parsed": [2010, 9, 25, 6, 27, 49, 5, 268, 0], "published": "2010-09-25T06:27:49Z", "published_parsed": [2010, 9, 25, 6, 27, 49, 5, 268, 0], "title": "Smart Bengali Cell Phone Keypad Layout", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=50&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Smart Bengali Cell Phone Keypad Layout"}, "summary": "Nowadays cell phone is the most common communicating used by mass people. SMS\nbased communication is a cheap and popular communication method. It is human\ntendency to have the opportunity to write SMS in their mother language. Text\ninput in mother language is more flexible when the alphabets of that language\nare printed on the keypad. Bangla mobile keypad based on phonetics has been\nproposed earlier. But the keypad is not scientific from frequency and\nflexibility point of view. Since it is not a feasible solution in this paper we\nhave proposed an efficient Bengali keypad for cell phone and other cellular\ndevice. The proposed keypad is based on the frequency of the alphabets in\nBengali language and also with the view of structure of human finger movements.\nWe took the two points in count to provide a flexible and fast cell phone\nkeypad.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=50&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Nowadays cell phone is the most common communicating used by mass people. SMS\nbased communication is a cheap and popular communication method. It is human\ntendency to have the opportunity to write SMS in their mother language. Text\ninput in mother language is more flexible when the alphabets of that language\nare printed on the keypad. Bangla mobile keypad based on phonetics has been\nproposed earlier. But the keypad is not scientific from frequency and\nflexibility point of view. Since it is not a feasible solution in this paper we\nhave proposed an efficient Bengali keypad for cell phone and other cellular\ndevice. The proposed keypad is based on the frequency of the alphabets in\nBengali language and also with the view of structure of human finger movements.\nWe took the two points in count to provide a flexible and fast cell phone\nkeypad."}, "authors": ["Md. Abul Kalam Azad", "Rezwana Sharmeen", "Shabbir Ahmad", "S. M. Kamruzzaman"], "author_detail": {"name": "S. M. Kamruzzaman"}, "author": "S. M. Kamruzzaman", "arxiv_comment": "4 Pages, International Conference", "links": [{"href": "http://arxiv.org/abs/1009.4979v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1009.4979v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.HC", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.HC", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1009.4979v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1009.4979v1", "journal_reference": "Proc. 8th International Conference on Computer and Information\n  Technology (ICCIT 2005), Dhaka, Bangladesh, pp. 1208-1211, Dec. 2005", "doi": null}
{"id": "http://arxiv.org/abs/1410.4013v1", "guidislink": true, "updated": "2014-10-15T11:19:33Z", "updated_parsed": [2014, 10, 15, 11, 19, 33, 2, 288, 0], "published": "2014-10-15T11:19:33Z", "published_parsed": [2014, 10, 15, 11, 19, 33, 2, 288, 0], "title": "A two-pass fuzzy-geno approach to pattern classification", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=50&max_results=10&sortBy=relevance&sortOrder=descending", "value": "A two-pass fuzzy-geno approach to pattern classification"}, "summary": "The work presents an extension of the fuzzy approach to 2-D shape recognition\n[1] through refinement of initial or coarse classification decisions under a\ntwo pass approach. In this approach, an unknown pattern is classified by\nrefining possible classification decisions obtained through coarse\nclassification of the same. To build a fuzzy model of a pattern class\nhorizontal and vertical fuzzy partitions on the sample images of the class are\noptimized using genetic algorithm. To make coarse classification decisions\nabout an unknown pattern, the fuzzy representation of the pattern is compared\nwith models of all pattern classes through a specially designed similarity\nmeasure. Coarse classification decisions are refined in the second pass to\nobtain the final classification decision of the unknown pattern. To do so,\noptimized horizontal and vertical fuzzy partitions are again created on certain\nregions of the image frame, specific to each group of similar type of pattern\nclasses. It is observed through experiments that the technique improves the\noverall recognition rate from 86.2%, in the first pass, to 90.4% after the\nsecond pass, with 500 training samples of handwritten digits.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=50&max_results=10&sortBy=relevance&sortOrder=descending", "value": "The work presents an extension of the fuzzy approach to 2-D shape recognition\n[1] through refinement of initial or coarse classification decisions under a\ntwo pass approach. In this approach, an unknown pattern is classified by\nrefining possible classification decisions obtained through coarse\nclassification of the same. To build a fuzzy model of a pattern class\nhorizontal and vertical fuzzy partitions on the sample images of the class are\noptimized using genetic algorithm. To make coarse classification decisions\nabout an unknown pattern, the fuzzy representation of the pattern is compared\nwith models of all pattern classes through a specially designed similarity\nmeasure. Coarse classification decisions are refined in the second pass to\nobtain the final classification decision of the unknown pattern. To do so,\noptimized horizontal and vertical fuzzy partitions are again created on certain\nregions of the image frame, specific to each group of similar type of pattern\nclasses. It is observed through experiments that the technique improves the\noverall recognition rate from 86.2%, in the first pass, to 90.4% after the\nsecond pass, with 500 training samples of handwritten digits."}, "authors": ["Subhadip Basu", "Mahantapas Kundu", "Mita Nasipuri", "Dipak Kumar Basu"], "author_detail": {"name": "Dipak Kumar Basu"}, "author": "Dipak Kumar Basu", "links": [{"href": "http://arxiv.org/abs/1410.4013v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1410.4013v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1410.4013v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1410.4013v1", "arxiv_comment": null, "journal_reference": "Proc. of International Conference on Computer Processing of\n  Bangla, pp. 130-134, Feb-2006, Dhaka", "doi": null}
{"id": "http://arxiv.org/abs/1804.04475v1", "guidislink": true, "updated": "2018-04-12T12:46:08Z", "updated_parsed": [2018, 4, 12, 12, 46, 8, 3, 102, 0], "published": "2018-04-12T12:46:08Z", "published_parsed": [2018, 4, 12, 12, 46, 8, 3, 102, 0], "title": "Learning Multilingual Embeddings for Cross-Lingual Information Retrieval\n  in the Presence of Topically Aligned Corpora", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=60&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Learning Multilingual Embeddings for Cross-Lingual Information Retrieval\n  in the Presence of Topically Aligned Corpora"}, "summary": "Cross-lingual information retrieval is a challenging task in the absence of\naligned parallel corpora. In this paper, we address this problem by considering\ntopically aligned corpora designed for evaluating an IR setup. To emphasize, we\nneither use any sentence-aligned corpora or document-aligned corpora, nor do we\nuse any language specific resources such as dictionary, thesaurus, or grammar\nrules. Instead, we use an embedding into a common space and learn word\ncorrespondences directly from there. We test our proposed approach for\nbilingual IR on standard FIRE datasets for Bangla, Hindi and English. The\nproposed method is superior to the state-of-the-art method not only for IR\nevaluation measures but also in terms of time requirements. We extend our\nmethod successfully to the trilingual setting.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=60&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Cross-lingual information retrieval is a challenging task in the absence of\naligned parallel corpora. In this paper, we address this problem by considering\ntopically aligned corpora designed for evaluating an IR setup. To emphasize, we\nneither use any sentence-aligned corpora or document-aligned corpora, nor do we\nuse any language specific resources such as dictionary, thesaurus, or grammar\nrules. Instead, we use an embedding into a common space and learn word\ncorrespondences directly from there. We test our proposed approach for\nbilingual IR on standard FIRE datasets for Bangla, Hindi and English. The\nproposed method is superior to the state-of-the-art method not only for IR\nevaluation measures but also in terms of time requirements. We extend our\nmethod successfully to the trilingual setting."}, "authors": ["Mitodru Niyogi", "Kripabandhu Ghosh", "Arnab Bhattacharya"], "author_detail": {"name": "Arnab Bhattacharya"}, "author": "Arnab Bhattacharya", "links": [{"href": "http://arxiv.org/abs/1804.04475v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1804.04475v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.IR", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.IR", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1804.04475v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1804.04475v1", "arxiv_comment": null, "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1707.08385v1", "guidislink": true, "updated": "2017-07-26T11:40:13Z", "updated_parsed": [2017, 7, 26, 11, 40, 13, 2, 207, 0], "published": "2017-07-26T11:40:13Z", "published_parsed": [2017, 7, 26, 11, 40, 13, 2, 207, 0], "title": "A Novel Transfer Learning Approach upon Hindi, Arabic, and Bangla\n  Numerals using Convolutional Neural Networks", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=60&max_results=10&sortBy=relevance&sortOrder=descending", "value": "A Novel Transfer Learning Approach upon Hindi, Arabic, and Bangla\n  Numerals using Convolutional Neural Networks"}, "summary": "Increased accuracy in predictive models for handwritten character recognition\nwill open up new frontiers for optical character recognition. Major drawbacks\nof predictive machine learning models are headed by the elongated training time\ntaken by some models, and the requirement that training and test data be in the\nsame feature space and consist of the same distribution. In this study, these\nobstacles are minimized by presenting a model for transferring knowledge from\none task to another. This model is presented for the recognition of handwritten\nnumerals in Indic languages. The model utilizes convolutional neural networks\nwith backpropagation for error reduction and dropout for data overfitting. The\noutput performance of the proposed neural network is shown to have closely\nmatched other state-of-the-art methods using only a fraction of time used by\nthe state-of-the-arts.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=60&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Increased accuracy in predictive models for handwritten character recognition\nwill open up new frontiers for optical character recognition. Major drawbacks\nof predictive machine learning models are headed by the elongated training time\ntaken by some models, and the requirement that training and test data be in the\nsame feature space and consist of the same distribution. In this study, these\nobstacles are minimized by presenting a model for transferring knowledge from\none task to another. This model is presented for the recognition of handwritten\nnumerals in Indic languages. The model utilizes convolutional neural networks\nwith backpropagation for error reduction and dropout for data overfitting. The\noutput performance of the proposed neural network is shown to have closely\nmatched other state-of-the-art methods using only a fraction of time used by\nthe state-of-the-arts."}, "authors": ["Abdul Kawsar Tushar", "Akm Ashiquzzaman", "Afia Afrin", "Md. Rashedul Islam"], "author_detail": {"name": "Md. Rashedul Islam"}, "author": "Md. Rashedul Islam", "arxiv_comment": "10 pages; 2 figures, 4 tables; conference - International Conference\n  On Computational Vision and Bio Inspired Computing 2017 (http://iccvbic.com/)\n  (accepted)", "links": [{"href": "http://arxiv.org/abs/1707.08385v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1707.08385v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1707.08385v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1707.08385v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1902.11133v1", "guidislink": true, "updated": "2019-02-25T13:52:53Z", "updated_parsed": [2019, 2, 25, 13, 52, 53, 0, 56, 0], "published": "2019-02-25T13:52:53Z", "published_parsed": [2019, 2, 25, 13, 52, 53, 0, 56, 0], "title": "Bengali Handwritten Character Classification using Transfer Learning on\n  Deep Convolutional Neural Network", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=60&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Bengali Handwritten Character Classification using Transfer Learning on\n  Deep Convolutional Neural Network"}, "summary": "In this paper, we propose a solution which uses state-of-the-art techniques\nin Deep Learning to tackle the problem of Bengali Handwritten Character\nRecognition ( HCR ). Our method uses lesser iterations to train than most other\ncomparable methods. We employ Transfer Learning on ResNet 50, a\nstate-of-the-art deep Convolutional Neural Network Model, pretrained on\nImageNet dataset. We also use other techniques like a modified version of One\nCycle Policy, varying the input image sizes etc. to ensure that our training\noccurs fast. We use the BanglaLekha-Isolated Dataset for evaluation of our\ntechnique which consists of 84 classes (50 Basic, 10 Numerals and 24 Compound\nCharacters). We are able to achieve 96.12% accuracy in just 47 epochs on\nBanglaLekha-Isolated dataset. When comparing our method with that of other\nresearchers, considering number of classes and without using Ensemble Learning,\nthe proposed solution achieves state of the art result for Handwritten Bengali\nCharacter Recognition. Code and weight files are available at\nhttps://github.com/swagato-c/bangla-hwcr-present.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=60&max_results=10&sortBy=relevance&sortOrder=descending", "value": "In this paper, we propose a solution which uses state-of-the-art techniques\nin Deep Learning to tackle the problem of Bengali Handwritten Character\nRecognition ( HCR ). Our method uses lesser iterations to train than most other\ncomparable methods. We employ Transfer Learning on ResNet 50, a\nstate-of-the-art deep Convolutional Neural Network Model, pretrained on\nImageNet dataset. We also use other techniques like a modified version of One\nCycle Policy, varying the input image sizes etc. to ensure that our training\noccurs fast. We use the BanglaLekha-Isolated Dataset for evaluation of our\ntechnique which consists of 84 classes (50 Basic, 10 Numerals and 24 Compound\nCharacters). We are able to achieve 96.12% accuracy in just 47 epochs on\nBanglaLekha-Isolated dataset. When comparing our method with that of other\nresearchers, considering number of classes and without using Ensemble Learning,\nthe proposed solution achieves state of the art result for Handwritten Bengali\nCharacter Recognition. Code and weight files are available at\nhttps://github.com/swagato-c/bangla-hwcr-present."}, "authors": ["Swagato Chatterjee", "Rwik Kumar Dutta", "Debayan Ganguly", "Kingshuk Chatterjee", "Sudipta Roy"], "author_detail": {"name": "Sudipta Roy"}, "author": "Sudipta Roy", "links": [{"href": "http://arxiv.org/abs/1902.11133v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1902.11133v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1902.11133v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1902.11133v1", "arxiv_comment": null, "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1912.12405v2", "guidislink": true, "updated": "2020-03-16T17:06:44Z", "updated_parsed": [2020, 3, 16, 17, 6, 44, 0, 76, 0], "published": "2019-12-28T05:37:28Z", "published_parsed": [2019, 12, 28, 5, 37, 28, 5, 362, 0], "title": "A Genetic Algorithm based Kernel-size Selection Approach for a\n  Multi-column Convolutional Neural Network", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=60&max_results=10&sortBy=relevance&sortOrder=descending", "value": "A Genetic Algorithm based Kernel-size Selection Approach for a\n  Multi-column Convolutional Neural Network"}, "summary": "Deep neural network-based architectures give promising results in various\ndomains including pattern recognition. Finding the optimal combination of the\nhyper-parameters of such a large-sized architecture is tedious and requires a\nlarge number of laboratory experiments. But, identifying the optimal\ncombination of a hyper-parameter or appropriate kernel size for a given\narchitecture of deep learning is always a challenging and tedious task. Here,\nwe introduced a genetic algorithm-based technique to reduce the efforts of\nfinding the optimal combination of a hyper-parameter (kernel size) of a\nconvolutional neural network-based architecture. The method is evaluated on\nthree popular datasets of different handwritten Bangla characters and digits.\nThe implementation of the proposed methodology can be found in the following\nlink: https://github.com/DeepQn/GA-Based-Kernel-Size.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=60&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Deep neural network-based architectures give promising results in various\ndomains including pattern recognition. Finding the optimal combination of the\nhyper-parameters of such a large-sized architecture is tedious and requires a\nlarge number of laboratory experiments. But, identifying the optimal\ncombination of a hyper-parameter or appropriate kernel size for a given\narchitecture of deep learning is always a challenging and tedious task. Here,\nwe introduced a genetic algorithm-based technique to reduce the efforts of\nfinding the optimal combination of a hyper-parameter (kernel size) of a\nconvolutional neural network-based architecture. The method is evaluated on\nthree popular datasets of different handwritten Bangla characters and digits.\nThe implementation of the proposed methodology can be found in the following\nlink: https://github.com/DeepQn/GA-Based-Kernel-Size."}, "authors": ["Animesh Singh", "Sandip Saha", "Ritesh Sarkhel", "Mahantapas Kundu", "Mita Nasipuri", "Nibaran Das"], "author_detail": {"name": "Nibaran Das"}, "author": "Nibaran Das", "links": [{"href": "http://arxiv.org/abs/1912.12405v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1912.12405v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1912.12405v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1912.12405v2", "arxiv_comment": null, "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/2003.07428v1", "guidislink": true, "updated": "2020-03-16T20:19:21Z", "updated_parsed": [2020, 3, 16, 20, 19, 21, 0, 76, 0], "published": "2020-03-16T20:19:21Z", "published_parsed": [2020, 3, 16, 20, 19, 21, 0, 76, 0], "title": "Developing a Multilingual Annotated Corpus of Misogyny and Aggression", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=60&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Developing a Multilingual Annotated Corpus of Misogyny and Aggression"}, "summary": "In this paper, we discuss the development of a multilingual annotated corpus\nof misogyny and aggression in Indian English, Hindi, and Indian Bangla as part\nof a project on studying and automatically identifying misogyny and communalism\non social media (the ComMA Project). The dataset is collected from comments on\nYouTube videos and currently contains a total of over 20,000 comments. The\ncomments are annotated at two levels - aggression (overtly aggressive, covertly\naggressive, and non-aggressive) and misogyny (gendered and non-gendered). We\ndescribe the process of data collection, the tagset used for annotation, and\nissues and challenges faced during the process of annotation. Finally, we\ndiscuss the results of the baseline experiments conducted to develop a\nclassifier for misogyny in the three languages.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=60&max_results=10&sortBy=relevance&sortOrder=descending", "value": "In this paper, we discuss the development of a multilingual annotated corpus\nof misogyny and aggression in Indian English, Hindi, and Indian Bangla as part\nof a project on studying and automatically identifying misogyny and communalism\non social media (the ComMA Project). The dataset is collected from comments on\nYouTube videos and currently contains a total of over 20,000 comments. The\ncomments are annotated at two levels - aggression (overtly aggressive, covertly\naggressive, and non-aggressive) and misogyny (gendered and non-gendered). We\ndescribe the process of data collection, the tagset used for annotation, and\nissues and challenges faced during the process of annotation. Finally, we\ndiscuss the results of the baseline experiments conducted to develop a\nclassifier for misogyny in the three languages."}, "authors": ["Shiladitya Bhattacharya", "Siddharth Singh", "Ritesh Kumar", "Akanksha Bansal", "Akash Bhagat", "Yogesh Dawer", "Bornini Lahiri", "Atul Kr. Ojha"], "author_detail": {"name": "Atul Kr. Ojha"}, "author": "Atul Kr. Ojha", "arxiv_comment": "Submitted for review to Second Workshop on Trolling, Aggression and\n  Cyberbullying (TRAC 2020)", "links": [{"href": "http://arxiv.org/abs/2003.07428v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2003.07428v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2003.07428v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2003.07428v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/2003.08384v5", "guidislink": true, "updated": "2021-01-05T18:11:50Z", "updated_parsed": [2021, 1, 5, 18, 11, 50, 1, 5, 0], "published": "2020-03-18T17:58:05Z", "published_parsed": [2020, 3, 18, 17, 58, 5, 2, 78, 0], "title": "Confronting the Constraints for Optical Character Segmentation from\n  Printed Bangla Text Image", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=60&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Confronting the Constraints for Optical Character Segmentation from\n  Printed Bangla Text Image"}, "summary": "In a world of digitization, optical character recognition holds the\nautomation to written history. Optical character recognition system basically\nconverts printed images into editable texts for better storage and usability.\nTo be completely functional, the system needs to go through some crucial\nmethods such as pre-processing and segmentation. Pre-processing helps printed\ndata to be noise free and gets rid of skewness efficiently whereas segmentation\nhelps the image fragment into line, word and character precisely for better\nconversion. These steps hold the door to better accuracy and consistent results\nfor a printed image to be ready for conversion. Our proposed algorithm is able\nto segment characters both from ideal and non-ideal cases of scanned or\ncaptured images giving a sustainable outcome. The implementation of our work is\nprovided here: https://cutt.ly/rgdfBIa", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=60&max_results=10&sortBy=relevance&sortOrder=descending", "value": "In a world of digitization, optical character recognition holds the\nautomation to written history. Optical character recognition system basically\nconverts printed images into editable texts for better storage and usability.\nTo be completely functional, the system needs to go through some crucial\nmethods such as pre-processing and segmentation. Pre-processing helps printed\ndata to be noise free and gets rid of skewness efficiently whereas segmentation\nhelps the image fragment into line, word and character precisely for better\nconversion. These steps hold the door to better accuracy and consistent results\nfor a printed image to be ready for conversion. Our proposed algorithm is able\nto segment characters both from ideal and non-ideal cases of scanned or\ncaptured images giving a sustainable outcome. The implementation of our work is\nprovided here: https://cutt.ly/rgdfBIa"}, "authors": ["Abu Saleh Md. Abir", "Sanjana Rahman", "Samia Ellin", "Maisha Farzana", "Md Hridoy Manik", "Chowdhury Rafeed Rahman"], "author_detail": {"name": "Chowdhury Rafeed Rahman"}, "author": "Chowdhury Rafeed Rahman", "links": [{"title": "doi", "href": "http://dx.doi.org/10.1145/3428363.3428367", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/2003.08384v5", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2003.08384v5", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2003.08384v5", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2003.08384v5", "arxiv_comment": null, "journal_reference": null, "doi": "10.1145/3428363.3428367"}
{"id": "http://arxiv.org/abs/2004.01551v1", "guidislink": true, "updated": "2020-04-03T13:20:12Z", "updated_parsed": [2020, 4, 3, 13, 20, 12, 4, 94, 0], "published": "2020-04-03T13:20:12Z", "published_parsed": [2020, 4, 3, 13, 20, 12, 4, 94, 0], "title": "Sparse Concept Coded Tetrolet Transform for Unconstrained Odia Character\n  Recognition", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=60&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Sparse Concept Coded Tetrolet Transform for Unconstrained Odia Character\n  Recognition"}, "summary": "Feature representation in the form of spatio-spectral decomposition is one of\nthe robust techniques adopted in automatic handwritten character recognition\nsystems. In this regard, we propose a new image representation approach for\nunconstrained handwritten alphanumeric characters using sparse concept coded\nTetrolets. Tetrolets, which does not use fixed dyadic square blocks for\nspectral decomposition like conventional wavelets, preserve the localized\nvariations in handwritings by adopting tetrominoes those capture the shape\ngeometry. The sparse concept coding of low entropy Tetrolet representation is\nfound to extract the important hidden information (concept) for superior\npattern discrimination. Large scale experimentation using ten databases in six\ndifferent scripts (Bangla, Devanagari, Odia, English, Arabic and Telugu) has\nbeen performed. The proposed feature representation along with standard\nclassifiers such as random forest, support vector machine (SVM), nearest\nneighbor and modified quadratic discriminant function (MQDF) is found to\nachieve state-of-the-art recognition performance in all the databases, viz.\n99.40% (MNIST); 98.72% and 93.24% (IITBBS); 99.38% and 99.22% (ISI Kolkata).\nThe proposed OCR system is shown to perform better than other sparse based\ntechniques such as PCA, SparsePCA and SparseLDA, as well as better than\nexisting transforms (Wavelet, Slantlet and Stockwell).", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=60&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Feature representation in the form of spatio-spectral decomposition is one of\nthe robust techniques adopted in automatic handwritten character recognition\nsystems. In this regard, we propose a new image representation approach for\nunconstrained handwritten alphanumeric characters using sparse concept coded\nTetrolets. Tetrolets, which does not use fixed dyadic square blocks for\nspectral decomposition like conventional wavelets, preserve the localized\nvariations in handwritings by adopting tetrominoes those capture the shape\ngeometry. The sparse concept coding of low entropy Tetrolet representation is\nfound to extract the important hidden information (concept) for superior\npattern discrimination. Large scale experimentation using ten databases in six\ndifferent scripts (Bangla, Devanagari, Odia, English, Arabic and Telugu) has\nbeen performed. The proposed feature representation along with standard\nclassifiers such as random forest, support vector machine (SVM), nearest\nneighbor and modified quadratic discriminant function (MQDF) is found to\nachieve state-of-the-art recognition performance in all the databases, viz.\n99.40% (MNIST); 98.72% and 93.24% (IITBBS); 99.38% and 99.22% (ISI Kolkata).\nThe proposed OCR system is shown to perform better than other sparse based\ntechniques such as PCA, SparsePCA and SparseLDA, as well as better than\nexisting transforms (Wavelet, Slantlet and Stockwell)."}, "authors": ["Kalyan S Dash", "N B Puhan", "G Panda"], "author_detail": {"name": "G Panda"}, "author": "G Panda", "links": [{"href": "http://arxiv.org/abs/2004.01551v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2004.01551v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "eess.IV", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2004.01551v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2004.01551v1", "arxiv_comment": null, "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/2009.07435v1", "guidislink": true, "updated": "2020-09-16T02:50:03Z", "updated_parsed": [2020, 9, 16, 2, 50, 3, 2, 260, 0], "published": "2020-09-16T02:50:03Z", "published_parsed": [2020, 9, 16, 2, 50, 3, 2, 260, 0], "title": "A New Approach for Texture based Script Identification At Block Level\n  using Quad Tree Decomposition", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=60&max_results=10&sortBy=relevance&sortOrder=descending", "value": "A New Approach for Texture based Script Identification At Block Level\n  using Quad Tree Decomposition"}, "summary": "A considerable amount of success has been achieved in developing monolingual\nOCR systems for Indic scripts. But in a country like India, where multi-script\nscenario is prevalent, identifying scripts beforehand becomes obligatory. In\nthis paper, we present the significance of Gabor wavelets filters in extracting\ndirectional energy and entropy distributions for 11 official handwritten\nscripts namely, Bangla, Devanagari, Gujarati, Gurumukhi, Kannada, Malayalam,\nOriya, Tamil, Telugu, Urdu and Roman. The experimentation is conducted at block\nlevel based on a quad-tree decomposition approach and evaluated using six\ndifferent well-known classifiers. Finally, the best identification accuracy of\n96.86% has been achieved by Multi Layer Perceptron (MLP) classifier for 3-fold\ncross validation at level-2 decomposition. The results serve to establish the\nefficacy of the present approach to the classification of handwritten Indic\nscripts", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=60&max_results=10&sortBy=relevance&sortOrder=descending", "value": "A considerable amount of success has been achieved in developing monolingual\nOCR systems for Indic scripts. But in a country like India, where multi-script\nscenario is prevalent, identifying scripts beforehand becomes obligatory. In\nthis paper, we present the significance of Gabor wavelets filters in extracting\ndirectional energy and entropy distributions for 11 official handwritten\nscripts namely, Bangla, Devanagari, Gujarati, Gurumukhi, Kannada, Malayalam,\nOriya, Tamil, Telugu, Urdu and Roman. The experimentation is conducted at block\nlevel based on a quad-tree decomposition approach and evaluated using six\ndifferent well-known classifiers. Finally, the best identification accuracy of\n96.86% has been achieved by Multi Layer Perceptron (MLP) classifier for 3-fold\ncross validation at level-2 decomposition. The results serve to establish the\nefficacy of the present approach to the classification of handwritten Indic\nscripts"}, "authors": ["Pawan Kumar Singh", "Supratim Das", "Ram Sarkar", "Mita Nasipuri"], "author_detail": {"name": "Mita Nasipuri"}, "author": "Mita Nasipuri", "arxiv_comment": "13 pages, 5 figures, conference", "links": [{"href": "http://arxiv.org/abs/2009.07435v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2009.07435v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2009.07435v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2009.07435v1", "journal_reference": "7th International Conference on Advances in Communication, Network\n  and Computing (CNC), pp. 247-259, 2016", "doi": null}
{"id": "http://arxiv.org/abs/2010.03065v1", "guidislink": true, "updated": "2020-10-06T22:33:58Z", "updated_parsed": [2020, 10, 6, 22, 33, 58, 1, 280, 0], "published": "2020-10-06T22:33:58Z", "published_parsed": [2020, 10, 6, 22, 33, 58, 1, 280, 0], "title": "Anubhuti -- An annotated dataset for emotional analysis of Bengali short\n  stories", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=60&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Anubhuti -- An annotated dataset for emotional analysis of Bengali short\n  stories"}, "summary": "Thousands of short stories and articles are being written in many different\nlanguages all around the world today. Bengali, or Bangla, is the second highest\nspoken language in India after Hindi and is the national language of the\ncountry of Bangladesh. This work reports in detail the creation of Anubhuti --\nthe first and largest text corpus for analyzing emotions expressed by writers\nof Bengali short stories. We explain the data collection methods, the manual\nannotation process and the resulting high inter-annotator agreement of the\ndataset due to the linguistic expertise of the annotators and the clear\nmethodology of labelling followed. We also address some of the challenges faced\nin the collection of raw data and annotation process of a low resource language\nlike Bengali. We have verified the performance of our dataset with baseline\nMachine Learning as well as a Deep Learning model for emotion classification\nand have found that these standard models have a high accuracy and relevant\nfeature selection on Anubhuti. In addition, we also explain how this dataset\ncan be of interest to linguists and data analysts to study the flow of emotions\nas expressed by writers of Bengali literature.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=60&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Thousands of short stories and articles are being written in many different\nlanguages all around the world today. Bengali, or Bangla, is the second highest\nspoken language in India after Hindi and is the national language of the\ncountry of Bangladesh. This work reports in detail the creation of Anubhuti --\nthe first and largest text corpus for analyzing emotions expressed by writers\nof Bengali short stories. We explain the data collection methods, the manual\nannotation process and the resulting high inter-annotator agreement of the\ndataset due to the linguistic expertise of the annotators and the clear\nmethodology of labelling followed. We also address some of the challenges faced\nin the collection of raw data and annotation process of a low resource language\nlike Bengali. We have verified the performance of our dataset with baseline\nMachine Learning as well as a Deep Learning model for emotion classification\nand have found that these standard models have a high accuracy and relevant\nfeature selection on Anubhuti. In addition, we also explain how this dataset\ncan be of interest to linguists and data analysts to study the flow of emotions\nas expressed by writers of Bengali literature."}, "authors": ["Aditya Pal", "Bhaskar Karn"], "author_detail": {"name": "Bhaskar Karn"}, "author": "Bhaskar Karn", "arxiv_comment": "4 pages, 6 figures", "links": [{"href": "http://arxiv.org/abs/2010.03065v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2010.03065v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2010.03065v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2010.03065v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/2010.08066v1", "guidislink": true, "updated": "2020-10-15T23:24:15Z", "updated_parsed": [2020, 10, 15, 23, 24, 15, 3, 289, 0], "published": "2020-10-15T23:24:15Z", "published_parsed": [2020, 10, 15, 23, 24, 15, 3, 289, 0], "title": "TextMage: The Automated Bangla Caption Generator Based On Deep Learning", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=60&max_results=10&sortBy=relevance&sortOrder=descending", "value": "TextMage: The Automated Bangla Caption Generator Based On Deep Learning"}, "summary": "Neural Networks and Deep Learning have seen an upsurge of research in the\npast decade due to the improved results. Generates text from the given image is\na crucial task that requires the combination of both sectors which are computer\nvision and natural language processing in order to understand an image and\nrepresent it using a natural language. However existing works have all been\ndone on a particular lingual domain and on the same set of data. This leads to\nthe systems being developed to perform poorly on images that belong to specific\nlocales' geographical context. TextMage is a system that is capable of\nunderstanding visual scenes that belong to the Bangladeshi geographical context\nand use its knowledge to represent what it understands in Bengali. Hence, we\nhave trained a model on our previously developed and published dataset named\nBanglaLekhaImageCaptions. This dataset contains 9,154 images along with two\nannotations for each image. In order to access performance, the proposed model\nhas been implemented and evaluated.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=60&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Neural Networks and Deep Learning have seen an upsurge of research in the\npast decade due to the improved results. Generates text from the given image is\na crucial task that requires the combination of both sectors which are computer\nvision and natural language processing in order to understand an image and\nrepresent it using a natural language. However existing works have all been\ndone on a particular lingual domain and on the same set of data. This leads to\nthe systems being developed to perform poorly on images that belong to specific\nlocales' geographical context. TextMage is a system that is capable of\nunderstanding visual scenes that belong to the Bangladeshi geographical context\nand use its knowledge to represent what it understands in Bengali. Hence, we\nhave trained a model on our previously developed and published dataset named\nBanglaLekhaImageCaptions. This dataset contains 9,154 images along with two\nannotations for each image. In order to access performance, the proposed model\nhas been implemented and evaluated."}, "authors": ["Abrar Hasin Kamal", "Md. Asifuzzaman Jishan", "Nafees Mansoor"], "author_detail": {"name": "Nafees Mansoor"}, "author": "Nafees Mansoor", "arxiv_comment": "5 pages", "links": [{"href": "http://arxiv.org/abs/2010.08066v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2010.08066v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2010.08066v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2010.08066v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1804.06254v1", "guidislink": true, "updated": "2018-04-17T13:52:59Z", "updated_parsed": [2018, 4, 17, 13, 52, 59, 1, 107, 0], "published": "2018-04-17T13:52:59Z", "published_parsed": [2018, 4, 17, 13, 52, 59, 1, 107, 0], "title": "Synthetic data generation for Indic handwritten text recognition", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=70&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Synthetic data generation for Indic handwritten text recognition"}, "summary": "This paper presents a novel approach to generate synthetic dataset for\nhandwritten word recognition systems. It is difficult to recognize handwritten\nscripts for which sufficient training data is not readily available or it may\nbe expensive to collect such data. Hence, it becomes hard to train recognition\nsystems owing to lack of proper dataset. To overcome such problems, synthetic\ndata could be used to create or expand the existing training dataset to improve\nrecognition performance. Any available digital data from online newspaper and\nsuch sources can be used to generate synthetic data. In this paper, we propose\nto add distortion/deformation to digital data in such a way that the underlying\npattern is preserved, so that the image so produced bears a close similarity to\nactual handwritten samples. The images thus produced can be used independently\nto train the system or be combined with natural handwritten data to augment the\noriginal dataset and improve the recognition system. We experimented using\nsynthetic data to improve the recognition accuracy of isolated characters and\nwords. The framework is tested on 2 Indic scripts - Devanagari (Hindi) and\nBengali (Bangla), for numeral, character and word recognition. We have obtained\nencouraging results from the experiment. Finally, the experiment with Latin\ntext verifies the utility of the approach.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=70&max_results=10&sortBy=relevance&sortOrder=descending", "value": "This paper presents a novel approach to generate synthetic dataset for\nhandwritten word recognition systems. It is difficult to recognize handwritten\nscripts for which sufficient training data is not readily available or it may\nbe expensive to collect such data. Hence, it becomes hard to train recognition\nsystems owing to lack of proper dataset. To overcome such problems, synthetic\ndata could be used to create or expand the existing training dataset to improve\nrecognition performance. Any available digital data from online newspaper and\nsuch sources can be used to generate synthetic data. In this paper, we propose\nto add distortion/deformation to digital data in such a way that the underlying\npattern is preserved, so that the image so produced bears a close similarity to\nactual handwritten samples. The images thus produced can be used independently\nto train the system or be combined with natural handwritten data to augment the\noriginal dataset and improve the recognition system. We experimented using\nsynthetic data to improve the recognition accuracy of isolated characters and\nwords. The framework is tested on 2 Indic scripts - Devanagari (Hindi) and\nBengali (Bangla), for numeral, character and word recognition. We have obtained\nencouraging results from the experiment. Finally, the experiment with Latin\ntext verifies the utility of the approach."}, "authors": ["Partha Pratim Roy", "Akash Mohta", "Bidyut B. Chaudhuri"], "author_detail": {"name": "Bidyut B. Chaudhuri"}, "author": "Bidyut B. Chaudhuri", "links": [{"href": "http://arxiv.org/abs/1804.06254v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1804.06254v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1804.06254v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1804.06254v1", "arxiv_comment": null, "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1807.06772v1", "guidislink": true, "updated": "2018-07-18T04:29:20Z", "updated_parsed": [2018, 7, 18, 4, 29, 20, 2, 199, 0], "published": "2018-07-18T04:29:20Z", "published_parsed": [2018, 7, 18, 4, 29, 20, 2, 199, 0], "title": "Bag-of-Visual-Words for Signature-Based Multi-Script Document Retrieval", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=70&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Bag-of-Visual-Words for Signature-Based Multi-Script Document Retrieval"}, "summary": "An end-to-end architecture for multi-script document retrieval using\nhandwritten signatures is proposed in this paper. The user supplies a query\nsignature sample and the system exclusively returns a set of documents that\ncontain the query signature. In the first stage, a component-wise\nclassification technique separates the potential signature components from all\nother components. A bag-of-visual-words powered by SIFT descriptors in a\npatch-based framework is proposed to compute the features and a Support Vector\nMachine (SVM)-based classifier was used to separate signatures from the\ndocuments. In the second stage, features from the foreground (i.e. signature\nstrokes) and the background spatial information (i.e. background loops,\nreservoirs etc.) were combined to characterize the signature object to match\nwith the query signature. Finally, three distance measures were used to match a\nquery signature with the signature present in target documents for retrieval.\nThe `Tobacco' document database and an Indian script database containing 560\ndocuments of Devanagari (Hindi) and Bangla scripts were used for the\nperformance evaluation. The proposed system was also tested on noisy documents\nand promising results were obtained. A comparative study shows that the\nproposed method outperforms the state-of-the-art approaches.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=70&max_results=10&sortBy=relevance&sortOrder=descending", "value": "An end-to-end architecture for multi-script document retrieval using\nhandwritten signatures is proposed in this paper. The user supplies a query\nsignature sample and the system exclusively returns a set of documents that\ncontain the query signature. In the first stage, a component-wise\nclassification technique separates the potential signature components from all\nother components. A bag-of-visual-words powered by SIFT descriptors in a\npatch-based framework is proposed to compute the features and a Support Vector\nMachine (SVM)-based classifier was used to separate signatures from the\ndocuments. In the second stage, features from the foreground (i.e. signature\nstrokes) and the background spatial information (i.e. background loops,\nreservoirs etc.) were combined to characterize the signature object to match\nwith the query signature. Finally, three distance measures were used to match a\nquery signature with the signature present in target documents for retrieval.\nThe `Tobacco' document database and an Indian script database containing 560\ndocuments of Devanagari (Hindi) and Bangla scripts were used for the\nperformance evaluation. The proposed system was also tested on noisy documents\nand promising results were obtained. A comparative study shows that the\nproposed method outperforms the state-of-the-art approaches."}, "authors": ["Ranju Mandal", "Partha Pratim Roy", "Umapada Pal", "Michael Blumenstein"], "author_detail": {"name": "Michael Blumenstein"}, "author": "Michael Blumenstein", "links": [{"href": "http://arxiv.org/abs/1807.06772v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1807.06772v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1807.06772v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1807.06772v1", "arxiv_comment": null, "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1712.06908v2", "guidislink": true, "updated": "2018-01-28T15:10:48Z", "updated_parsed": [2018, 1, 28, 15, 10, 48, 6, 28, 0], "published": "2017-12-19T13:12:29Z", "published_parsed": [2017, 12, 19, 13, 12, 29, 1, 353, 0], "title": "Cross-language Framework for Word Recognition and Spotting of Indic\n  Scripts", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=70&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Cross-language Framework for Word Recognition and Spotting of Indic\n  Scripts"}, "summary": "Handwritten word recognition and spotting of low-resource scripts are\ndifficult as sufficient training data is not available and it is often\nexpensive for collecting data of such scripts. This paper presents a novel\ncross language platform for handwritten word recognition and spotting for such\nlow-resource scripts where training is performed with a sufficiently large\ndataset of an available script (considered as source script) and testing is\ndone on other scripts (considered as target script). Training with one source\nscript and testing with another script to have a reasonable result is not easy\nin handwriting domain due to the complex nature of handwriting variability\namong scripts. Also it is difficult in mapping between source and target\ncharacters when they appear in cursive word images. The proposed Indic cross\nlanguage framework exploits a large resource of dataset for training and uses\nit for recognizing and spotting text of other target scripts where sufficient\namount of training data is not available. Since, Indic scripts are mostly\nwritten in 3 zones, namely, upper, middle and lower, we employ zone-wise\ncharacter (or component) mapping for efficient learning purpose. The\nperformance of our cross-language framework depends on the extent of similarity\nbetween the source and target scripts. Hence, we devise an entropy based script\nsimilarity score using source to target character mapping that will provide a\nfeasibility of cross language transcription. We have tested our approach in\nthree Indic scripts, namely, Bangla, Devanagari and Gurumukhi, and the\ncorresponding results are reported.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=70&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Handwritten word recognition and spotting of low-resource scripts are\ndifficult as sufficient training data is not available and it is often\nexpensive for collecting data of such scripts. This paper presents a novel\ncross language platform for handwritten word recognition and spotting for such\nlow-resource scripts where training is performed with a sufficiently large\ndataset of an available script (considered as source script) and testing is\ndone on other scripts (considered as target script). Training with one source\nscript and testing with another script to have a reasonable result is not easy\nin handwriting domain due to the complex nature of handwriting variability\namong scripts. Also it is difficult in mapping between source and target\ncharacters when they appear in cursive word images. The proposed Indic cross\nlanguage framework exploits a large resource of dataset for training and uses\nit for recognizing and spotting text of other target scripts where sufficient\namount of training data is not available. Since, Indic scripts are mostly\nwritten in 3 zones, namely, upper, middle and lower, we employ zone-wise\ncharacter (or component) mapping for efficient learning purpose. The\nperformance of our cross-language framework depends on the extent of similarity\nbetween the source and target scripts. Hence, we devise an entropy based script\nsimilarity score using source to target character mapping that will provide a\nfeasibility of cross language transcription. We have tested our approach in\nthree Indic scripts, namely, Bangla, Devanagari and Gurumukhi, and the\ncorresponding results are reported."}, "authors": ["Ayan Kumar Bhunia", "Partha Pratim Roy", "Akash Mohta", "Umapada Pal"], "author_detail": {"name": "Umapada Pal"}, "author": "Umapada Pal", "links": [{"title": "doi", "href": "http://dx.doi.org/10.1016/j.patcog.2018.01.034", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/1712.06908v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1712.06908v2", "rel": "related", "type": "application/pdf"}], "arxiv_comment": "Accepted in Pattern Recognition, Elsevier(2018)", "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1712.06908v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1712.06908v2", "journal_reference": null, "doi": "10.1016/j.patcog.2018.01.034"}
{"id": "http://arxiv.org/abs/1811.08816v2", "guidislink": true, "updated": "2019-07-22T11:02:06Z", "updated_parsed": [2019, 7, 22, 11, 2, 6, 0, 203, 0], "published": "2018-11-21T16:36:08Z", "published_parsed": [2018, 11, 21, 16, 36, 8, 2, 325, 0], "title": "Learning cross-lingual phonological and orthagraphic adaptations: a case\n  study in improving neural machine translation between low-resource languages", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=70&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Learning cross-lingual phonological and orthagraphic adaptations: a case\n  study in improving neural machine translation between low-resource languages"}, "summary": "Out-of-vocabulary (OOV) words can pose serious challenges for machine\ntranslation (MT) tasks, and in particular, for low-resource language (LRL)\npairs, i.e., language pairs for which few or no parallel corpora exist. Our\nwork adapts variants of seq2seq models to perform transduction of such words\nfrom Hindi to Bhojpuri (an LRL instance), learning from a set of cognate pairs\nbuilt from a bilingual dictionary of Hindi--Bhojpuri words. We demonstrate that\nour models can be effectively used for language pairs that have limited\nparallel corpora; our models work at the character level to grasp phonetic and\northographic similarities across multiple types of word adaptations, whether\nsynchronic or diachronic, loan words or cognates. We describe the training\naspects of several character level NMT systems that we adapted to this task and\ncharacterize their typical errors. Our method improves BLEU score by 6.3 on the\nHindi-to-Bhojpuri translation task. Further, we show that such transductions\ncan generalize well to other languages by applying it successfully to Hindi --\nBangla cognate pairs. Our work can be seen as an important step in the process\nof: (i) resolving the OOV words problem arising in MT tasks, (ii) creating\neffective parallel corpora for resource-constrained languages, and (iii)\nleveraging the enhanced semantic knowledge captured by word-level embeddings to\nperform character-level tasks.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=70&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Out-of-vocabulary (OOV) words can pose serious challenges for machine\ntranslation (MT) tasks, and in particular, for low-resource language (LRL)\npairs, i.e., language pairs for which few or no parallel corpora exist. Our\nwork adapts variants of seq2seq models to perform transduction of such words\nfrom Hindi to Bhojpuri (an LRL instance), learning from a set of cognate pairs\nbuilt from a bilingual dictionary of Hindi--Bhojpuri words. We demonstrate that\nour models can be effectively used for language pairs that have limited\nparallel corpora; our models work at the character level to grasp phonetic and\northographic similarities across multiple types of word adaptations, whether\nsynchronic or diachronic, loan words or cognates. We describe the training\naspects of several character level NMT systems that we adapted to this task and\ncharacterize their typical errors. Our method improves BLEU score by 6.3 on the\nHindi-to-Bhojpuri translation task. Further, we show that such transductions\ncan generalize well to other languages by applying it successfully to Hindi --\nBangla cognate pairs. Our work can be seen as an important step in the process\nof: (i) resolving the OOV words problem arising in MT tasks, (ii) creating\neffective parallel corpora for resource-constrained languages, and (iii)\nleveraging the enhanced semantic knowledge captured by word-level embeddings to\nperform character-level tasks."}, "authors": ["Saurav Jha", "Akhilesh Sudhakar", "Anil Kumar Singh"], "author_detail": {"name": "Anil Kumar Singh"}, "author": "Anil Kumar Singh", "arxiv_comment": "47 pages, 4 figures, 21 tables (including Appendices)", "links": [{"href": "http://arxiv.org/abs/1811.08816v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1811.08816v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1811.08816v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1811.08816v2", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/2012.14353v1", "guidislink": true, "updated": "2020-12-28T16:46:03Z", "updated_parsed": [2020, 12, 28, 16, 46, 3, 0, 363, 0], "published": "2020-12-28T16:46:03Z", "published_parsed": [2020, 12, 28, 16, 46, 3, 0, 363, 0], "title": "DeepHateExplainer: Explainable Hate Speech Detection in Under-resourced\n  Bengali Language", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=70&max_results=10&sortBy=relevance&sortOrder=descending", "value": "DeepHateExplainer: Explainable Hate Speech Detection in Under-resourced\n  Bengali Language"}, "summary": "Exponential growths of social media and micro-blogging sites not only provide\nplatforms for empowering freedom of expressions and individual voices, but also\nenables people to express anti-social behavior like online harassment,\ncyberbullying, and hate speech. Numerous works have been proposed to utilize\nthese data for social and anti-social behavior analysis, by predicting the\ncontexts mostly for highly-resourced languages like English. However, some\nlanguages such as Bengali are under-resourced that lack of computational\nresources for natural language processing(NLP). In this paper, we propose an\nexplainable approach for hate speech detection from under-resourced Bengali\nlanguage, which we called DeepHateExplainer. In our approach, Bengali texts are\nfirst comprehensively preprocessed, before classifying them into political,\npersonal, geopolitical, and religious hates, by employing neural ensemble of\ndifferent transformer-based neural architectures(i.e., monolingual Bangla\nBERT-base, multilingual BERT-cased and uncased, and XLM-RoBERTa), followed by\nidentifying important terms with sensitivity analysis and layer-wise relevance\npropagation(LRP) to provide human-interpretable explanations. Evaluations\nagainst several machine learning~(linear and tree-based models) and deep neural\nnetworks (i.e., CNN, Bi-LSTM, and Conv-LSTM with word embeddings) baselines\nyield F1 scores of 84%, 90%, 88%, and 88%, for political, personal,\ngeopolitical, and religious hates, respectively, during 3-fold cross-validation\ntests.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bangla&id_list=&start=70&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Exponential growths of social media and micro-blogging sites not only provide\nplatforms for empowering freedom of expressions and individual voices, but also\nenables people to express anti-social behavior like online harassment,\ncyberbullying, and hate speech. Numerous works have been proposed to utilize\nthese data for social and anti-social behavior analysis, by predicting the\ncontexts mostly for highly-resourced languages like English. However, some\nlanguages such as Bengali are under-resourced that lack of computational\nresources for natural language processing(NLP). In this paper, we propose an\nexplainable approach for hate speech detection from under-resourced Bengali\nlanguage, which we called DeepHateExplainer. In our approach, Bengali texts are\nfirst comprehensively preprocessed, before classifying them into political,\npersonal, geopolitical, and religious hates, by employing neural ensemble of\ndifferent transformer-based neural architectures(i.e., monolingual Bangla\nBERT-base, multilingual BERT-cased and uncased, and XLM-RoBERTa), followed by\nidentifying important terms with sensitivity analysis and layer-wise relevance\npropagation(LRP) to provide human-interpretable explanations. Evaluations\nagainst several machine learning~(linear and tree-based models) and deep neural\nnetworks (i.e., CNN, Bi-LSTM, and Conv-LSTM with word embeddings) baselines\nyield F1 scores of 84%, 90%, 88%, and 88%, for political, personal,\ngeopolitical, and religious hates, respectively, during 3-fold cross-validation\ntests."}, "authors": ["Md. Rezaul Karim", "Sumon Kanti Dey", "Bharathi Raja Chakravarthi"], "author_detail": {"name": "Bharathi Raja Chakravarthi"}, "author": "Bharathi Raja Chakravarthi", "arxiv_comment": "Extended version of this paper is currently under review in the IEEE\n  Access journal", "links": [{"href": "http://arxiv.org/abs/2012.14353v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2012.14353v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2012.14353v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2012.14353v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1706.08205v1", "guidislink": true, "updated": "2017-06-26T02:07:11Z", "updated_parsed": [2017, 6, 26, 2, 7, 11, 0, 177, 0], "published": "2017-06-26T02:07:11Z", "published_parsed": [2017, 6, 26, 2, 7, 11, 0, 177, 0], "title": "Metrics for Bengali Text Entry Research", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=0&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Metrics for Bengali Text Entry Research"}, "summary": "With the intention of bringing uniformity to Bengali text entry research,\nhere we present a new approach for calculating the most popular English text\nentry evaluation metrics for Bengali. To demonstrate our approach, we conducted\na user study where we evaluated four popular Bengali text entry techniques.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=0&max_results=10&sortBy=relevance&sortOrder=descending", "value": "With the intention of bringing uniformity to Bengali text entry research,\nhere we present a new approach for calculating the most popular English text\nentry evaluation metrics for Bengali. To demonstrate our approach, we conducted\na user study where we evaluated four popular Bengali text entry techniques."}, "authors": ["Sayan Sarcar", "Ahmed Sabbir Arif", "Ali Mazalek"], "author_detail": {"name": "Ali Mazalek"}, "author": "Ali Mazalek", "arxiv_comment": "This paper has been accepted and presented as a position paper at ACM\n  CHI 2015 workshop on \"Text entry on the edge\"", "links": [{"href": "http://arxiv.org/abs/1706.08205v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1706.08205v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.HC", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.HC", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1706.08205v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1706.08205v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1908.08674v1", "guidislink": true, "updated": "2019-08-23T05:45:27Z", "updated_parsed": [2019, 8, 23, 5, 45, 27, 4, 235, 0], "published": "2019-08-23T05:45:27Z", "published_parsed": [2019, 8, 23, 5, 45, 27, 4, 235, 0], "title": "A BLSTM Network for Printed Bengali OCR System with High Accuracy", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=0&max_results=10&sortBy=relevance&sortOrder=descending", "value": "A BLSTM Network for Printed Bengali OCR System with High Accuracy"}, "summary": "This paper presents a printed Bengali and English text OCR system developed\nby us using a single hidden BLSTM-CTC architecture having 128 units. Here, we\ndid not use any peephole connection and dropout in the BLSTM, which helped us\nin getting better accuracy. This architecture was trained by 47,720 text lines\nthat include English words also. When tested over 20 different Bengali fonts,\nit has produced character level accuracy of 99.32% and word level accuracy of\n96.65%. A good Indic multi script OCR system is also developed by Google. It\nsometimes recognizes a character of Bengali into the same character of a\nnon-Bengali script, especially Assamese, which has no distinction from Bengali,\nexcept for a few characters. For example, Bengali character for 'RA' is\nsometimes recognized as that of Assamese, mainly in conjunct consonant forms.\nOur OCR is free from such errors. This OCR system is available online at\nhttps://banglaocr.nltr.org", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=0&max_results=10&sortBy=relevance&sortOrder=descending", "value": "This paper presents a printed Bengali and English text OCR system developed\nby us using a single hidden BLSTM-CTC architecture having 128 units. Here, we\ndid not use any peephole connection and dropout in the BLSTM, which helped us\nin getting better accuracy. This architecture was trained by 47,720 text lines\nthat include English words also. When tested over 20 different Bengali fonts,\nit has produced character level accuracy of 99.32% and word level accuracy of\n96.65%. A good Indic multi script OCR system is also developed by Google. It\nsometimes recognizes a character of Bengali into the same character of a\nnon-Bengali script, especially Assamese, which has no distinction from Bengali,\nexcept for a few characters. For example, Bengali character for 'RA' is\nsometimes recognized as that of Assamese, mainly in conjunct consonant forms.\nOur OCR is free from such errors. This OCR system is available online at\nhttps://banglaocr.nltr.org"}, "authors": ["Debabrata Paul", "Bidyut Baran Chaudhuri"], "author_detail": {"name": "Bidyut Baran Chaudhuri"}, "author": "Bidyut Baran Chaudhuri", "arxiv_comment": "6 pages, 6 figures, This OCR system is available online at\n  https://banglaocr.nltr.org", "links": [{"href": "http://arxiv.org/abs/1908.08674v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1908.08674v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1908.08674v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1908.08674v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/2009.09359v2", "guidislink": true, "updated": "2020-10-07T05:33:13Z", "updated_parsed": [2020, 10, 7, 5, 33, 13, 2, 281, 0], "published": "2020-09-20T06:06:27Z", "published_parsed": [2020, 9, 20, 6, 6, 27, 6, 264, 0], "title": "Not Low-Resource Anymore: Aligner Ensembling, Batch Filtering, and New\n  Datasets for Bengali-English Machine Translation", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=0&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Not Low-Resource Anymore: Aligner Ensembling, Batch Filtering, and New\n  Datasets for Bengali-English Machine Translation"}, "summary": "Despite being the seventh most widely spoken language in the world, Bengali\nhas received much less attention in machine translation literature due to being\nlow in resources. Most publicly available parallel corpora for Bengali are not\nlarge enough; and have rather poor quality, mostly because of incorrect\nsentence alignments resulting from erroneous sentence segmentation, and also\nbecause of a high volume of noise present in them. In this work, we build a\ncustomized sentence segmenter for Bengali and propose two novel methods for\nparallel corpus creation on low-resource setups: aligner ensembling and batch\nfiltering. With the segmenter and the two methods combined, we compile a\nhigh-quality Bengali-English parallel corpus comprising of 2.75 million\nsentence pairs, more than 2 million of which were not available before.\nTraining on neural models, we achieve an improvement of more than 9 BLEU score\nover previous approaches to Bengali-English machine translation. We also\nevaluate on a new test set of 1000 pairs made with extensive quality control.\nWe release the segmenter, parallel corpus, and the evaluation set, thus\nelevating Bengali from its low-resource status. To the best of our knowledge,\nthis is the first ever large scale study on Bengali-English machine\ntranslation. We believe our study will pave the way for future research on\nBengali-English machine translation as well as other low-resource languages.\nOur data and code are available at https://github.com/csebuetnlp/banglanmt.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=0&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Despite being the seventh most widely spoken language in the world, Bengali\nhas received much less attention in machine translation literature due to being\nlow in resources. Most publicly available parallel corpora for Bengali are not\nlarge enough; and have rather poor quality, mostly because of incorrect\nsentence alignments resulting from erroneous sentence segmentation, and also\nbecause of a high volume of noise present in them. In this work, we build a\ncustomized sentence segmenter for Bengali and propose two novel methods for\nparallel corpus creation on low-resource setups: aligner ensembling and batch\nfiltering. With the segmenter and the two methods combined, we compile a\nhigh-quality Bengali-English parallel corpus comprising of 2.75 million\nsentence pairs, more than 2 million of which were not available before.\nTraining on neural models, we achieve an improvement of more than 9 BLEU score\nover previous approaches to Bengali-English machine translation. We also\nevaluate on a new test set of 1000 pairs made with extensive quality control.\nWe release the segmenter, parallel corpus, and the evaluation set, thus\nelevating Bengali from its low-resource status. To the best of our knowledge,\nthis is the first ever large scale study on Bengali-English machine\ntranslation. We believe our study will pave the way for future research on\nBengali-English machine translation as well as other low-resource languages.\nOur data and code are available at https://github.com/csebuetnlp/banglanmt."}, "authors": ["Tahmid Hasan", "Abhik Bhattacharjee", "Kazi Samin", "Masum Hasan", "Madhusudan Basak", "M. Sohel Rahman", "Rifat Shahriyar"], "author_detail": {"name": "Rifat Shahriyar"}, "author": "Rifat Shahriyar", "arxiv_comment": "EMNLP 2020", "links": [{"href": "http://arxiv.org/abs/2009.09359v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2009.09359v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2009.09359v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2009.09359v2", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1201.2240v1", "guidislink": true, "updated": "2012-01-11T04:56:59Z", "updated_parsed": [2012, 1, 11, 4, 56, 59, 2, 11, 0], "published": "2012-01-11T04:56:59Z", "published_parsed": [2012, 1, 11, 4, 56, 59, 2, 11, 0], "title": "Bengali text summarization by sentence extraction", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=0&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Bengali text summarization by sentence extraction"}, "summary": "Text summarization is a process to produce an abstract or a summary by\nselecting significant portion of the information from one or more texts. In an\nautomatic text summarization process, a text is given to the computer and the\ncomputer returns a shorter less redundant extract or abstract of the original\ntext(s). Many techniques have been developed for summarizing English text(s).\nBut, a very few attempts have been made for Bengali text summarization. This\npaper presents a method for Bengali text summarization which extracts important\nsentences from a Bengali document to produce a summary.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=0&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Text summarization is a process to produce an abstract or a summary by\nselecting significant portion of the information from one or more texts. In an\nautomatic text summarization process, a text is given to the computer and the\ncomputer returns a shorter less redundant extract or abstract of the original\ntext(s). Many techniques have been developed for summarizing English text(s).\nBut, a very few attempts have been made for Bengali text summarization. This\npaper presents a method for Bengali text summarization which extracts important\nsentences from a Bengali document to produce a summary."}, "authors": ["Kamal Sarkar"], "author_detail": {"name": "Kamal Sarkar"}, "author": "Kamal Sarkar", "links": [{"href": "http://arxiv.org/abs/1201.2240v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1201.2240v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.IR", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.IR", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1201.2240v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1201.2240v1", "arxiv_comment": null, "journal_reference": "Proceedings of International Conference on Business and\n  Information Management(ICBIM-2012),NIT Durgapur, PP 233-245", "doi": null}
{"id": "http://arxiv.org/abs/1406.3915v1", "guidislink": true, "updated": "2014-06-16T06:41:54Z", "updated_parsed": [2014, 6, 16, 6, 41, 54, 0, 167, 0], "published": "2014-06-16T06:41:54Z", "published_parsed": [2014, 6, 16, 6, 41, 54, 0, 167, 0], "title": "A Bengali HMM Based Speech Synthesis System", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=0&max_results=10&sortBy=relevance&sortOrder=descending", "value": "A Bengali HMM Based Speech Synthesis System"}, "summary": "The paper presents the capability of an HMM-based TTS system to produce\nBengali speech. In this synthesis method, trajectories of speech parameters are\ngenerated from the trained Hidden Markov Models. A final speech waveform is\nsynthesized from those speech parameters. In our experiments, spectral\nproperties were represented by Mel Cepstrum Coefficients. Both the training and\nsynthesis issues are investigated in this paper using annotated Bengali speech\ndatabase. Experimental evaluation depicts that the developed text-to-speech\nsystem is capable of producing adequately natural speech in terms of\nintelligibility and intonation for Bengali.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=0&max_results=10&sortBy=relevance&sortOrder=descending", "value": "The paper presents the capability of an HMM-based TTS system to produce\nBengali speech. In this synthesis method, trajectories of speech parameters are\ngenerated from the trained Hidden Markov Models. A final speech waveform is\nsynthesized from those speech parameters. In our experiments, spectral\nproperties were represented by Mel Cepstrum Coefficients. Both the training and\nsynthesis issues are investigated in this paper using annotated Bengali speech\ndatabase. Experimental evaluation depicts that the developed text-to-speech\nsystem is capable of producing adequately natural speech in terms of\nintelligibility and intonation for Bengali."}, "authors": ["Sankar Mukherjee", "Shyamal Kumar Das Mandal"], "author_detail": {"name": "Shyamal Kumar Das Mandal"}, "author": "Shyamal Kumar Das Mandal", "links": [{"href": "http://arxiv.org/abs/1406.3915v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1406.3915v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.SD", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.SD", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.MM", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1406.3915v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1406.3915v1", "arxiv_comment": null, "journal_reference": "Oriental COCOSDA 2012, pp.225 259", "doi": null}
{"id": "http://arxiv.org/abs/1407.1976v1", "guidislink": true, "updated": "2014-07-08T07:35:16Z", "updated_parsed": [2014, 7, 8, 7, 35, 16, 1, 189, 0], "published": "2014-07-08T07:35:16Z", "published_parsed": [2014, 7, 8, 7, 35, 16, 1, 189, 0], "title": "Inter-Rater Agreement Study on Readability Assessment in Bengali", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=0&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Inter-Rater Agreement Study on Readability Assessment in Bengali"}, "summary": "An inter-rater agreement study is performed for readability assessment in\nBengali. A 1-7 rating scale was used to indicate different levels of\nreadability. We obtained moderate to fair agreement among seven independent\nannotators on 30 text passages written by four eminent Bengali authors. As a by\nproduct of our study, we obtained a readability-annotated ground truth dataset\nin Bengali. .", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=0&max_results=10&sortBy=relevance&sortOrder=descending", "value": "An inter-rater agreement study is performed for readability assessment in\nBengali. A 1-7 rating scale was used to indicate different levels of\nreadability. We obtained moderate to fair agreement among seven independent\nannotators on 30 text passages written by four eminent Bengali authors. As a by\nproduct of our study, we obtained a readability-annotated ground truth dataset\nin Bengali. ."}, "authors": ["Shanta Phani", "Shibamouli Lahiri", "Arindam Biswas"], "author_detail": {"name": "Arindam Biswas"}, "author": "Arindam Biswas", "arxiv_comment": "6 pages, 4 tables, Accepted in ICCONAC, 2014", "links": [{"href": "http://arxiv.org/abs/1407.1976v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1407.1976v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1407.1976v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1407.1976v1", "journal_reference": "International Journal on Natural Language Computing (IJNLC), 3(3),\n  2014", "doi": null}
{"id": "http://arxiv.org/abs/1701.08156v2", "guidislink": true, "updated": "2018-04-26T18:17:00Z", "updated_parsed": [2018, 4, 26, 18, 17, 0, 3, 116, 0], "published": "2017-01-27T12:38:47Z", "published_parsed": [2017, 1, 27, 12, 38, 47, 4, 27, 0], "title": "A Comprehensive Survey on Bengali Phoneme Recognition", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=0&max_results=10&sortBy=relevance&sortOrder=descending", "value": "A Comprehensive Survey on Bengali Phoneme Recognition"}, "summary": "Hidden Markov model based various phoneme recognition methods for Bengali\nlanguage is reviewed. Automatic phoneme recognition for Bengali language using\nmultilayer neural network is reviewed. Usefulness of multilayer neural network\nover single layer neural network is discussed. Bangla phonetic feature table\nconstruction and enhancement for Bengali speech recognition is also discussed.\nComparison among these methods is discussed.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=0&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Hidden Markov model based various phoneme recognition methods for Bengali\nlanguage is reviewed. Automatic phoneme recognition for Bengali language using\nmultilayer neural network is reviewed. Usefulness of multilayer neural network\nover single layer neural network is discussed. Bangla phonetic feature table\nconstruction and enhancement for Bengali speech recognition is also discussed.\nComparison among these methods is discussed."}, "authors": ["Sadia Tasnim Swarna", "Shamim Ehsan", "Md. Saiful Islam", "Marium E Jannat"], "author_detail": {"name": "Marium E Jannat"}, "author": "Marium E Jannat", "arxiv_comment": "7 pages, reference added in phoneme recognition methods", "links": [{"href": "http://arxiv.org/abs/1701.08156v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1701.08156v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.SD", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.SD", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1701.08156v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1701.08156v2", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1909.00823v1", "guidislink": true, "updated": "2019-09-02T18:28:14Z", "updated_parsed": [2019, 9, 2, 18, 28, 14, 0, 245, 0], "published": "2019-09-02T18:28:14Z", "published_parsed": [2019, 9, 2, 18, 28, 14, 0, 245, 0], "title": "HishabNet: Detection, Localization and Calculation of Handwritten\n  Bengali Mathematical Expressions", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=0&max_results=10&sortBy=relevance&sortOrder=descending", "value": "HishabNet: Detection, Localization and Calculation of Handwritten\n  Bengali Mathematical Expressions"}, "summary": "Recently, recognition of handwritten Bengali letters and digits have captured\na lot of attention among the researchers of the AI community. In this work, we\npropose a Convolutional Neural Network (CNN) based object detection model which\ncan recognize and evaluate handwritten Bengali mathematical expressions. This\nmethod is able to detect multiple Bengali digits and operators and locate their\npositions in the image. With that information, it is able to construct numbers\nfrom series of digits and perform mathematical operations on them. For the\nobject detection task, the state-of-the-art YOLOv3 algorithm was utilized. For\ntraining and evaluating the model, we have engineered a new dataset 'Hishab'\nwhich is the first Bengali handwritten digits dataset intended for object\ndetection. The model achieved an overall validation mean average precision\n(mAP) of 98.6%. Also, the classification accuracy of the feature extractor\nbackbone CNN used in our model was tested on two publicly available Bengali\nhandwritten digits datasets: NumtaDB and CMATERdb. The backbone CNN achieved a\ntest set accuracy of 99.6252% on NumtaDB and 99.0833% on CMATERdb.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=0&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Recently, recognition of handwritten Bengali letters and digits have captured\na lot of attention among the researchers of the AI community. In this work, we\npropose a Convolutional Neural Network (CNN) based object detection model which\ncan recognize and evaluate handwritten Bengali mathematical expressions. This\nmethod is able to detect multiple Bengali digits and operators and locate their\npositions in the image. With that information, it is able to construct numbers\nfrom series of digits and perform mathematical operations on them. For the\nobject detection task, the state-of-the-art YOLOv3 algorithm was utilized. For\ntraining and evaluating the model, we have engineered a new dataset 'Hishab'\nwhich is the first Bengali handwritten digits dataset intended for object\ndetection. The model achieved an overall validation mean average precision\n(mAP) of 98.6%. Also, the classification accuracy of the feature extractor\nbackbone CNN used in our model was tested on two publicly available Bengali\nhandwritten digits datasets: NumtaDB and CMATERdb. The backbone CNN achieved a\ntest set accuracy of 99.6252% on NumtaDB and 99.0833% on CMATERdb."}, "authors": ["Md Nafee Al Islam", "Siamul Karim Khan"], "author_detail": {"name": "Siamul Karim Khan"}, "author": "Siamul Karim Khan", "arxiv_comment": "6 pages, 5 figures, This paper is under review in \"22nd International\n  Conference on Computer and Information Technology (ICCIT), 2019\"", "links": [{"href": "http://arxiv.org/abs/1909.00823v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1909.00823v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1909.00823v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1909.00823v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/2003.03484v2", "guidislink": true, "updated": "2020-05-21T15:49:16Z", "updated_parsed": [2020, 5, 21, 15, 49, 16, 3, 142, 0], "published": "2020-03-07T01:52:19Z", "published_parsed": [2020, 3, 7, 1, 52, 19, 5, 67, 0], "title": "Synthetic Error Dataset Generation Mimicking Bengali Writing Pattern", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=0&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Synthetic Error Dataset Generation Mimicking Bengali Writing Pattern"}, "summary": "While writing Bengali using English keyboard, users often make spelling\nmistakes. The accuracy of any Bengali spell checker or paragraph correction\nmodule largely depends on the kind of error dataset it is based on. Manual\ngeneration of such error dataset is a cumbersome process. In this research, We\npresent an algorithm for automatic misspelled Bengali word generation from\ncorrect word through analyzing Bengali writing pattern using QWERTY layout\nEnglish keyboard. As part of our analysis, we have formed a list of most\ncommonly used Bengali words, phonetically similar replaceable clusters,\nfrequently mispressed replaceable clusters, frequently mispressed insertion\nprone clusters and some rules for Juktakkhar (constant letter clusters)\nhandling while generating errors.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=0&max_results=10&sortBy=relevance&sortOrder=descending", "value": "While writing Bengali using English keyboard, users often make spelling\nmistakes. The accuracy of any Bengali spell checker or paragraph correction\nmodule largely depends on the kind of error dataset it is based on. Manual\ngeneration of such error dataset is a cumbersome process. In this research, We\npresent an algorithm for automatic misspelled Bengali word generation from\ncorrect word through analyzing Bengali writing pattern using QWERTY layout\nEnglish keyboard. As part of our analysis, we have formed a list of most\ncommonly used Bengali words, phonetically similar replaceable clusters,\nfrequently mispressed replaceable clusters, frequently mispressed insertion\nprone clusters and some rules for Juktakkhar (constant letter clusters)\nhandling while generating errors."}, "authors": ["Md. Habibur Rahman Sifat", "Chowdhury Rafeed Rahman", "Mohammad Rafsan", "Md. Hasibur Rahman"], "author_detail": {"name": "Md. Hasibur Rahman"}, "author": "Md. Hasibur Rahman", "links": [{"href": "http://arxiv.org/abs/2003.03484v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2003.03484v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2003.03484v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2003.03484v2", "arxiv_comment": null, "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1508.01349v1", "guidislink": true, "updated": "2015-08-06T10:26:40Z", "updated_parsed": [2015, 8, 6, 10, 26, 40, 3, 218, 0], "published": "2015-08-06T10:26:40Z", "published_parsed": [2015, 8, 6, 10, 26, 40, 3, 218, 0], "title": "Automatic classification of bengali sentences based on sense definitions\n  present in bengali wordnet", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=10&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Automatic classification of bengali sentences based on sense definitions\n  present in bengali wordnet"}, "summary": "Based on the sense definition of words available in the Bengali WordNet, an\nattempt is made to classify the Bengali sentences automatically into different\ngroups in accordance with their underlying senses. The input sentences are\ncollected from 50 different categories of the Bengali text corpus developed in\nthe TDIL project of the Govt. of India, while information about the different\nsenses of particular ambiguous lexical item is collected from Bengali WordNet.\nIn an experimental basis we have used Naive Bayes probabilistic model as a\nuseful classifier of sentences. We have applied the algorithm over 1747\nsentences that contain a particular Bengali lexical item which, because of its\nambiguous nature, is able to trigger different senses that render sentences in\ndifferent meanings. In our experiment we have achieved around 84% accurate\nresult on the sense classification over the total input sentences. We have\nanalyzed those residual sentences that did not comply with our experiment and\ndid affect the results to note that in many cases, wrong syntactic structures\nand less semantic information are the main hurdles in semantic classification\nof sentences. The applicational relevance of this study is attested in\nautomatic text classification, machine learning, information extraction, and\nword sense disambiguation.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=10&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Based on the sense definition of words available in the Bengali WordNet, an\nattempt is made to classify the Bengali sentences automatically into different\ngroups in accordance with their underlying senses. The input sentences are\ncollected from 50 different categories of the Bengali text corpus developed in\nthe TDIL project of the Govt. of India, while information about the different\nsenses of particular ambiguous lexical item is collected from Bengali WordNet.\nIn an experimental basis we have used Naive Bayes probabilistic model as a\nuseful classifier of sentences. We have applied the algorithm over 1747\nsentences that contain a particular Bengali lexical item which, because of its\nambiguous nature, is able to trigger different senses that render sentences in\ndifferent meanings. In our experiment we have achieved around 84% accurate\nresult on the sense classification over the total input sentences. We have\nanalyzed those residual sentences that did not comply with our experiment and\ndid affect the results to note that in many cases, wrong syntactic structures\nand less semantic information are the main hurdles in semantic classification\nof sentences. The applicational relevance of this study is attested in\nautomatic text classification, machine learning, information extraction, and\nword sense disambiguation."}, "authors": ["Alok Ranjan Pal", "Diganta Saha", "Niladri Sekhar Dash"], "author_detail": {"name": "Niladri Sekhar Dash"}, "author": "Niladri Sekhar Dash", "links": [{"title": "doi", "href": "http://dx.doi.org/10.5121/ijctcm.2015.5101", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/1508.01349v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1508.01349v1", "rel": "related", "type": "application/pdf"}], "arxiv_comment": "International Journal of Control Theory and Computer Modeling\n  (IJCTCM) Vol.5, No.1, January 2015", "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1508.01349v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1508.01349v1", "journal_reference": null, "doi": "10.5121/ijctcm.2015.5101"}
{"id": "http://arxiv.org/abs/2007.14576v1", "guidislink": true, "updated": "2020-07-29T03:42:07Z", "updated_parsed": [2020, 7, 29, 3, 42, 7, 2, 211, 0], "published": "2020-07-29T03:42:07Z", "published_parsed": [2020, 7, 29, 3, 42, 7, 2, 211, 0], "title": "Development of POS tagger for English-Bengali Code-Mixed data", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=10&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Development of POS tagger for English-Bengali Code-Mixed data"}, "summary": "Code-mixed texts are widespread nowadays due to the advent of social media.\nSince these texts combine two languages to formulate a sentence, it gives rise\nto various research problems related to Natural Language Processing. In this\npaper, we try to excavate one such problem, namely, Parts of Speech tagging of\ncode-mixed texts. We have built a system that can POS tag English-Bengali\ncode-mixed data where the Bengali words were written in Roman script. Our\napproach initially involves the collection and cleaning of English-Bengali\ncode-mixed tweets. These tweets were used as a development dataset for building\nour system. The proposed system is a modular approach that starts by tagging\nindividual tokens with their respective languages and then passes them to\ndifferent POS taggers, designed for different languages (English and Bengali,\nin our case). Tags given by the two systems are later joined together and the\nfinal result is then mapped to a universal POS tag set. Our system was checked\nusing 100 manually POS tagged code-mixed sentences and it returned an accuracy\nof 75.29%", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=10&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Code-mixed texts are widespread nowadays due to the advent of social media.\nSince these texts combine two languages to formulate a sentence, it gives rise\nto various research problems related to Natural Language Processing. In this\npaper, we try to excavate one such problem, namely, Parts of Speech tagging of\ncode-mixed texts. We have built a system that can POS tag English-Bengali\ncode-mixed data where the Bengali words were written in Roman script. Our\napproach initially involves the collection and cleaning of English-Bengali\ncode-mixed tweets. These tweets were used as a development dataset for building\nour system. The proposed system is a modular approach that starts by tagging\nindividual tokens with their respective languages and then passes them to\ndifferent POS taggers, designed for different languages (English and Bengali,\nin our case). Tags given by the two systems are later joined together and the\nfinal result is then mapped to a universal POS tag set. Our system was checked\nusing 100 manually POS tagged code-mixed sentences and it returned an accuracy\nof 75.29%"}, "authors": ["Tathagata Raha", "Sainik Kumar Mahata", "Dipankar Das", "Sivaji Bandyopadhyay"], "author_detail": {"name": "Sivaji Bandyopadhyay"}, "author": "Sivaji Bandyopadhyay", "arxiv_comment": "Accepted and published in The sixteenth International Conference on\n  Natural Language Processing (ICON-2019)", "links": [{"href": "http://arxiv.org/abs/2007.14576v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2007.14576v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2007.14576v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2007.14576v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/2010.03065v1", "guidislink": true, "updated": "2020-10-06T22:33:58Z", "updated_parsed": [2020, 10, 6, 22, 33, 58, 1, 280, 0], "published": "2020-10-06T22:33:58Z", "published_parsed": [2020, 10, 6, 22, 33, 58, 1, 280, 0], "title": "Anubhuti -- An annotated dataset for emotional analysis of Bengali short\n  stories", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=10&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Anubhuti -- An annotated dataset for emotional analysis of Bengali short\n  stories"}, "summary": "Thousands of short stories and articles are being written in many different\nlanguages all around the world today. Bengali, or Bangla, is the second highest\nspoken language in India after Hindi and is the national language of the\ncountry of Bangladesh. This work reports in detail the creation of Anubhuti --\nthe first and largest text corpus for analyzing emotions expressed by writers\nof Bengali short stories. We explain the data collection methods, the manual\nannotation process and the resulting high inter-annotator agreement of the\ndataset due to the linguistic expertise of the annotators and the clear\nmethodology of labelling followed. We also address some of the challenges faced\nin the collection of raw data and annotation process of a low resource language\nlike Bengali. We have verified the performance of our dataset with baseline\nMachine Learning as well as a Deep Learning model for emotion classification\nand have found that these standard models have a high accuracy and relevant\nfeature selection on Anubhuti. In addition, we also explain how this dataset\ncan be of interest to linguists and data analysts to study the flow of emotions\nas expressed by writers of Bengali literature.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=10&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Thousands of short stories and articles are being written in many different\nlanguages all around the world today. Bengali, or Bangla, is the second highest\nspoken language in India after Hindi and is the national language of the\ncountry of Bangladesh. This work reports in detail the creation of Anubhuti --\nthe first and largest text corpus for analyzing emotions expressed by writers\nof Bengali short stories. We explain the data collection methods, the manual\nannotation process and the resulting high inter-annotator agreement of the\ndataset due to the linguistic expertise of the annotators and the clear\nmethodology of labelling followed. We also address some of the challenges faced\nin the collection of raw data and annotation process of a low resource language\nlike Bengali. We have verified the performance of our dataset with baseline\nMachine Learning as well as a Deep Learning model for emotion classification\nand have found that these standard models have a high accuracy and relevant\nfeature selection on Anubhuti. In addition, we also explain how this dataset\ncan be of interest to linguists and data analysts to study the flow of emotions\nas expressed by writers of Bengali literature."}, "authors": ["Aditya Pal", "Bhaskar Karn"], "author_detail": {"name": "Bhaskar Karn"}, "author": "Bhaskar Karn", "arxiv_comment": "4 pages, 6 figures", "links": [{"href": "http://arxiv.org/abs/2010.03065v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2010.03065v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2010.03065v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2010.03065v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1806.02452v1", "guidislink": true, "updated": "2018-06-06T23:02:06Z", "updated_parsed": [2018, 6, 6, 23, 2, 6, 2, 157, 0], "published": "2018-06-06T23:02:06Z", "published_parsed": [2018, 6, 6, 23, 2, 6, 2, 157, 0], "title": "NumtaDB - Assembled Bengali Handwritten Digits", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=10&max_results=10&sortBy=relevance&sortOrder=descending", "value": "NumtaDB - Assembled Bengali Handwritten Digits"}, "summary": "To benchmark Bengali digit recognition algorithms, a large publicly available\ndataset is required which is free from biases originating from geographical\nlocation, gender, and age. With this aim in mind, NumtaDB, a dataset consisting\nof more than 85,000 images of hand-written Bengali digits, has been assembled.\nThis paper documents the collection and curation process of numerals along with\nthe salient statistics of the dataset.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=10&max_results=10&sortBy=relevance&sortOrder=descending", "value": "To benchmark Bengali digit recognition algorithms, a large publicly available\ndataset is required which is free from biases originating from geographical\nlocation, gender, and age. With this aim in mind, NumtaDB, a dataset consisting\nof more than 85,000 images of hand-written Bengali digits, has been assembled.\nThis paper documents the collection and curation process of numerals along with\nthe salient statistics of the dataset."}, "authors": ["Samiul Alam", "Tahsin Reasat", "Rashed Mohammad Doha", "Ahmed Imtiaz Humayun"], "author_detail": {"name": "Ahmed Imtiaz Humayun"}, "author": "Ahmed Imtiaz Humayun", "arxiv_comment": "6 page, 12 figures", "links": [{"href": "http://arxiv.org/abs/1806.02452v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1806.02452v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "68T10", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "I.5.1; I.5.4", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1806.02452v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1806.02452v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/2008.07853v1", "guidislink": true, "updated": "2020-08-18T11:02:25Z", "updated_parsed": [2020, 8, 18, 11, 2, 25, 1, 231, 0], "published": "2020-08-18T11:02:25Z", "published_parsed": [2020, 8, 18, 11, 2, 25, 1, 231, 0], "title": "Image Pre-processing on NumtaDB for Bengali Handwritten Digit\n  Recognition", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=10&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Image Pre-processing on NumtaDB for Bengali Handwritten Digit\n  Recognition"}, "summary": "NumtaDB is by far the largest data-set collection for handwritten digits in\nBengali. This is a diverse dataset containing more than 85000 images. But this\ndiversity also makes this dataset very difficult to work with. The goal of this\npaper is to find the benchmark for pre-processed images which gives good\naccuracy on any machine learning models. The reason being, there are no\navailable pre-processed data for Bengali digit recognition to work with like\nthe English digits for MNIST.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=10&max_results=10&sortBy=relevance&sortOrder=descending", "value": "NumtaDB is by far the largest data-set collection for handwritten digits in\nBengali. This is a diverse dataset containing more than 85000 images. But this\ndiversity also makes this dataset very difficult to work with. The goal of this\npaper is to find the benchmark for pre-processed images which gives good\naccuracy on any machine learning models. The reason being, there are no\navailable pre-processed data for Bengali digit recognition to work with like\nthe English digits for MNIST."}, "authors": ["Ovi Paul"], "author_detail": {"name": "Ovi Paul"}, "author": "Ovi Paul", "links": [{"title": "doi", "href": "http://dx.doi.org/10.1109/ICBSLP.2018.8554910", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/2008.07853v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2008.07853v1", "rel": "related", "type": "application/pdf"}], "arxiv_comment": "5 pages, 8 figures and 4 tables", "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2008.07853v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2008.07853v1", "journal_reference": "2018 International Conference on Bangla Speech and Language\n  Processing (ICBSLP), Sylhet, 2018, pp. 1-6", "doi": "10.1109/ICBSLP.2018.8554910"}
{"id": "http://arxiv.org/abs/2012.07538v1", "guidislink": true, "updated": "2020-12-03T10:21:11Z", "updated_parsed": [2020, 12, 3, 10, 21, 11, 3, 338, 0], "published": "2020-12-03T10:21:11Z", "published_parsed": [2020, 12, 3, 10, 21, 11, 3, 338, 0], "title": "Sentiment analysis in Bengali via transfer learning using multi-lingual\n  BERT", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=10&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Sentiment analysis in Bengali via transfer learning using multi-lingual\n  BERT"}, "summary": "Sentiment analysis (SA) in Bengali is challenging due to this Indo-Aryan\nlanguage's highly inflected properties with more than 160 different inflected\nforms for verbs and 36 different forms for noun and 24 different forms for\npronouns. The lack of standard labeled datasets in the Bengali domain makes the\ntask of SA even harder. In this paper, we present manually tagged 2-class and\n3-class SA datasets in Bengali. We also demonstrate that the multi-lingual BERT\nmodel with relevant extensions can be trained via the approach of transfer\nlearning over those novel datasets to improve the state-of-the-art performance\nin sentiment classification tasks. This deep learning model achieves an\naccuracy of 71\\% for 2-class sentiment classification compared to the current\nstate-of-the-art accuracy of 68\\%. We also present the very first Bengali SA\nclassifier for the 3-class manually tagged dataset, and our proposed model\nachieves an accuracy of 60\\%. We further use this model to analyze the\nsentiment of public comments in the online daily newspaper. Our analysis shows\nthat people post negative comments for political or sports news more often,\nwhile the religious article comments represent positive sentiment. The dataset\nand code is publicly available at\nhttps://github.com/KhondokerIslam/Bengali\\_Sentiment.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=10&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Sentiment analysis (SA) in Bengali is challenging due to this Indo-Aryan\nlanguage's highly inflected properties with more than 160 different inflected\nforms for verbs and 36 different forms for noun and 24 different forms for\npronouns. The lack of standard labeled datasets in the Bengali domain makes the\ntask of SA even harder. In this paper, we present manually tagged 2-class and\n3-class SA datasets in Bengali. We also demonstrate that the multi-lingual BERT\nmodel with relevant extensions can be trained via the approach of transfer\nlearning over those novel datasets to improve the state-of-the-art performance\nin sentiment classification tasks. This deep learning model achieves an\naccuracy of 71\\% for 2-class sentiment classification compared to the current\nstate-of-the-art accuracy of 68\\%. We also present the very first Bengali SA\nclassifier for the 3-class manually tagged dataset, and our proposed model\nachieves an accuracy of 60\\%. We further use this model to analyze the\nsentiment of public comments in the online daily newspaper. Our analysis shows\nthat people post negative comments for political or sports news more often,\nwhile the religious article comments represent positive sentiment. The dataset\nand code is publicly available at\nhttps://github.com/KhondokerIslam/Bengali\\_Sentiment."}, "authors": ["Khondoker Ittehadul Islam", "Md. Saiful Islam", "Md Ruhul Amin"], "author_detail": {"name": "Md Ruhul Amin"}, "author": "Md Ruhul Amin", "arxiv_comment": "5 pages", "links": [{"href": "http://arxiv.org/abs/2012.07538v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2012.07538v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2012.07538v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2012.07538v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/2012.07701v1", "guidislink": true, "updated": "2020-12-09T01:41:35Z", "updated_parsed": [2020, 12, 9, 1, 41, 35, 2, 344, 0], "published": "2020-12-09T01:41:35Z", "published_parsed": [2020, 12, 9, 1, 41, 35, 2, 344, 0], "title": "Simple or Complex? Learning to Predict Readability of Bengali Texts", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=10&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Simple or Complex? Learning to Predict Readability of Bengali Texts"}, "summary": "Determining the readability of a text is the first step to its\nsimplification. In this paper, we present a readability analysis tool capable\nof analyzing text written in the Bengali language to provide in-depth\ninformation on its readability and complexity. Despite being the 7th most\nspoken language in the world with 230 million native speakers, Bengali suffers\nfrom a lack of fundamental resources for natural language processing.\nReadability related research of the Bengali language so far can be considered\nto be narrow and sometimes faulty due to the lack of resources. Therefore, we\ncorrectly adopt document-level readability formulas traditionally used for U.S.\nbased education system to the Bengali language with a proper age-to-age\ncomparison. Due to the unavailability of large-scale human-annotated corpora,\nwe further divide the document-level task into sentence-level and experiment\nwith neural architectures, which will serve as a baseline for the future works\nof Bengali readability prediction. During the process, we present several\nhuman-annotated corpora and dictionaries such as a document-level dataset\ncomprising 618 documents with 12 different grade levels, a large-scale\nsentence-level dataset comprising more than 96K sentences with simple and\ncomplex labels, a consonant conjunct count algorithm and a corpus of 341 words\nto validate the effectiveness of the algorithm, a list of 3,396 easy words, and\nan updated pronunciation dictionary with more than 67K words. These resources\ncan be useful for several other tasks of this low-resource language. We make\nour Code & Dataset publicly available at\nhttps://github.com/tafseer-nayeem/BengaliReadability} for reproduciblity.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=10&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Determining the readability of a text is the first step to its\nsimplification. In this paper, we present a readability analysis tool capable\nof analyzing text written in the Bengali language to provide in-depth\ninformation on its readability and complexity. Despite being the 7th most\nspoken language in the world with 230 million native speakers, Bengali suffers\nfrom a lack of fundamental resources for natural language processing.\nReadability related research of the Bengali language so far can be considered\nto be narrow and sometimes faulty due to the lack of resources. Therefore, we\ncorrectly adopt document-level readability formulas traditionally used for U.S.\nbased education system to the Bengali language with a proper age-to-age\ncomparison. Due to the unavailability of large-scale human-annotated corpora,\nwe further divide the document-level task into sentence-level and experiment\nwith neural architectures, which will serve as a baseline for the future works\nof Bengali readability prediction. During the process, we present several\nhuman-annotated corpora and dictionaries such as a document-level dataset\ncomprising 618 documents with 12 different grade levels, a large-scale\nsentence-level dataset comprising more than 96K sentences with simple and\ncomplex labels, a consonant conjunct count algorithm and a corpus of 341 words\nto validate the effectiveness of the algorithm, a list of 3,396 easy words, and\nan updated pronunciation dictionary with more than 67K words. These resources\ncan be useful for several other tasks of this low-resource language. We make\nour Code & Dataset publicly available at\nhttps://github.com/tafseer-nayeem/BengaliReadability} for reproduciblity."}, "authors": ["Susmoy Chakraborty", "Mir Tafseer Nayeem", "Wasi Uddin Ahmad"], "author_detail": {"name": "Wasi Uddin Ahmad"}, "author": "Wasi Uddin Ahmad", "arxiv_comment": "Accepted for publication at AAAI 2021", "links": [{"href": "http://arxiv.org/abs/2012.07701v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2012.07701v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2012.07701v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2012.07701v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1607.05650v2", "guidislink": true, "updated": "2016-09-07T03:09:17Z", "updated_parsed": [2016, 9, 7, 3, 9, 17, 2, 251, 0], "published": "2016-07-13T06:57:38Z", "published_parsed": [2016, 7, 13, 6, 57, 38, 2, 195, 0], "title": "A Supervised Authorship Attribution Framework for Bengali Language", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=10&max_results=10&sortBy=relevance&sortOrder=descending", "value": "A Supervised Authorship Attribution Framework for Bengali Language"}, "summary": "Authorship Attribution is a long-standing problem in Natural Language\nProcessing. Several statistical and computational methods have been used to\nfind a solution to this problem. In this paper, we have proposed methods to\ndeal with the authorship attribution problem in Bengali.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=10&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Authorship Attribution is a long-standing problem in Natural Language\nProcessing. Several statistical and computational methods have been used to\nfind a solution to this problem. In this paper, we have proposed methods to\ndeal with the authorship attribution problem in Bengali."}, "authors": ["Shanta Phani", "Shibamouli Lahiri", "Arindam Biswas"], "author_detail": {"name": "Arindam Biswas"}, "author": "Arindam Biswas", "arxiv_comment": "This paper has been withdrawn by the authors as the results need to\n  be changed", "links": [{"href": "http://arxiv.org/abs/1607.05650v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1607.05650v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.DL", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1607.05650v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1607.05650v2", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1607.05755v4", "guidislink": true, "updated": "2017-03-14T02:36:05Z", "updated_parsed": [2017, 3, 14, 2, 36, 5, 1, 73, 0], "published": "2016-07-10T19:14:00Z", "published_parsed": [2016, 7, 10, 19, 14, 0, 6, 192, 0], "title": "A New Bengali Readability Score", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=10&max_results=10&sortBy=relevance&sortOrder=descending", "value": "A New Bengali Readability Score"}, "summary": "In this paper we have proposed methods to analyze the readability of Bengali\nlanguage texts. We have got some exceptionally good results out of the\nexperiments.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=10&max_results=10&sortBy=relevance&sortOrder=descending", "value": "In this paper we have proposed methods to analyze the readability of Bengali\nlanguage texts. We have got some exceptionally good results out of the\nexperiments."}, "authors": ["Shanta Phani", "Shibamouli Lahiri", "Arindam Biswas"], "author_detail": {"name": "Arindam Biswas"}, "author": "Arindam Biswas", "arxiv_comment": "This paper has been withdrawn by the author as the results need to be\n  changed", "links": [{"href": "http://arxiv.org/abs/1607.05755v4", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1607.05755v4", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1607.05755v4", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1607.05755v4", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1202.3046v1", "guidislink": true, "updated": "2012-02-14T14:22:24Z", "updated_parsed": [2012, 2, 14, 14, 22, 24, 1, 45, 0], "published": "2012-02-14T14:22:24Z", "published_parsed": [2012, 2, 14, 14, 22, 24, 1, 45, 0], "title": "Segmentation of Offline Handwritten Bengali Script", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=20&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Segmentation of Offline Handwritten Bengali Script"}, "summary": "Character segmentation has long been one of the most critical areas of\noptical character recognition process. Through this operation, an image of a\nsequence of characters, which may be connected in some cases, is decomposed\ninto sub-images of individual alphabetic symbols. In this paper, segmentation\nof cursive handwritten script of world's fourth popular language, Bengali, is\nconsidered. Unlike English script, Bengali handwritten characters and its\ncomponents often encircle the main character, making the conventional\nsegmentation methodologies inapplicable. Experimental results, using the\nproposed segmentation technique, on sample cursive handwritten data containing\n218 ideal segmentation points show a success rate of 97.7%. Further\nfeature-analysis on these segments may lead to actual recognition of\nhandwritten cursive Bengali script.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=20&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Character segmentation has long been one of the most critical areas of\noptical character recognition process. Through this operation, an image of a\nsequence of characters, which may be connected in some cases, is decomposed\ninto sub-images of individual alphabetic symbols. In this paper, segmentation\nof cursive handwritten script of world's fourth popular language, Bengali, is\nconsidered. Unlike English script, Bengali handwritten characters and its\ncomponents often encircle the main character, making the conventional\nsegmentation methodologies inapplicable. Experimental results, using the\nproposed segmentation technique, on sample cursive handwritten data containing\n218 ideal segmentation points show a success rate of 97.7%. Further\nfeature-analysis on these segments may lead to actual recognition of\nhandwritten cursive Bengali script."}, "authors": ["Subhadip Basu", "Chitrita Chaudhuri", "Mahantapas Kundu", "Mita Nasipuri", "Dipak K. Basu"], "author_detail": {"name": "Dipak K. Basu"}, "author": "Dipak K. Basu", "arxiv_comment": "Proceedings of 28th IEEE ACE, pp. 171-174, December 2002, Science\n  City, Kolkata", "links": [{"href": "http://arxiv.org/abs/1202.3046v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1202.3046v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1202.3046v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1202.3046v1", "journal_reference": "Proceedings of 28th IEEE ACE, pp. 171-174, December 2002, Science\n  City, Kolkata", "doi": null}
{"id": "http://arxiv.org/abs/1208.6268v4", "guidislink": true, "updated": "2013-02-24T09:27:56Z", "updated_parsed": [2013, 2, 24, 9, 27, 56, 6, 55, 0], "published": "2012-08-30T19:09:24Z", "published_parsed": [2012, 8, 30, 19, 9, 24, 3, 243, 0], "title": "Authorship Identification in Bengali Literature: a Comparative Analysis", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=20&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Authorship Identification in Bengali Literature: a Comparative Analysis"}, "summary": "Stylometry is the study of the unique linguistic styles and writing behaviors\nof individuals. It belongs to the core task of text categorization like\nauthorship identification, plagiarism detection etc. Though reasonable number\nof studies have been conducted in English language, no major work has been done\nso far in Bengali. In this work, We will present a demonstration of authorship\nidentification of the documents written in Bengali. We adopt a set of\nfine-grained stylistic features for the analysis of the text and use them to\ndevelop two different models: statistical similarity model consisting of three\nmeasures and their combination, and machine learning model with Decision Tree,\nNeural Network and SVM. Experimental results show that SVM outperforms other\nstate-of-the-art methods after 10-fold cross validations. We also validate the\nrelative importance of each stylistic feature to show that some of them remain\nconsistently significant in every model used in this experiment.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=20&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Stylometry is the study of the unique linguistic styles and writing behaviors\nof individuals. It belongs to the core task of text categorization like\nauthorship identification, plagiarism detection etc. Though reasonable number\nof studies have been conducted in English language, no major work has been done\nso far in Bengali. In this work, We will present a demonstration of authorship\nidentification of the documents written in Bengali. We adopt a set of\nfine-grained stylistic features for the analysis of the text and use them to\ndevelop two different models: statistical similarity model consisting of three\nmeasures and their combination, and machine learning model with Decision Tree,\nNeural Network and SVM. Experimental results show that SVM outperforms other\nstate-of-the-art methods after 10-fold cross validations. We also validate the\nrelative importance of each stylistic feature to show that some of them remain\nconsistently significant in every model used in this experiment."}, "authors": ["Tanmoy Chakraborty"], "author_detail": {"name": "Tanmoy Chakraborty"}, "author": "Tanmoy Chakraborty", "arxiv_comment": "9 pages, 5 tables, 4 pictures", "links": [{"href": "http://arxiv.org/abs/1208.6268v4", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1208.6268v4", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.IR", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1208.6268v4", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1208.6268v4", "journal_reference": "Chakraborty, T., Authorship Identification in Bengali Literature:\n  a Comparative Analysis, Proceedings of COLING 2012: Demonstration Papers,\n  December, 2012, pp. 41-50", "doi": null}
{"id": "http://arxiv.org/abs/1810.04452v1", "guidislink": true, "updated": "2018-10-10T10:59:28Z", "updated_parsed": [2018, 10, 10, 10, 59, 28, 2, 283, 0], "published": "2018-10-10T10:59:28Z", "published_parsed": [2018, 10, 10, 10, 59, 28, 2, 283, 0], "title": "AI Learns to Recognize Bengali Handwritten Digits: Bengali.AI Computer\n  Vision Challenge 2018", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=20&max_results=10&sortBy=relevance&sortOrder=descending", "value": "AI Learns to Recognize Bengali Handwritten Digits: Bengali.AI Computer\n  Vision Challenge 2018"}, "summary": "Solving problems with Artificial intelligence in a competitive manner has\nlong been absent in Bangladesh and Bengali-speaking community. On the other\nhand, there has not been a well structured database for Bengali Handwritten\ndigits for mass public use. To bring out the best minds working in machine\nlearning and use their expertise to create a model which can easily recognize\nBengali Handwritten digits, we organized Bengali.AI Computer Vision\nChallenge.The challenge saw both local and international teams participating\nwith unprecedented efforts.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=20&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Solving problems with Artificial intelligence in a competitive manner has\nlong been absent in Bangladesh and Bengali-speaking community. On the other\nhand, there has not been a well structured database for Bengali Handwritten\ndigits for mass public use. To bring out the best minds working in machine\nlearning and use their expertise to create a model which can easily recognize\nBengali Handwritten digits, we organized Bengali.AI Computer Vision\nChallenge.The challenge saw both local and international teams participating\nwith unprecedented efforts."}, "authors": ["Sharif Amit Kamran", "Ahmed Imtiaz Humayun", "Samiul Alam", "Rashed Mohammad Doha", "Manash Kumar Mandal", "Tahsin Reasat", "Fuad Rahman"], "author_detail": {"name": "Fuad Rahman"}, "author": "Fuad Rahman", "arxiv_comment": "5 pages, 3 figures", "links": [{"href": "http://arxiv.org/abs/1810.04452v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1810.04452v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1810.04452v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1810.04452v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1812.05758v1", "guidislink": true, "updated": "2018-12-14T02:01:13Z", "updated_parsed": [2018, 12, 14, 2, 1, 13, 4, 348, 0], "published": "2018-12-14T02:01:13Z", "published_parsed": [2018, 12, 14, 2, 1, 13, 4, 348, 0], "title": "On Stacked Denoising Autoencoder based Pre-training of ANN for Isolated\n  Handwritten Bengali Numerals Dataset Recognition", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=20&max_results=10&sortBy=relevance&sortOrder=descending", "value": "On Stacked Denoising Autoencoder based Pre-training of ANN for Isolated\n  Handwritten Bengali Numerals Dataset Recognition"}, "summary": "This work attempts to find the most optimal parameter setting of a deep\nartificial neural network (ANN) for Bengali digit dataset by pre-training it\nusing stacked denoising autoencoder (SDA). Although SDA based recognition is\nhugely popular in image, speech and language processing related tasks among the\nresearchers, it was never tried in Bengali dataset recognition. For this work,\na dataset of 70000 handwritten samples were used from (Chowdhury and Rahman,\n2016) and was recognized using several settings of network architecture. Among\nall these settings, the most optimal setting being found to be five or more\ndeeper hidden layers with sigmoid activation and one output layer with softmax\nactivation. We proposed the optimal number of neurons that can be used in the\nhidden layer is 1500 or more. The minimum validation error found from this work\nis 2.34% which is the lowest error rate on handwritten Bengali dataset proposed\ntill date.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=20&max_results=10&sortBy=relevance&sortOrder=descending", "value": "This work attempts to find the most optimal parameter setting of a deep\nartificial neural network (ANN) for Bengali digit dataset by pre-training it\nusing stacked denoising autoencoder (SDA). Although SDA based recognition is\nhugely popular in image, speech and language processing related tasks among the\nresearchers, it was never tried in Bengali dataset recognition. For this work,\na dataset of 70000 handwritten samples were used from (Chowdhury and Rahman,\n2016) and was recognized using several settings of network architecture. Among\nall these settings, the most optimal setting being found to be five or more\ndeeper hidden layers with sigmoid activation and one output layer with softmax\nactivation. We proposed the optimal number of neurons that can be used in the\nhidden layer is 1500 or more. The minimum validation error found from this work\nis 2.34% which is the lowest error rate on handwritten Bengali dataset proposed\ntill date."}, "authors": ["Al Mehdi Saadat Chowdhury", "M. Shahidur Rahman", "Asia Khanom", "Tamanna Islam Chowdhury", "Afaz Uddin"], "author_detail": {"name": "Afaz Uddin"}, "author": "Afaz Uddin", "links": [{"href": "http://arxiv.org/abs/1812.05758v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1812.05758v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.ML", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1812.05758v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1812.05758v1", "arxiv_comment": null, "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/2001.05315v1", "guidislink": true, "updated": "2020-01-11T14:50:57Z", "updated_parsed": [2020, 1, 11, 14, 50, 57, 5, 11, 0], "published": "2020-01-11T14:50:57Z", "published_parsed": [2020, 1, 11, 14, 50, 57, 5, 11, 0], "title": "A Continuous Space Neural Language Model for Bengali Language", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=20&max_results=10&sortBy=relevance&sortOrder=descending", "value": "A Continuous Space Neural Language Model for Bengali Language"}, "summary": "Language models are generally employed to estimate the probability\ndistribution of various linguistic units, making them one of the fundamental\nparts of natural language processing. Applications of language models include a\nwide spectrum of tasks such as text summarization, translation and\nclassification. For a low resource language like Bengali, the research in this\narea so far can be considered to be narrow at the very least, with some\ntraditional count based models being proposed. This paper attempts to address\nthe issue and proposes a continuous-space neural language model, or more\nspecifically an ASGD weight dropped LSTM language model, along with techniques\nto efficiently train it for Bengali Language. The performance analysis with\nsome currently existing count based models illustrated in this paper also shows\nthat the proposed architecture outperforms its counterparts by achieving an\ninference perplexity as low as 51.2 on the held out data set for Bengali.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=20&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Language models are generally employed to estimate the probability\ndistribution of various linguistic units, making them one of the fundamental\nparts of natural language processing. Applications of language models include a\nwide spectrum of tasks such as text summarization, translation and\nclassification. For a low resource language like Bengali, the research in this\narea so far can be considered to be narrow at the very least, with some\ntraditional count based models being proposed. This paper attempts to address\nthe issue and proposes a continuous-space neural language model, or more\nspecifically an ASGD weight dropped LSTM language model, along with techniques\nto efficiently train it for Bengali Language. The performance analysis with\nsome currently existing count based models illustrated in this paper also shows\nthat the proposed architecture outperforms its counterparts by achieving an\ninference perplexity as low as 51.2 on the held out data set for Bengali."}, "authors": ["Hemayet Ahmed Chowdhury", "Md. Azizul Haque Imon", "Anisur Rahman", "Aisha Khatun", "Md. Saiful Islam"], "author_detail": {"name": "Md. Saiful Islam"}, "author": "Md. Saiful Islam", "arxiv_comment": "6 pages", "links": [{"href": "http://arxiv.org/abs/2001.05315v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2001.05315v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2001.05315v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2001.05315v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/2004.07820v1", "guidislink": true, "updated": "2020-04-15T22:38:54Z", "updated_parsed": [2020, 4, 15, 22, 38, 54, 2, 106, 0], "published": "2020-04-15T22:38:54Z", "published_parsed": [2020, 4, 15, 22, 38, 54, 2, 106, 0], "title": "Speaker Recognition in Bengali Language from Nonlinear Features", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=20&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Speaker Recognition in Bengali Language from Nonlinear Features"}, "summary": "At present Automatic Speaker Recognition system is a very important issue due\nto its diverse applications. Hence, it becomes absolutely necessary to obtain\nmodels that take into consideration the speaking style of a person, vocal tract\ninformation, timbral qualities of his voice and other congenital information\nregarding his voice. The study of Bengali speech recognition and speaker\nidentification is scarce in the literature. Hence the need arises for involving\nBengali subjects in modelling our speaker identification engine. In this work,\nwe have extracted some acoustic features of speech using non linear\nmultifractal analysis. The Multifractal Detrended Fluctuation Analysis reveals\nessentially the complexity associated with the speech signals taken. The source\ncharacteristics have been quantified with the help of different techniques like\nCorrelation Matrix, skewness of MFDFA spectrum etc. The Results obtained from\nthis study gives a good recognition rate for Bengali Speakers.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=20&max_results=10&sortBy=relevance&sortOrder=descending", "value": "At present Automatic Speaker Recognition system is a very important issue due\nto its diverse applications. Hence, it becomes absolutely necessary to obtain\nmodels that take into consideration the speaking style of a person, vocal tract\ninformation, timbral qualities of his voice and other congenital information\nregarding his voice. The study of Bengali speech recognition and speaker\nidentification is scarce in the literature. Hence the need arises for involving\nBengali subjects in modelling our speaker identification engine. In this work,\nwe have extracted some acoustic features of speech using non linear\nmultifractal analysis. The Multifractal Detrended Fluctuation Analysis reveals\nessentially the complexity associated with the speech signals taken. The source\ncharacteristics have been quantified with the help of different techniques like\nCorrelation Matrix, skewness of MFDFA spectrum etc. The Results obtained from\nthis study gives a good recognition rate for Bengali Speakers."}, "authors": ["Uddalok Sarkar", "Soumyadeep Pal", "Sayan Nag", "Chirayata Bhattacharya", "Shankha Sanyal", "Archi Banerjee", "Ranjan Sengupta", "Dipak Ghosh"], "author_detail": {"name": "Dipak Ghosh"}, "author": "Dipak Ghosh", "arxiv_comment": "arXiv admin note: text overlap with arXiv:1612.00171,\n  arXiv:1601.07709", "links": [{"href": "http://arxiv.org/abs/2004.07820v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2004.07820v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.SD", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.SD", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "eess.AS", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2004.07820v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2004.07820v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1401.6122v1", "guidislink": true, "updated": "2014-01-23T19:03:18Z", "updated_parsed": [2014, 1, 23, 19, 3, 18, 3, 23, 0], "published": "2014-01-23T19:03:18Z", "published_parsed": [2014, 1, 23, 19, 3, 18, 3, 23, 0], "title": "Identifying Bengali Multiword Expressions using Semantic Clustering", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=20&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Identifying Bengali Multiword Expressions using Semantic Clustering"}, "summary": "One of the key issues in both natural language understanding and generation\nis the appropriate processing of Multiword Expressions (MWEs). MWEs pose a huge\nproblem to the precise language processing due to their idiosyncratic nature\nand diversity in lexical, syntactical and semantic properties. The semantics of\na MWE cannot be expressed after combining the semantics of its constituents.\nTherefore, the formalism of semantic clustering is often viewed as an\ninstrument for extracting MWEs especially for resource constraint languages\nlike Bengali. The present semantic clustering approach contributes to locate\nclusters of the synonymous noun tokens present in the document. These clusters\nin turn help measure the similarity between the constituent words of a\npotentially candidate phrase using a vector space model and judge the\nsuitability of this phrase to be a MWE. In this experiment, we apply the\nsemantic clustering approach for noun-noun bigram MWEs, though it can be\nextended to any types of MWEs. In parallel, the well known statistical models,\nnamely Point-wise Mutual Information (PMI), Log Likelihood Ratio (LLR),\nSignificance function are also employed to extract MWEs from the Bengali\ncorpus. The comparative evaluation shows that the semantic clustering approach\noutperforms all other competing statistical models. As a by-product of this\nexperiment, we have started developing a standard lexicon in Bengali that\nserves as a productive Bengali linguistic thesaurus.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=20&max_results=10&sortBy=relevance&sortOrder=descending", "value": "One of the key issues in both natural language understanding and generation\nis the appropriate processing of Multiword Expressions (MWEs). MWEs pose a huge\nproblem to the precise language processing due to their idiosyncratic nature\nand diversity in lexical, syntactical and semantic properties. The semantics of\na MWE cannot be expressed after combining the semantics of its constituents.\nTherefore, the formalism of semantic clustering is often viewed as an\ninstrument for extracting MWEs especially for resource constraint languages\nlike Bengali. The present semantic clustering approach contributes to locate\nclusters of the synonymous noun tokens present in the document. These clusters\nin turn help measure the similarity between the constituent words of a\npotentially candidate phrase using a vector space model and judge the\nsuitability of this phrase to be a MWE. In this experiment, we apply the\nsemantic clustering approach for noun-noun bigram MWEs, though it can be\nextended to any types of MWEs. In parallel, the well known statistical models,\nnamely Point-wise Mutual Information (PMI), Log Likelihood Ratio (LLR),\nSignificance function are also employed to extract MWEs from the Bengali\ncorpus. The comparative evaluation shows that the semantic clustering approach\noutperforms all other competing statistical models. As a by-product of this\nexperiment, we have started developing a standard lexicon in Bengali that\nserves as a productive Bengali linguistic thesaurus."}, "authors": ["Tanmoy Chakraborty", "Dipankar Das", "Sivaji Bandyopadhyay"], "author_detail": {"name": "Sivaji Bandyopadhyay"}, "author": "Sivaji Bandyopadhyay", "arxiv_comment": "25 pages, 3 figures, 5 tables, International Journal of Linguistics\n  and Language Resources (Lingvistic{\\ae} Investigationes), 2014", "links": [{"href": "http://arxiv.org/abs/1401.6122v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1401.6122v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1401.6122v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1401.6122v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1906.03786v5", "guidislink": true, "updated": "2020-03-12T13:15:44Z", "updated_parsed": [2020, 3, 12, 13, 15, 44, 3, 72, 0], "published": "2019-06-10T03:31:58Z", "published_parsed": [2019, 6, 10, 3, 31, 58, 0, 161, 0], "title": "BDNet: Bengali Handwritten Numeral Digit Recognition based on Densely\n  connected Convolutional Neural Networks", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=20&max_results=10&sortBy=relevance&sortOrder=descending", "value": "BDNet: Bengali Handwritten Numeral Digit Recognition based on Densely\n  connected Convolutional Neural Networks"}, "summary": "Images of handwritten digits are different from natural images as the\norientation of a digit, as well as similarity of features of different digits,\nmakes confusion. On the other hand, deep convolutional neural networks are\nachieving huge success in computer vision problems, especially in image\nclassification. BDNet is a densely connected deep convolutional neural network\nmodel used to classify (recognize) Bengali handwritten numeral digits. It is\nend-to-end trained using ISI Bengali handwritten numeral dataset. During\ntraining, untraditional data preprocessing and augmentation techniques are used\nso that the trained model works on a different dataset. The model has achieved\nthe test accuracy of 99.775%(baseline was 99.40%) on the test dataset of ISI\nBengali handwritten numerals. So, the BDNet model gives 62.5% error reduction\ncompared to previous state-of-the-art models. Here we have also created a\ndataset of 1000 images of Bengali handwritten numerals to test the trained\nmodel, and it giving promising results. Codes, trained model and our own\ndataset are available at: {https://github.com/Sufianlab/BDNet}.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=20&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Images of handwritten digits are different from natural images as the\norientation of a digit, as well as similarity of features of different digits,\nmakes confusion. On the other hand, deep convolutional neural networks are\nachieving huge success in computer vision problems, especially in image\nclassification. BDNet is a densely connected deep convolutional neural network\nmodel used to classify (recognize) Bengali handwritten numeral digits. It is\nend-to-end trained using ISI Bengali handwritten numeral dataset. During\ntraining, untraditional data preprocessing and augmentation techniques are used\nso that the trained model works on a different dataset. The model has achieved\nthe test accuracy of 99.775%(baseline was 99.40%) on the test dataset of ISI\nBengali handwritten numerals. So, the BDNet model gives 62.5% error reduction\ncompared to previous state-of-the-art models. Here we have also created a\ndataset of 1000 images of Bengali handwritten numerals to test the trained\nmodel, and it giving promising results. Codes, trained model and our own\ndataset are available at: {https://github.com/Sufianlab/BDNet}."}, "authors": ["A. Sufian", "Anirudha Ghosh", "Avijit Naskar", "Farhana Sultana", "Jaya Sil", "M M Hafizur Rahman"], "author_detail": {"name": "M M Hafizur Rahman"}, "author": "M M Hafizur Rahman", "links": [{"title": "doi", "href": "http://dx.doi.org/10.1016/j.jksuci.2020.03.002", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/1906.03786v5", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1906.03786v5", "rel": "related", "type": "application/pdf"}], "arxiv_comment": "23 pages, 11 figures, 7 tables, Accepted Manuscript. Journal of King\n  Saud University - Computer and Information Sciences (2019)", "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1906.03786v5", "affiliation": "King Faisal University, Saudi Arabia", "arxiv_url": "http://arxiv.org/abs/1906.03786v5", "journal_reference": "Journal of King Saud University - Computer and Information\n  Sciences, Elsevier, Online, 2020", "doi": "10.1016/j.jksuci.2020.03.002"}
{"id": "http://arxiv.org/abs/2010.00170v3", "guidislink": true, "updated": "2021-01-13T17:19:52Z", "updated_parsed": [2021, 1, 13, 17, 19, 52, 2, 13, 0], "published": "2020-10-01T01:51:45Z", "published_parsed": [2020, 10, 1, 1, 51, 45, 3, 275, 0], "title": "A Large Multi-Target Dataset of Common Bengali Handwritten Graphemes", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=20&max_results=10&sortBy=relevance&sortOrder=descending", "value": "A Large Multi-Target Dataset of Common Bengali Handwritten Graphemes"}, "summary": "Latin has historically led the state-of-the-art in handwritten optical\ncharacter recognition (OCR) research. Adapting existing systems from Latin to\nalpha-syllabary languages is particularly challenging due to a sharp contrast\nbetween their orthographies. The segmentation of graphical constituents\ncorresponding to characters becomes significantly hard due to a cursive writing\nsystem and frequent use of diacritics in the alpha-syllabary family of\nlanguages. We propose a labeling scheme based on graphemes (linguistic segments\nof word formation) that makes segmentation in-side alpha-syllabary words linear\nand present the first dataset of Bengali handwritten graphemes that are\ncommonly used in an everyday context. The dataset contains 411k curated samples\nof 1295 unique commonly used Bengali graphemes. Additionally, the test set\ncontains 900 uncommon Bengali graphemes for out of dictionary performance\nevaluation. The dataset is open-sourced as a part of a public Handwritten\nGrapheme Classification Challenge on Kaggle to benchmark vision algorithms for\nmulti-target grapheme classification. The unique graphemes present in this\ndataset are selected based on commonality in the Google Bengali ASR corpus.\nFrom competition proceedings, we see that deep-learning methods can generalize\nto a large span of out of dictionary graphemes which are absent during\ntraining. Dataset and starter codes at www.kaggle.com/c/bengaliai-cv19.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=20&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Latin has historically led the state-of-the-art in handwritten optical\ncharacter recognition (OCR) research. Adapting existing systems from Latin to\nalpha-syllabary languages is particularly challenging due to a sharp contrast\nbetween their orthographies. The segmentation of graphical constituents\ncorresponding to characters becomes significantly hard due to a cursive writing\nsystem and frequent use of diacritics in the alpha-syllabary family of\nlanguages. We propose a labeling scheme based on graphemes (linguistic segments\nof word formation) that makes segmentation in-side alpha-syllabary words linear\nand present the first dataset of Bengali handwritten graphemes that are\ncommonly used in an everyday context. The dataset contains 411k curated samples\nof 1295 unique commonly used Bengali graphemes. Additionally, the test set\ncontains 900 uncommon Bengali graphemes for out of dictionary performance\nevaluation. The dataset is open-sourced as a part of a public Handwritten\nGrapheme Classification Challenge on Kaggle to benchmark vision algorithms for\nmulti-target grapheme classification. The unique graphemes present in this\ndataset are selected based on commonality in the Google Bengali ASR corpus.\nFrom competition proceedings, we see that deep-learning methods can generalize\nto a large span of out of dictionary graphemes which are absent during\ntraining. Dataset and starter codes at www.kaggle.com/c/bengaliai-cv19."}, "authors": ["Samiul Alam", "Tahsin Reasat", "Asif Shahriyar Sushmit", "Sadi Mohammad Siddiquee", "Fuad Rahman", "Mahady Hasan", "Ahmed Imtiaz Humayun"], "author_detail": {"name": "Ahmed Imtiaz Humayun"}, "author": "Ahmed Imtiaz Humayun", "arxiv_comment": "15 pages, 12 figures, 6 Tables, Submitted to CVPR-21", "links": [{"href": "http://arxiv.org/abs/2010.00170v3", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2010.00170v3", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2010.00170v3", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2010.00170v3", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/2012.12139v1", "guidislink": true, "updated": "2020-12-22T16:22:02Z", "updated_parsed": [2020, 12, 22, 16, 22, 2, 1, 357, 0], "published": "2020-12-22T16:22:02Z", "published_parsed": [2020, 12, 22, 16, 22, 2, 1, 357, 0], "title": "Image to Bengali Caption Generation Using Deep CNN and Bidirectional\n  Gated Recurrent Unit", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=20&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Image to Bengali Caption Generation Using Deep CNN and Bidirectional\n  Gated Recurrent Unit"}, "summary": "There is very little notable research on generating descriptions of the\nBengali language. About 243 million people speak in Bengali, and it is the 7th\nmost spoken language on the planet. The purpose of this research is to propose\na CNN and Bidirectional GRU based architecture model that generates natural\nlanguage captions in the Bengali language from an image. Bengali people can use\nthis research to break the language barrier and better understand each other's\nperspectives. It will also help many blind people with their everyday lives.\nThis paper used an encoder-decoder approach to generate captions. We used a\npre-trained Deep convolutional neural network (DCNN) called InceptonV3image\nembedding model as the encoder for analysis, classification, and annotation of\nthe dataset's images Bidirectional Gated Recurrent unit (BGRU) layer as the\ndecoder to generate captions. Argmax and Beam search is used to produce the\nhighest possible quality of the captions. A new dataset called BNATURE is used,\nwhich comprises 8000 images with five captions per image. It is used for\ntraining and testing the proposed model. We obtained BLEU-1, BLEU-2, BLEU-3,\nBLEU-4 and Meteor is 42.6, 27.95, 23, 66, 16.41, 28.7 respectively.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=20&max_results=10&sortBy=relevance&sortOrder=descending", "value": "There is very little notable research on generating descriptions of the\nBengali language. About 243 million people speak in Bengali, and it is the 7th\nmost spoken language on the planet. The purpose of this research is to propose\na CNN and Bidirectional GRU based architecture model that generates natural\nlanguage captions in the Bengali language from an image. Bengali people can use\nthis research to break the language barrier and better understand each other's\nperspectives. It will also help many blind people with their everyday lives.\nThis paper used an encoder-decoder approach to generate captions. We used a\npre-trained Deep convolutional neural network (DCNN) called InceptonV3image\nembedding model as the encoder for analysis, classification, and annotation of\nthe dataset's images Bidirectional Gated Recurrent unit (BGRU) layer as the\ndecoder to generate captions. Argmax and Beam search is used to produce the\nhighest possible quality of the captions. A new dataset called BNATURE is used,\nwhich comprises 8000 images with five captions per image. It is used for\ntraining and testing the proposed model. We obtained BLEU-1, BLEU-2, BLEU-3,\nBLEU-4 and Meteor is 42.6, 27.95, 23, 66, 16.41, 28.7 respectively."}, "authors": ["Al Momin Faruk", "Hasan Al Faraby", "Md. Muzahidul Azad", "Md. Riduyan Fedous", "Md. Kishor Morol"], "author_detail": {"name": "Md. Kishor Morol"}, "author": "Md. Kishor Morol", "arxiv_comment": "Accepted at ICCIT2020", "links": [{"href": "http://arxiv.org/abs/2012.12139v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2012.12139v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2012.12139v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2012.12139v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1405.7397v1", "guidislink": true, "updated": "2014-05-28T21:05:00Z", "updated_parsed": [2014, 5, 28, 21, 5, 0, 2, 148, 0], "published": "2014-05-28T21:05:00Z", "published_parsed": [2014, 5, 28, 21, 5, 0, 2, 148, 0], "title": "An HMM Based Named Entity Recognition System for Indian Languages: The\n  JU System at ICON 2013", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=30&max_results=10&sortBy=relevance&sortOrder=descending", "value": "An HMM Based Named Entity Recognition System for Indian Languages: The\n  JU System at ICON 2013"}, "summary": "This paper reports about our work in the ICON 2013 NLP TOOLS CONTEST on Named\nEntity Recognition. We submitted runs for Bengali, English, Hindi, Marathi,\nPunjabi, Tamil and Telugu. A statistical HMM (Hidden Markov Models) based model\nhas been used to implement our system. The system has been trained and tested\non the NLP TOOLS CONTEST: ICON 2013 datasets. Our system obtains F-measures of\n0.8599, 0.7704, 0.7520, 0.4289, 0.5455, 0.4466, and 0.4003 for Bengali,\nEnglish, Hindi, Marathi, Punjabi, Tamil and Telugu respectively.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=30&max_results=10&sortBy=relevance&sortOrder=descending", "value": "This paper reports about our work in the ICON 2013 NLP TOOLS CONTEST on Named\nEntity Recognition. We submitted runs for Bengali, English, Hindi, Marathi,\nPunjabi, Tamil and Telugu. A statistical HMM (Hidden Markov Models) based model\nhas been used to implement our system. The system has been trained and tested\non the NLP TOOLS CONTEST: ICON 2013 datasets. Our system obtains F-measures of\n0.8599, 0.7704, 0.7520, 0.4289, 0.5455, 0.4466, and 0.4003 for Bengali,\nEnglish, Hindi, Marathi, Punjabi, Tamil and Telugu respectively."}, "authors": ["Vivekananda Gayen", "Kamal Sarkar"], "author_detail": {"name": "Kamal Sarkar"}, "author": "Kamal Sarkar", "arxiv_comment": "The ICON 2013 tools contest on Named Entity Recognition in Indian\n  languages (IL) co-located with the 10th International Conference on Natural\n  Language Processing(ICON), CDAC Noida, India,18-20 December, 2013", "links": [{"href": "http://arxiv.org/abs/1405.7397v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1405.7397v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1405.7397v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1405.7397v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1409.8008v1", "guidislink": true, "updated": "2014-09-29T07:11:30Z", "updated_parsed": [2014, 9, 29, 7, 11, 30, 0, 272, 0], "published": "2014-09-29T07:11:30Z", "published_parsed": [2014, 9, 29, 7, 11, 30, 0, 272, 0], "title": "CRF-based Named Entity Recognition @ICON 2013", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=30&max_results=10&sortBy=relevance&sortOrder=descending", "value": "CRF-based Named Entity Recognition @ICON 2013"}, "summary": "This paper describes performance of CRF based systems for Named Entity\nRecognition (NER) in Indian language as a part of ICON 2013 shared task. In\nthis task we have considered a set of language independent features for all the\nlanguages. Only for English a language specific feature, i.e. capitalization,\nhas been added. Next the use of gazetteer is explored for Bengali, Hindi and\nEnglish. The gazetteers are built from Wikipedia and other sources. Test\nresults show that the system achieves the highest F measure of 88% for English\nand the lowest F measure of 69% for both Tamil and Telugu. Note that for the\nleast performing two languages no gazetteer was used. NER in Bengali and Hindi\nfinds accuracy (F measure) of 87% and 79%, respectively.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=30&max_results=10&sortBy=relevance&sortOrder=descending", "value": "This paper describes performance of CRF based systems for Named Entity\nRecognition (NER) in Indian language as a part of ICON 2013 shared task. In\nthis task we have considered a set of language independent features for all the\nlanguages. Only for English a language specific feature, i.e. capitalization,\nhas been added. Next the use of gazetteer is explored for Bengali, Hindi and\nEnglish. The gazetteers are built from Wikipedia and other sources. Test\nresults show that the system achieves the highest F measure of 88% for English\nand the lowest F measure of 69% for both Tamil and Telugu. Note that for the\nleast performing two languages no gazetteer was used. NER in Bengali and Hindi\nfinds accuracy (F measure) of 87% and 79%, respectively."}, "authors": ["Arjun Das", "Utpal Garain"], "author_detail": {"name": "Utpal Garain"}, "author": "Utpal Garain", "links": [{"href": "http://arxiv.org/abs/1409.8008v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1409.8008v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1409.8008v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1409.8008v1", "arxiv_comment": null, "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1706.03266v1", "guidislink": true, "updated": "2017-06-10T18:29:19Z", "updated_parsed": [2017, 6, 10, 18, 29, 19, 5, 161, 0], "published": "2017-06-10T18:29:19Z", "published_parsed": [2017, 6, 10, 18, 29, 19, 5, 161, 0], "title": "An Empirical Study of Some Selected IR Models for Bengali Monolingual\n  Information Retrieval", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=30&max_results=10&sortBy=relevance&sortOrder=descending", "value": "An Empirical Study of Some Selected IR Models for Bengali Monolingual\n  Information Retrieval"}, "summary": "This paper presents an evaluation and an analysis of some selected\ninformation retrieval models for Bengali monolingual information retrieval\ntask. Two models, TF-IDF model and the Okapi BM25 model have been considered\nfor our study. The developed IR models are tested on FIRE ad hoc retrieval data\nsets released for different years from 2008 to 2012 and the obtained results\nhave been reported in this paper.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=30&max_results=10&sortBy=relevance&sortOrder=descending", "value": "This paper presents an evaluation and an analysis of some selected\ninformation retrieval models for Bengali monolingual information retrieval\ntask. Two models, TF-IDF model and the Okapi BM25 model have been considered\nfor our study. The developed IR models are tested on FIRE ad hoc retrieval data\nsets released for different years from 2008 to 2012 and the obtained results\nhave been reported in this paper."}, "authors": ["Kamal Sarkar", "Avisek Gupta"], "author_detail": {"name": "Avisek Gupta"}, "author": "Avisek Gupta", "arxiv_comment": "6 pages, In Proceedings of ICBIM 2016", "links": [{"href": "http://arxiv.org/abs/1706.03266v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1706.03266v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.IR", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.IR", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1706.03266v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1706.03266v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1803.03859v2", "guidislink": true, "updated": "2018-06-27T16:26:55Z", "updated_parsed": [2018, 6, 27, 16, 26, 55, 2, 178, 0], "published": "2018-03-10T20:38:55Z", "published_parsed": [2018, 3, 10, 20, 38, 55, 5, 69, 0], "title": "Language Identification of Bengali-English Code-Mixed data using\n  Character & Phonetic based LSTM Models", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=30&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Language Identification of Bengali-English Code-Mixed data using\n  Character & Phonetic based LSTM Models"}, "summary": "Language identification of social media text still remains a challenging task\ndue to properties like code-mixing and inconsistent phonetic transliterations.\nIn this paper, we present a supervised learning approach for language\nidentification at the word level of low resource Bengali-English code-mixed\ndata taken from social media. We employ two methods of word encoding, namely\ncharacter based and root phone based to train our deep LSTM models. Utilizing\nthese two models we created two ensemble models using stacking and threshold\ntechnique which gave 91.78% and 92.35% accuracies respectively on our testing\ndata.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=30&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Language identification of social media text still remains a challenging task\ndue to properties like code-mixing and inconsistent phonetic transliterations.\nIn this paper, we present a supervised learning approach for language\nidentification at the word level of low resource Bengali-English code-mixed\ndata taken from social media. We employ two methods of word encoding, namely\ncharacter based and root phone based to train our deep LSTM models. Utilizing\nthese two models we created two ensemble models using stacking and threshold\ntechnique which gave 91.78% and 92.35% accuracies respectively on our testing\ndata."}, "authors": ["Soumil Mandal", "Sourya Dipta Das", "Dipankar Das"], "author_detail": {"name": "Dipankar Das"}, "author": "Dipankar Das", "arxiv_comment": "6 pages, 5 figures, 5 tables", "links": [{"href": "http://arxiv.org/abs/1803.03859v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1803.03859v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1803.03859v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1803.03859v2", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1911.03059v2", "guidislink": true, "updated": "2019-11-19T16:37:41Z", "updated_parsed": [2019, 11, 19, 16, 37, 41, 1, 323, 0], "published": "2019-11-08T05:30:33Z", "published_parsed": [2019, 11, 8, 5, 30, 33, 4, 312, 0], "title": "A Comprehensive Comparison of Machine Learning Based Methods Used in\n  Bengali Question Classification", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=30&max_results=10&sortBy=relevance&sortOrder=descending", "value": "A Comprehensive Comparison of Machine Learning Based Methods Used in\n  Bengali Question Classification"}, "summary": "QA classification system maps questions asked by humans to an appropriate\nanswer category. A sound question classification (QC) system model is the\npre-requisite of a sound QA system. This work demonstrates phases of assembling\na QA type classification model. We present a comprehensive comparison\n(performance and computational complexity) among some machine learning based\napproaches used in QC for Bengali language.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=30&max_results=10&sortBy=relevance&sortOrder=descending", "value": "QA classification system maps questions asked by humans to an appropriate\nanswer category. A sound question classification (QC) system model is the\npre-requisite of a sound QA system. This work demonstrates phases of assembling\na QA type classification model. We present a comprehensive comparison\n(performance and computational complexity) among some machine learning based\napproaches used in QC for Bengali language."}, "authors": ["Afra Anika", "Md. Hasibur Rahman", "Salekul Islam", "Abu Shafin Mohammad Mahdee Jameel", "Chowdhury Rafeed Rahman"], "author_detail": {"name": "Chowdhury Rafeed Rahman"}, "author": "Chowdhury Rafeed Rahman", "links": [{"href": "http://arxiv.org/abs/1911.03059v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1911.03059v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1911.03059v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1911.03059v2", "arxiv_comment": null, "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1208.0590v1", "guidislink": true, "updated": "2012-08-02T16:58:39Z", "updated_parsed": [2012, 8, 2, 16, 58, 39, 3, 215, 0], "published": "2012-08-02T16:58:39Z", "published_parsed": [2012, 8, 2, 16, 58, 39, 3, 215, 0], "title": "Translation of Bengali Terms in Mobile Phones: a Simplified Approach\n  Based on the Prescriptions of Conventional Accent Understand Ability", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=30&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Translation of Bengali Terms in Mobile Phones: a Simplified Approach\n  Based on the Prescriptions of Conventional Accent Understand Ability"}, "summary": "Technology is the making, usage and knowledge of tools, techniques, crafts,\nsystems or methods of organization in order to solve a problem or serve some\npurpose. This is true for humanitarian issues also. Such as the issue of\nlanguage and its primitive attraction for its native speakers which is visible\nin the cases of the language spoken at home, outside home, in its choice of\nnewspapers, and TV channels. Everyone finds to accomplish its need by the same\nway. Example includes the preference of using mobile phones in English. The\nsatisfactory answer to this tendency may be the lack of finding the\ntranslations in native language---Bengali terms used in current mobile phones\nare hard to understand by users. I have investigated various mobile phone\nmodels available in Indian market which have lot of problems in Bengali\ninterpretation. I have sort out the root cause of this problem to be the\nconventional accent understand ability. Depending on this I have created a set\nof equivalent terms that I hope to be simpler in use. In this paper I have\nperformed experiments to compare the new terms to the available ones. Our\nfindings show that the newly derived terms do better in term of performance\nthan to current ones. It has also been seen that acceptance of Bengali terms in\nmobile phones might grow if the parameter of simpler and conventional accent\nunderstand ability are met while designing.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=30&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Technology is the making, usage and knowledge of tools, techniques, crafts,\nsystems or methods of organization in order to solve a problem or serve some\npurpose. This is true for humanitarian issues also. Such as the issue of\nlanguage and its primitive attraction for its native speakers which is visible\nin the cases of the language spoken at home, outside home, in its choice of\nnewspapers, and TV channels. Everyone finds to accomplish its need by the same\nway. Example includes the preference of using mobile phones in English. The\nsatisfactory answer to this tendency may be the lack of finding the\ntranslations in native language---Bengali terms used in current mobile phones\nare hard to understand by users. I have investigated various mobile phone\nmodels available in Indian market which have lot of problems in Bengali\ninterpretation. I have sort out the root cause of this problem to be the\nconventional accent understand ability. Depending on this I have created a set\nof equivalent terms that I hope to be simpler in use. In this paper I have\nperformed experiments to compare the new terms to the available ones. Our\nfindings show that the newly derived terms do better in term of performance\nthan to current ones. It has also been seen that acceptance of Bengali terms in\nmobile phones might grow if the parameter of simpler and conventional accent\nunderstand ability are met while designing."}, "authors": ["Partha Pratim Ray"], "author_detail": {"name": "Partha Pratim Ray"}, "author": "Partha Pratim Ray", "arxiv_comment": "ISSN 2249-2593", "links": [{"href": "http://arxiv.org/abs/1208.0590v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1208.0590v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.HC", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.HC", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1208.0590v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1208.0590v1", "journal_reference": "International Journal of Computer and Organization Trends (IJCOT)\n  2(1) 33-38 (2012)", "doi": null}
{"id": "http://arxiv.org/abs/2004.00089v1", "guidislink": true, "updated": "2020-03-31T20:22:10Z", "updated_parsed": [2020, 3, 31, 20, 22, 10, 1, 91, 0], "published": "2020-03-31T20:22:10Z", "published_parsed": [2020, 3, 31, 20, 22, 10, 1, 91, 0], "title": "Automatic Extraction of Bengali Root Verbs using Paninian Grammar", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=30&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Automatic Extraction of Bengali Root Verbs using Paninian Grammar"}, "summary": "In this research work, we have proposed an algorithm based on supervised\nlearning methodology to extract the root forms of the Bengali verbs using the\ngrammatical rules proposed by Panini [1] in Ashtadhyayi. This methodology can\nbe applied for the languages which are derived from Sanskrit. The proposed\nsystem has been developed based on tense, person and morphological inflections\nof the verbs to find their root forms. The work has been executed in two\nphases: first, the surface level forms or inflected forms of the verbs have\nbeen classified into a certain number of groups of similar tense and person.\nFor this task, a standard pattern, available in Bengali language has been used.\nNext, a set of rules have been applied to extract the root form from the\nsurface level forms of a verb. The system has been tested on 10000 verbs\ncollected from the Bengali text corpus developed in the TDIL project of the\nGovt. of India. The accuracy of the output has been achieved 98% which is\nverified by a linguistic expert. Root verb identification is a key step in\nsemantic searching, multi-sentence search query processing, understanding the\nmeaning of a language, disambiguation of word sense, classification of the\nsentences etc.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=30&max_results=10&sortBy=relevance&sortOrder=descending", "value": "In this research work, we have proposed an algorithm based on supervised\nlearning methodology to extract the root forms of the Bengali verbs using the\ngrammatical rules proposed by Panini [1] in Ashtadhyayi. This methodology can\nbe applied for the languages which are derived from Sanskrit. The proposed\nsystem has been developed based on tense, person and morphological inflections\nof the verbs to find their root forms. The work has been executed in two\nphases: first, the surface level forms or inflected forms of the verbs have\nbeen classified into a certain number of groups of similar tense and person.\nFor this task, a standard pattern, available in Bengali language has been used.\nNext, a set of rules have been applied to extract the root form from the\nsurface level forms of a verb. The system has been tested on 10000 verbs\ncollected from the Bengali text corpus developed in the TDIL project of the\nGovt. of India. The accuracy of the output has been achieved 98% which is\nverified by a linguistic expert. Root verb identification is a key step in\nsemantic searching, multi-sentence search query processing, understanding the\nmeaning of a language, disambiguation of word sense, classification of the\nsentences etc."}, "authors": ["Arijit Das", "Tapas Halder", "Diganta Saha"], "author_detail": {"name": "Diganta Saha"}, "author": "Diganta Saha", "links": [{"title": "doi", "href": "http://dx.doi.org/10.1109/RTEICT.2017.8256739", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/2004.00089v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2004.00089v1", "rel": "related", "type": "application/pdf"}], "arxiv_comment": "published in 2017 2nd IEEE International Conference on Recent Trends\n  in Electronics, Information & Communication Technology (RTEICT), Bangalore,\n  2017", "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2004.00089v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2004.00089v1", "journal_reference": null, "doi": "10.1109/RTEICT.2017.8256739"}
{"id": "http://arxiv.org/abs/2004.07807v2", "guidislink": true, "updated": "2020-04-19T17:21:30Z", "updated_parsed": [2020, 4, 19, 17, 21, 30, 6, 110, 0], "published": "2020-04-11T22:17:04Z", "published_parsed": [2020, 4, 11, 22, 17, 4, 5, 102, 0], "title": "Classification Benchmarks for Under-resourced Bengali Language based on\n  Multichannel Convolutional-LSTM Network", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=30&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Classification Benchmarks for Under-resourced Bengali Language based on\n  Multichannel Convolutional-LSTM Network"}, "summary": "Exponential growths of social media and micro-blogging sites not only provide\nplatforms for empowering freedom of expressions and individual voices but also\nenables people to express anti-social behaviour like online harassment,\ncyberbullying, and hate speech. Numerous works have been proposed to utilize\nthese data for social and anti-social behaviours analysis, document\ncharacterization, and sentiment analysis by predicting the contexts mostly for\nhighly resourced languages such as English. However, there are languages that\nare under-resources, e.g., South Asian languages like Bengali, Tamil, Assamese,\nTelugu that lack of computational resources for the NLP tasks. In this paper,\nwe provide several classification benchmarks for Bengali, an under-resourced\nlanguage. We prepared three datasets of expressing hate, commonly used topics,\nand opinions for hate speech detection, document classification, and sentiment\nanalysis, respectively. We built the largest Bengali word embedding models to\ndate based on 250 million articles, which we call BengFastText. We perform\nthree different experiments, covering document classification, sentiment\nanalysis, and hate speech detection. We incorporate word embeddings into a\nMultichannel Convolutional-LSTM (MConv-LSTM) network for predicting different\ntypes of hate speech, document classification, and sentiment analysis.\nExperiments demonstrate that BengFastText can capture the semantics of words\nfrom respective contexts correctly. Evaluations against several baseline\nembedding models, e.g., Word2Vec and GloVe yield up to 92.30%, 82.25%, and\n90.45% F1-scores in case of document classification, sentiment analysis, and\nhate speech detection, respectively during 5-fold cross-validation tests.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=30&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Exponential growths of social media and micro-blogging sites not only provide\nplatforms for empowering freedom of expressions and individual voices but also\nenables people to express anti-social behaviour like online harassment,\ncyberbullying, and hate speech. Numerous works have been proposed to utilize\nthese data for social and anti-social behaviours analysis, document\ncharacterization, and sentiment analysis by predicting the contexts mostly for\nhighly resourced languages such as English. However, there are languages that\nare under-resources, e.g., South Asian languages like Bengali, Tamil, Assamese,\nTelugu that lack of computational resources for the NLP tasks. In this paper,\nwe provide several classification benchmarks for Bengali, an under-resourced\nlanguage. We prepared three datasets of expressing hate, commonly used topics,\nand opinions for hate speech detection, document classification, and sentiment\nanalysis, respectively. We built the largest Bengali word embedding models to\ndate based on 250 million articles, which we call BengFastText. We perform\nthree different experiments, covering document classification, sentiment\nanalysis, and hate speech detection. We incorporate word embeddings into a\nMultichannel Convolutional-LSTM (MConv-LSTM) network for predicting different\ntypes of hate speech, document classification, and sentiment analysis.\nExperiments demonstrate that BengFastText can capture the semantics of words\nfrom respective contexts correctly. Evaluations against several baseline\nembedding models, e.g., Word2Vec and GloVe yield up to 92.30%, 82.25%, and\n90.45% F1-scores in case of document classification, sentiment analysis, and\nhate speech detection, respectively during 5-fold cross-validation tests."}, "authors": ["Md. Rezaul Karim", "Bharathi Raja Chakravarthi", "John P. McCrae", "Michael Cochez"], "author_detail": {"name": "Michael Cochez"}, "author": "Michael Cochez", "arxiv_comment": "This paper is under review in the Journal of Natural Language\n  Engineering", "links": [{"href": "http://arxiv.org/abs/2004.07807v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2004.07807v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.ML", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2004.07807v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2004.07807v2", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/2007.10718v1", "guidislink": true, "updated": "2020-07-21T11:21:26Z", "updated_parsed": [2020, 7, 21, 11, 21, 26, 1, 203, 0], "published": "2020-07-21T11:21:26Z", "published_parsed": [2020, 7, 21, 11, 21, 26, 1, 203, 0], "title": "Human Abnormality Detection Based on Bengali Text", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=30&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Human Abnormality Detection Based on Bengali Text"}, "summary": "In the field of natural language processing and human-computer interaction,\nhuman attitudes and sentiments have attracted the researchers. However, in the\nfield of human-computer interaction, human abnormality detection has not been\ninvestigated extensively and most works depend on image-based information. In\nnatural language processing, effective meaning can potentially convey by all\nwords. Each word may bring out difficult encounters because of their semantic\nconnection with ideas or categories. In this paper, an efficient and effective\nhuman abnormality detection model is introduced, that only uses Bengali text.\nThis proposed model can recognize whether the person is in a normal or abnormal\nstate by analyzing their typed Bengali text. To the best of our knowledge, this\nis the first attempt in developing a text based human abnormality detection\nsystem. We have created our Bengali dataset (contains 2000 sentences) that is\ngenerated by voluntary conversations. We have performed the comparative\nanalysis by using Naive Bayes and Support Vector Machine as classifiers. Two\ndifferent feature extraction techniques count vector, and TF-IDF is used to\nexperiment on our constructed dataset. We have achieved a maximum 89% accuracy\nand 92% F1-score with our constructed dataset in our experiment.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=30&max_results=10&sortBy=relevance&sortOrder=descending", "value": "In the field of natural language processing and human-computer interaction,\nhuman attitudes and sentiments have attracted the researchers. However, in the\nfield of human-computer interaction, human abnormality detection has not been\ninvestigated extensively and most works depend on image-based information. In\nnatural language processing, effective meaning can potentially convey by all\nwords. Each word may bring out difficult encounters because of their semantic\nconnection with ideas or categories. In this paper, an efficient and effective\nhuman abnormality detection model is introduced, that only uses Bengali text.\nThis proposed model can recognize whether the person is in a normal or abnormal\nstate by analyzing their typed Bengali text. To the best of our knowledge, this\nis the first attempt in developing a text based human abnormality detection\nsystem. We have created our Bengali dataset (contains 2000 sentences) that is\ngenerated by voluntary conversations. We have performed the comparative\nanalysis by using Naive Bayes and Support Vector Machine as classifiers. Two\ndifferent feature extraction techniques count vector, and TF-IDF is used to\nexperiment on our constructed dataset. We have achieved a maximum 89% accuracy\nand 92% F1-score with our constructed dataset in our experiment."}, "authors": ["M. F. Mridha", "Md. Saifur Rahman", "Abu Quwsar Ohi"], "author_detail": {"name": "Abu Quwsar Ohi"}, "author": "Abu Quwsar Ohi", "arxiv_comment": "The paper is accepted in IEEE Region 10 Symposium (TENSYMP) 2020", "links": [{"href": "http://arxiv.org/abs/2007.10718v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2007.10718v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.IR", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2007.10718v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2007.10718v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/2012.00288v1", "guidislink": true, "updated": "2020-12-01T06:09:44Z", "updated_parsed": [2020, 12, 1, 6, 9, 44, 1, 336, 0], "published": "2020-12-01T06:09:44Z", "published_parsed": [2020, 12, 1, 6, 9, 44, 1, 336, 0], "title": "BAN-ABSA: An Aspect-Based Sentiment Analysis dataset for Bengali and\n  it's baseline evaluation", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=30&max_results=10&sortBy=relevance&sortOrder=descending", "value": "BAN-ABSA: An Aspect-Based Sentiment Analysis dataset for Bengali and\n  it's baseline evaluation"}, "summary": "Due to the breathtaking growth of social media or newspaper user comments,\nonline product reviews comments, sentiment analysis (SA) has captured\nsubstantial interest from the researchers. With the fast increase of domain, SA\nwork aims not only to predict the sentiment of a sentence or document but also\nto give the necessary detail on different aspects of the sentence or document\n(i.e. aspect-based sentiment analysis). A considerable number of datasets for\nSA and aspect-based sentiment analysis (ABSA) have been made available for\nEnglish and other well-known European languages. In this paper, we present a\nmanually annotated Bengali dataset of high quality, BAN-ABSA, which is\nannotated with aspect and its associated sentiment by 3 native Bengali\nspeakers. The dataset consists of 2,619 positive, 4,721 negative and 1,669\nneutral data samples from 9,009 unique comments gathered from some famous\nBengali news portals. In addition, we conducted a baseline evaluation with a\nfocus on deep learning model, achieved an accuracy of 78.75% for aspect term\nextraction and accuracy of 71.08% for sentiment classification. Experiments on\nthe BAN-ABSA dataset show that the CNN model is better in terms of accuracy\nthough Bi-LSTM significantly outperforms CNN model in terms of average\nF1-score.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=30&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Due to the breathtaking growth of social media or newspaper user comments,\nonline product reviews comments, sentiment analysis (SA) has captured\nsubstantial interest from the researchers. With the fast increase of domain, SA\nwork aims not only to predict the sentiment of a sentence or document but also\nto give the necessary detail on different aspects of the sentence or document\n(i.e. aspect-based sentiment analysis). A considerable number of datasets for\nSA and aspect-based sentiment analysis (ABSA) have been made available for\nEnglish and other well-known European languages. In this paper, we present a\nmanually annotated Bengali dataset of high quality, BAN-ABSA, which is\nannotated with aspect and its associated sentiment by 3 native Bengali\nspeakers. The dataset consists of 2,619 positive, 4,721 negative and 1,669\nneutral data samples from 9,009 unique comments gathered from some famous\nBengali news portals. In addition, we conducted a baseline evaluation with a\nfocus on deep learning model, achieved an accuracy of 78.75% for aspect term\nextraction and accuracy of 71.08% for sentiment classification. Experiments on\nthe BAN-ABSA dataset show that the CNN model is better in terms of accuracy\nthough Bi-LSTM significantly outperforms CNN model in terms of average\nF1-score."}, "authors": ["Mahfuz Ahmed Masum", "Sheikh Junayed Ahmed", "Ayesha Tasnim", "Md Saiful Islam"], "author_detail": {"name": "Md Saiful Islam"}, "author": "Md Saiful Islam", "arxiv_comment": "11 pages,2 figures, 8 tables Included in proceedings of International\n  Joint Conference on Advances in Computational Intelligence (IJCACI) 2020", "links": [{"href": "http://arxiv.org/abs/2012.00288v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2012.00288v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "68T50", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "I.2.7", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2012.00288v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2012.00288v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/2012.09686v1", "guidislink": true, "updated": "2020-12-17T15:53:54Z", "updated_parsed": [2020, 12, 17, 15, 53, 54, 3, 352, 0], "published": "2020-12-17T15:53:54Z", "published_parsed": [2020, 12, 17, 15, 53, 54, 3, 352, 0], "title": "Hate Speech detection in the Bengali language: A dataset and its\n  baseline evaluation", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=40&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Hate Speech detection in the Bengali language: A dataset and its\n  baseline evaluation"}, "summary": "Social media sites such as YouTube and Facebook have become an integral part\nof everyone's life and in the last few years, hate speech in the social media\ncomment section has increased rapidly. Detection of hate speech on social media\nwebsites faces a variety of challenges including small imbalanced data sets,\nthe findings of an appropriate model and also the choice of feature analysis\nmethod. further more, this problem is more severe for the Bengali speaking\ncommunity due to the lack of gold standard labelled datasets. This paper\npresents a new dataset of 30,000 user comments tagged by crowd sourcing and\nvarified by experts. All the comments are collected from YouTube and Facebook\ncomment section and classified into seven categories: sports, entertainment,\nreligion, politics, crime, celebrity and TikTok & meme. A total of 50\nannotators annotated each comment three times and the majority vote was taken\nas the final annotation. Nevertheless, we have conducted base line experiments\nand several deep learning models along with extensive pre-trained Bengali word\nembedding such as Word2Vec, FastText and BengFastText on this dataset to\nfacilitate future research opportunities. The experiment illustrated that\nalthough all deep learning models performed well, SVM achieved the best result\nwith 87.5% accuracy. Our core contribution is to make this benchmark dataset\navailable and accessible to facilitate further research in the field of in the\nfield of Bengali hate speech detection.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=40&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Social media sites such as YouTube and Facebook have become an integral part\nof everyone's life and in the last few years, hate speech in the social media\ncomment section has increased rapidly. Detection of hate speech on social media\nwebsites faces a variety of challenges including small imbalanced data sets,\nthe findings of an appropriate model and also the choice of feature analysis\nmethod. further more, this problem is more severe for the Bengali speaking\ncommunity due to the lack of gold standard labelled datasets. This paper\npresents a new dataset of 30,000 user comments tagged by crowd sourcing and\nvarified by experts. All the comments are collected from YouTube and Facebook\ncomment section and classified into seven categories: sports, entertainment,\nreligion, politics, crime, celebrity and TikTok & meme. A total of 50\nannotators annotated each comment three times and the majority vote was taken\nas the final annotation. Nevertheless, we have conducted base line experiments\nand several deep learning models along with extensive pre-trained Bengali word\nembedding such as Word2Vec, FastText and BengFastText on this dataset to\nfacilitate future research opportunities. The experiment illustrated that\nalthough all deep learning models performed well, SVM achieved the best result\nwith 87.5% accuracy. Our core contribution is to make this benchmark dataset\navailable and accessible to facilitate further research in the field of in the\nfield of Bengali hate speech detection."}, "authors": ["Nauros Romim", "Mosahed Ahmed", "Hriteshwar Talukder", "Md Saiful Islam"], "author_detail": {"name": "Md Saiful Islam"}, "author": "Md Saiful Islam", "arxiv_comment": "13 pages, 02 figures. To appear on International Joint Conference on\n  Advances in Computational Intelligence, 20-21 November 2020", "links": [{"href": "http://arxiv.org/abs/2012.09686v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2012.09686v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2012.09686v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2012.09686v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/2012.14353v1", "guidislink": true, "updated": "2020-12-28T16:46:03Z", "updated_parsed": [2020, 12, 28, 16, 46, 3, 0, 363, 0], "published": "2020-12-28T16:46:03Z", "published_parsed": [2020, 12, 28, 16, 46, 3, 0, 363, 0], "title": "DeepHateExplainer: Explainable Hate Speech Detection in Under-resourced\n  Bengali Language", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=40&max_results=10&sortBy=relevance&sortOrder=descending", "value": "DeepHateExplainer: Explainable Hate Speech Detection in Under-resourced\n  Bengali Language"}, "summary": "Exponential growths of social media and micro-blogging sites not only provide\nplatforms for empowering freedom of expressions and individual voices, but also\nenables people to express anti-social behavior like online harassment,\ncyberbullying, and hate speech. Numerous works have been proposed to utilize\nthese data for social and anti-social behavior analysis, by predicting the\ncontexts mostly for highly-resourced languages like English. However, some\nlanguages such as Bengali are under-resourced that lack of computational\nresources for natural language processing(NLP). In this paper, we propose an\nexplainable approach for hate speech detection from under-resourced Bengali\nlanguage, which we called DeepHateExplainer. In our approach, Bengali texts are\nfirst comprehensively preprocessed, before classifying them into political,\npersonal, geopolitical, and religious hates, by employing neural ensemble of\ndifferent transformer-based neural architectures(i.e., monolingual Bangla\nBERT-base, multilingual BERT-cased and uncased, and XLM-RoBERTa), followed by\nidentifying important terms with sensitivity analysis and layer-wise relevance\npropagation(LRP) to provide human-interpretable explanations. Evaluations\nagainst several machine learning~(linear and tree-based models) and deep neural\nnetworks (i.e., CNN, Bi-LSTM, and Conv-LSTM with word embeddings) baselines\nyield F1 scores of 84%, 90%, 88%, and 88%, for political, personal,\ngeopolitical, and religious hates, respectively, during 3-fold cross-validation\ntests.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=40&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Exponential growths of social media and micro-blogging sites not only provide\nplatforms for empowering freedom of expressions and individual voices, but also\nenables people to express anti-social behavior like online harassment,\ncyberbullying, and hate speech. Numerous works have been proposed to utilize\nthese data for social and anti-social behavior analysis, by predicting the\ncontexts mostly for highly-resourced languages like English. However, some\nlanguages such as Bengali are under-resourced that lack of computational\nresources for natural language processing(NLP). In this paper, we propose an\nexplainable approach for hate speech detection from under-resourced Bengali\nlanguage, which we called DeepHateExplainer. In our approach, Bengali texts are\nfirst comprehensively preprocessed, before classifying them into political,\npersonal, geopolitical, and religious hates, by employing neural ensemble of\ndifferent transformer-based neural architectures(i.e., monolingual Bangla\nBERT-base, multilingual BERT-cased and uncased, and XLM-RoBERTa), followed by\nidentifying important terms with sensitivity analysis and layer-wise relevance\npropagation(LRP) to provide human-interpretable explanations. Evaluations\nagainst several machine learning~(linear and tree-based models) and deep neural\nnetworks (i.e., CNN, Bi-LSTM, and Conv-LSTM with word embeddings) baselines\nyield F1 scores of 84%, 90%, 88%, and 88%, for political, personal,\ngeopolitical, and religious hates, respectively, during 3-fold cross-validation\ntests."}, "authors": ["Md. Rezaul Karim", "Sumon Kanti Dey", "Bharathi Raja Chakravarthi"], "author_detail": {"name": "Bharathi Raja Chakravarthi"}, "author": "Bharathi Raja Chakravarthi", "arxiv_comment": "Extended version of this paper is currently under review in the IEEE\n  Access journal", "links": [{"href": "http://arxiv.org/abs/2012.14353v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2012.14353v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2012.14353v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2012.14353v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1009.4590v1", "guidislink": true, "updated": "2010-09-23T12:01:38Z", "updated_parsed": [2010, 9, 23, 12, 1, 38, 3, 266, 0], "published": "2010-09-23T12:01:38Z", "published_parsed": [2010, 9, 23, 12, 1, 38, 3, 266, 0], "title": "A Unique 10 Segment Display for Bengali Numerals", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=40&max_results=10&sortBy=relevance&sortOrder=descending", "value": "A Unique 10 Segment Display for Bengali Numerals"}, "summary": "Segmented display is widely used for efficient display of alphanumeric\ncharacters. English numerals are displayed by 7 segment and 16 segment display.\nThe segment size is uniform in this two display architecture. Display\narchitecture using 8, 10, 11, 18 segments have been proposed for Bengali\nnumerals 0...9 yet no display architecture is designed using segments of\nuniform size and uniform power consumption. In this paper we have proposed a\nuniform 10 segment architecture for Bengali numerals. This segment architecture\nuses segments of uniform size and no bent segment is used.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=40&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Segmented display is widely used for efficient display of alphanumeric\ncharacters. English numerals are displayed by 7 segment and 16 segment display.\nThe segment size is uniform in this two display architecture. Display\narchitecture using 8, 10, 11, 18 segments have been proposed for Bengali\nnumerals 0...9 yet no display architecture is designed using segments of\nuniform size and uniform power consumption. In this paper we have proposed a\nuniform 10 segment architecture for Bengali numerals. This segment architecture\nuses segments of uniform size and no bent segment is used."}, "authors": ["Md. Abul Kalam Azad", "Rezwana Sharmeen", "Shabbir Ahmad", "S. M. Kamruzzaman"], "author_detail": {"name": "S. M. Kamruzzaman"}, "author": "S. M. Kamruzzaman", "arxiv_comment": "3 Pages, International Conference", "links": [{"href": "http://arxiv.org/abs/1009.4590v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1009.4590v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.AR", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.AR", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1009.4590v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1009.4590v1", "journal_reference": "Proc. 8th International Conference on Computer and Information\n  Technology (ICCIT 2005), Dhaka, Bangladesh, pp. 97-99, Dec. 2005", "doi": null}
{"id": "http://arxiv.org/abs/1009.4979v1", "guidislink": true, "updated": "2010-09-25T06:27:49Z", "updated_parsed": [2010, 9, 25, 6, 27, 49, 5, 268, 0], "published": "2010-09-25T06:27:49Z", "published_parsed": [2010, 9, 25, 6, 27, 49, 5, 268, 0], "title": "Smart Bengali Cell Phone Keypad Layout", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=40&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Smart Bengali Cell Phone Keypad Layout"}, "summary": "Nowadays cell phone is the most common communicating used by mass people. SMS\nbased communication is a cheap and popular communication method. It is human\ntendency to have the opportunity to write SMS in their mother language. Text\ninput in mother language is more flexible when the alphabets of that language\nare printed on the keypad. Bangla mobile keypad based on phonetics has been\nproposed earlier. But the keypad is not scientific from frequency and\nflexibility point of view. Since it is not a feasible solution in this paper we\nhave proposed an efficient Bengali keypad for cell phone and other cellular\ndevice. The proposed keypad is based on the frequency of the alphabets in\nBengali language and also with the view of structure of human finger movements.\nWe took the two points in count to provide a flexible and fast cell phone\nkeypad.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=40&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Nowadays cell phone is the most common communicating used by mass people. SMS\nbased communication is a cheap and popular communication method. It is human\ntendency to have the opportunity to write SMS in their mother language. Text\ninput in mother language is more flexible when the alphabets of that language\nare printed on the keypad. Bangla mobile keypad based on phonetics has been\nproposed earlier. But the keypad is not scientific from frequency and\nflexibility point of view. Since it is not a feasible solution in this paper we\nhave proposed an efficient Bengali keypad for cell phone and other cellular\ndevice. The proposed keypad is based on the frequency of the alphabets in\nBengali language and also with the view of structure of human finger movements.\nWe took the two points in count to provide a flexible and fast cell phone\nkeypad."}, "authors": ["Md. Abul Kalam Azad", "Rezwana Sharmeen", "Shabbir Ahmad", "S. M. Kamruzzaman"], "author_detail": {"name": "S. M. Kamruzzaman"}, "author": "S. M. Kamruzzaman", "arxiv_comment": "4 Pages, International Conference", "links": [{"href": "http://arxiv.org/abs/1009.4979v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1009.4979v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.HC", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.HC", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1009.4979v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1009.4979v1", "journal_reference": "Proc. 8th International Conference on Computer and Information\n  Technology (ICCIT 2005), Dhaka, Bangladesh, pp. 1208-1211, Dec. 2005", "doi": null}
{"id": "http://arxiv.org/abs/1210.3729v1", "guidislink": true, "updated": "2012-10-13T18:02:26Z", "updated_parsed": [2012, 10, 13, 18, 2, 26, 5, 287, 0], "published": "2012-10-13T18:02:26Z", "published_parsed": [2012, 10, 13, 18, 2, 26, 5, 287, 0], "title": "Inference of Fine-grained Attributes of Bengali Corpus for Stylometry\n  Detection", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=40&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Inference of Fine-grained Attributes of Bengali Corpus for Stylometry\n  Detection"}, "summary": "Stylometry, the science of inferring characteristics of the author from the\ncharacteristics of documents written by that author, is a problem with a long\nhistory and belongs to the core task of Text categorization that involves\nauthorship identification, plagiarism detection, forensic investigation,\ncomputer security, copyright and estate disputes etc. In this work, we present\na strategy for stylometry detection of documents written in Bengali. We adopt a\nset of fine-grained attribute features with a set of lexical markers for the\nanalysis of the text and use three semi-supervised measures for making\ndecisions. Finally, a majority voting approach has been taken for final\nclassification. The system is fully automatic and language-independent.\nEvaluation results of our attempt for Bengali author's stylometry detection\nshow reasonably promising accuracy in comparison to the baseline model.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=40&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Stylometry, the science of inferring characteristics of the author from the\ncharacteristics of documents written by that author, is a problem with a long\nhistory and belongs to the core task of Text categorization that involves\nauthorship identification, plagiarism detection, forensic investigation,\ncomputer security, copyright and estate disputes etc. In this work, we present\na strategy for stylometry detection of documents written in Bengali. We adopt a\nset of fine-grained attribute features with a set of lexical markers for the\nanalysis of the text and use three semi-supervised measures for making\ndecisions. Finally, a majority voting approach has been taken for final\nclassification. The system is fully automatic and language-independent.\nEvaluation results of our attempt for Bengali author's stylometry detection\nshow reasonably promising accuracy in comparison to the baseline model."}, "authors": ["Tanmoy Chakraborty", "Sivaji Bandyopadhyay"], "author_detail": {"name": "Sivaji Bandyopadhyay"}, "author": "Sivaji Bandyopadhyay", "arxiv_comment": "5 pages, 2 figures, 4 tables. arXiv admin note: substantial text\n  overlap with arXiv:1208.6268", "links": [{"href": "http://arxiv.org/abs/1210.3729v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1210.3729v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1210.3729v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1210.3729v1", "journal_reference": "Polibits (44) 2011, pp. 79-83", "doi": null}
{"id": "http://arxiv.org/abs/1401.6567v1", "guidislink": true, "updated": "2014-01-25T18:45:29Z", "updated_parsed": [2014, 1, 25, 18, 45, 29, 5, 25, 0], "published": "2014-01-25T18:45:29Z", "published_parsed": [2014, 1, 25, 18, 45, 29, 5, 25, 0], "title": "A Machine Learning Approach for the Identification of Bengali Noun-Noun\n  Compound Multiword Expressions", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=40&max_results=10&sortBy=relevance&sortOrder=descending", "value": "A Machine Learning Approach for the Identification of Bengali Noun-Noun\n  Compound Multiword Expressions"}, "summary": "This paper presents a machine learning approach for identification of Bengali\nmultiword expressions (MWE) which are bigram nominal compounds. Our proposed\napproach has two steps: (1) candidate extraction using chunk information and\nvarious heuristic rules and (2) training the machine learning algorithm called\nRandom Forest to classify the candidates into two groups: bigram nominal\ncompound MWE or not bigram nominal compound MWE. A variety of association\nmeasures, syntactic and linguistic clues and a set of WordNet-based similarity\nfeatures have been used for our MWE identification task. The approach presented\nin this paper can be used to identify bigram nominal compound MWE in Bengali\nrunning text.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=40&max_results=10&sortBy=relevance&sortOrder=descending", "value": "This paper presents a machine learning approach for identification of Bengali\nmultiword expressions (MWE) which are bigram nominal compounds. Our proposed\napproach has two steps: (1) candidate extraction using chunk information and\nvarious heuristic rules and (2) training the machine learning algorithm called\nRandom Forest to classify the candidates into two groups: bigram nominal\ncompound MWE or not bigram nominal compound MWE. A variety of association\nmeasures, syntactic and linguistic clues and a set of WordNet-based similarity\nfeatures have been used for our MWE identification task. The approach presented\nin this paper can be used to identify bigram nominal compound MWE in Bengali\nrunning text."}, "authors": ["Vivekananda Gayen", "Kamal Sarkar"], "author_detail": {"name": "Kamal Sarkar"}, "author": "Kamal Sarkar", "links": [{"href": "http://arxiv.org/abs/1401.6567v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1401.6567v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1401.6567v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1401.6567v1", "arxiv_comment": null, "journal_reference": "In Proceedings of ICON-2013: 10th International Conference on\n  Natural Language Processing, pp 290-296", "doi": null}
{"id": "http://arxiv.org/abs/1504.01182v1", "guidislink": true, "updated": "2015-04-06T01:18:24Z", "updated_parsed": [2015, 4, 6, 1, 18, 24, 0, 96, 0], "published": "2015-04-06T01:18:24Z", "published_parsed": [2015, 4, 6, 1, 18, 24, 0, 96, 0], "title": "Bengali to Assamese Statistical Machine Translation using Moses (Corpus\n  Based)", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=40&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Bengali to Assamese Statistical Machine Translation using Moses (Corpus\n  Based)"}, "summary": "Machine dialect interpretation assumes a real part in encouraging man-machine\ncorrespondence and in addition men-men correspondence in Natural Language\nProcessing (NLP). Machine Translation (MT) alludes to utilizing machine to\nchange one dialect to an alternate. Statistical Machine Translation is a type\nof MT consisting of Language Model (LM), Translation Model (TM) and decoder. In\nthis paper, Bengali to Assamese Statistical Machine Translation Model has been\ncreated by utilizing Moses. Other translation tools like IRSTLM for Language\nModel and GIZA-PP-V1.0.7 for Translation model are utilized within this\nframework which is accessible in Linux situations. The purpose of the LM is to\nencourage fluent output and the purpose of TM is to encourage similarity\nbetween input and output, the decoder increases the probability of translated\ntext in target language. A parallel corpus of 17100 sentences in Bengali and\nAssamese has been utilized for preparing within this framework. Measurable MT\nprocedures have not so far been generally investigated for Indian dialects. It\nmight be intriguing to discover to what degree these models can help the\nimmense continuous MT deliberations in the nation.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=40&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Machine dialect interpretation assumes a real part in encouraging man-machine\ncorrespondence and in addition men-men correspondence in Natural Language\nProcessing (NLP). Machine Translation (MT) alludes to utilizing machine to\nchange one dialect to an alternate. Statistical Machine Translation is a type\nof MT consisting of Language Model (LM), Translation Model (TM) and decoder. In\nthis paper, Bengali to Assamese Statistical Machine Translation Model has been\ncreated by utilizing Moses. Other translation tools like IRSTLM for Language\nModel and GIZA-PP-V1.0.7 for Translation model are utilized within this\nframework which is accessible in Linux situations. The purpose of the LM is to\nencourage fluent output and the purpose of TM is to encourage similarity\nbetween input and output, the decoder increases the probability of translated\ntext in target language. A parallel corpus of 17100 sentences in Bengali and\nAssamese has been utilized for preparing within this framework. Measurable MT\nprocedures have not so far been generally investigated for Indian dialects. It\nmight be intriguing to discover to what degree these models can help the\nimmense continuous MT deliberations in the nation."}, "authors": ["Nayan Jyoti Kalita", "Baharul Islam"], "author_detail": {"name": "Baharul Islam"}, "author": "Baharul Islam", "arxiv_comment": "6 pages, International Conference on Cognitive Computing and\n  Information Processing (CCIP-15), 3-4 March 2015, Noida (India)", "links": [{"href": "http://arxiv.org/abs/1504.01182v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1504.01182v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1504.01182v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1504.01182v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1802.05737v1", "guidislink": true, "updated": "2018-02-15T20:02:43Z", "updated_parsed": [2018, 2, 15, 20, 2, 43, 3, 46, 0], "published": "2018-02-15T20:02:43Z", "published_parsed": [2018, 2, 15, 20, 2, 43, 3, 46, 0], "title": "JU_KS@SAIL_CodeMixed-2017: Sentiment Analysis for Indian Code Mixed\n  Social Media Texts", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=40&max_results=10&sortBy=relevance&sortOrder=descending", "value": "JU_KS@SAIL_CodeMixed-2017: Sentiment Analysis for Indian Code Mixed\n  Social Media Texts"}, "summary": "This paper reports about our work in the NLP Tool Contest @ICON-2017, shared\ntask on Sentiment Analysis for Indian Languages (SAIL) (code mixed). To\nimplement our system, we have used a machine learning algo-rithm called\nMultinomial Na\\\"ive Bayes trained using n-gram and SentiWordnet features. We\nhave also used a small SentiWordnet for English and a small SentiWordnet for\nBengali. But we have not used any SentiWordnet for Hindi language. We have\ntested our system on Hindi-English and Bengali-English code mixed social media\ndata sets released for the contest. The performance of our system is very close\nto the best system participated in the contest. For both Bengali-English and\nHindi-English runs, our system was ranked at the 3rd position out of all\nsubmitted runs and awarded the 3rd prize in the contest.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=40&max_results=10&sortBy=relevance&sortOrder=descending", "value": "This paper reports about our work in the NLP Tool Contest @ICON-2017, shared\ntask on Sentiment Analysis for Indian Languages (SAIL) (code mixed). To\nimplement our system, we have used a machine learning algo-rithm called\nMultinomial Na\\\"ive Bayes trained using n-gram and SentiWordnet features. We\nhave also used a small SentiWordnet for English and a small SentiWordnet for\nBengali. But we have not used any SentiWordnet for Hindi language. We have\ntested our system on Hindi-English and Bengali-English code mixed social media\ndata sets released for the contest. The performance of our system is very close\nto the best system participated in the contest. For both Bengali-English and\nHindi-English runs, our system was ranked at the 3rd position out of all\nsubmitted runs and awarded the 3rd prize in the contest."}, "authors": ["Kamal Sarkar"], "author_detail": {"name": "Kamal Sarkar"}, "author": "Kamal Sarkar", "arxiv_comment": "NLP Tool Contest on Sentiment Analysis for Indian Languages (Code\n  Mixed) held in conjunction with the 14th International Conference on Natural\n  Language Processing, 2017", "links": [{"href": "http://arxiv.org/abs/1802.05737v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1802.05737v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "68T50", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1802.05737v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1802.05737v1", "journal_reference": "Kamal Sarkar, JU_KS@SAIL_CodeMixed-2017: Sentiment Analysis for\n  Indian Code Mixed Social Media Texts, NLP Tool Contest@the 14th International\n  Conference on Natural Language Processing, 2017", "doi": null}
{"id": "http://arxiv.org/abs/1808.09500v1", "guidislink": true, "updated": "2018-08-28T19:11:20Z", "updated_parsed": [2018, 8, 28, 19, 11, 20, 1, 240, 0], "published": "2018-08-28T19:11:20Z", "published_parsed": [2018, 8, 28, 19, 11, 20, 1, 240, 0], "title": "Adapting Word Embeddings to New Languages with Morphological and\n  Phonological Subword Representations", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=40&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Adapting Word Embeddings to New Languages with Morphological and\n  Phonological Subword Representations"}, "summary": "Much work in Natural Language Processing (NLP) has been for resource-rich\nlanguages, making generalization to new, less-resourced languages challenging.\nWe present two approaches for improving generalization to low-resourced\nlanguages by adapting continuous word representations using linguistically\nmotivated subword units: phonemes, morphemes and graphemes. Our method requires\nneither parallel corpora nor bilingual dictionaries and provides a significant\ngain in performance over previous methods relying on these resources. We\ndemonstrate the effectiveness of our approaches on Named Entity Recognition for\nfour languages, namely Uyghur, Turkish, Bengali and Hindi, of which Uyghur and\nBengali are low resource languages, and also perform experiments on Machine\nTranslation. Exploiting subwords with transfer learning gives us a boost of\n+15.2 NER F1 for Uyghur and +9.7 F1 for Bengali. We also show improvements in\nthe monolingual setting where we achieve (avg.) +3 F1 and (avg.) +1.35 BLEU.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=40&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Much work in Natural Language Processing (NLP) has been for resource-rich\nlanguages, making generalization to new, less-resourced languages challenging.\nWe present two approaches for improving generalization to low-resourced\nlanguages by adapting continuous word representations using linguistically\nmotivated subword units: phonemes, morphemes and graphemes. Our method requires\nneither parallel corpora nor bilingual dictionaries and provides a significant\ngain in performance over previous methods relying on these resources. We\ndemonstrate the effectiveness of our approaches on Named Entity Recognition for\nfour languages, namely Uyghur, Turkish, Bengali and Hindi, of which Uyghur and\nBengali are low resource languages, and also perform experiments on Machine\nTranslation. Exploiting subwords with transfer learning gives us a boost of\n+15.2 NER F1 for Uyghur and +9.7 F1 for Bengali. We also show improvements in\nthe monolingual setting where we achieve (avg.) +3 F1 and (avg.) +1.35 BLEU."}, "authors": ["Aditi Chaudhary", "Chunting Zhou", "Lori Levin", "Graham Neubig", "David R. Mortensen", "Jaime G. Carbonell"], "author_detail": {"name": "Jaime G. Carbonell"}, "author": "Jaime G. Carbonell", "arxiv_comment": "Accepted at EMNLP 2018", "links": [{"href": "http://arxiv.org/abs/1808.09500v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1808.09500v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1808.09500v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1808.09500v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1902.11133v1", "guidislink": true, "updated": "2019-02-25T13:52:53Z", "updated_parsed": [2019, 2, 25, 13, 52, 53, 0, 56, 0], "published": "2019-02-25T13:52:53Z", "published_parsed": [2019, 2, 25, 13, 52, 53, 0, 56, 0], "title": "Bengali Handwritten Character Classification using Transfer Learning on\n  Deep Convolutional Neural Network", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=40&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Bengali Handwritten Character Classification using Transfer Learning on\n  Deep Convolutional Neural Network"}, "summary": "In this paper, we propose a solution which uses state-of-the-art techniques\nin Deep Learning to tackle the problem of Bengali Handwritten Character\nRecognition ( HCR ). Our method uses lesser iterations to train than most other\ncomparable methods. We employ Transfer Learning on ResNet 50, a\nstate-of-the-art deep Convolutional Neural Network Model, pretrained on\nImageNet dataset. We also use other techniques like a modified version of One\nCycle Policy, varying the input image sizes etc. to ensure that our training\noccurs fast. We use the BanglaLekha-Isolated Dataset for evaluation of our\ntechnique which consists of 84 classes (50 Basic, 10 Numerals and 24 Compound\nCharacters). We are able to achieve 96.12% accuracy in just 47 epochs on\nBanglaLekha-Isolated dataset. When comparing our method with that of other\nresearchers, considering number of classes and without using Ensemble Learning,\nthe proposed solution achieves state of the art result for Handwritten Bengali\nCharacter Recognition. Code and weight files are available at\nhttps://github.com/swagato-c/bangla-hwcr-present.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=40&max_results=10&sortBy=relevance&sortOrder=descending", "value": "In this paper, we propose a solution which uses state-of-the-art techniques\nin Deep Learning to tackle the problem of Bengali Handwritten Character\nRecognition ( HCR ). Our method uses lesser iterations to train than most other\ncomparable methods. We employ Transfer Learning on ResNet 50, a\nstate-of-the-art deep Convolutional Neural Network Model, pretrained on\nImageNet dataset. We also use other techniques like a modified version of One\nCycle Policy, varying the input image sizes etc. to ensure that our training\noccurs fast. We use the BanglaLekha-Isolated Dataset for evaluation of our\ntechnique which consists of 84 classes (50 Basic, 10 Numerals and 24 Compound\nCharacters). We are able to achieve 96.12% accuracy in just 47 epochs on\nBanglaLekha-Isolated dataset. When comparing our method with that of other\nresearchers, considering number of classes and without using Ensemble Learning,\nthe proposed solution achieves state of the art result for Handwritten Bengali\nCharacter Recognition. Code and weight files are available at\nhttps://github.com/swagato-c/bangla-hwcr-present."}, "authors": ["Swagato Chatterjee", "Rwik Kumar Dutta", "Debayan Ganguly", "Kingshuk Chatterjee", "Sudipta Roy"], "author_detail": {"name": "Sudipta Roy"}, "author": "Sudipta Roy", "links": [{"href": "http://arxiv.org/abs/1902.11133v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1902.11133v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1902.11133v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1902.11133v1", "arxiv_comment": null, "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/2007.02758v1", "guidislink": true, "updated": "2020-07-06T13:58:51Z", "updated_parsed": [2020, 7, 6, 13, 58, 51, 0, 188, 0], "published": "2020-07-06T13:58:51Z", "published_parsed": [2020, 7, 6, 13, 58, 51, 0, 188, 0], "title": "Sentiment Polarity Detection on Bengali Book Reviews Using Multinomial\n  Naive Bayes", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=50&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Sentiment Polarity Detection on Bengali Book Reviews Using Multinomial\n  Naive Bayes"}, "summary": "Recently, sentiment polarity detection has increased attention to NLP\nresearchers due to the massive availability of customer's opinions or reviews\nin the online platform. Due to the continued expansion of e-commerce sites, the\nrate of purchase of various products, including books, are growing enormously\namong the people. Reader's opinions/reviews affect the buying decision of a\ncustomer in most cases. This work introduces a machine learning-based technique\nto determine sentiment polarities (either positive or negative category) from\nBengali book reviews. To assess the effectiveness of the proposed technique, a\ncorpus with 2000 reviews on Bengali books is developed. A comparative analysis\nwith various approaches (such as logistic regression, naive Bayes, SVM, and\nSGD) also performed by taking into consideration of the unigram, bigram, and\ntrigram features, respectively. Experimental result reveals that the\nmultinomial Naive Bayes with unigram feature outperforms the other techniques\nwith 84% accuracy on the test set.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=50&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Recently, sentiment polarity detection has increased attention to NLP\nresearchers due to the massive availability of customer's opinions or reviews\nin the online platform. Due to the continued expansion of e-commerce sites, the\nrate of purchase of various products, including books, are growing enormously\namong the people. Reader's opinions/reviews affect the buying decision of a\ncustomer in most cases. This work introduces a machine learning-based technique\nto determine sentiment polarities (either positive or negative category) from\nBengali book reviews. To assess the effectiveness of the proposed technique, a\ncorpus with 2000 reviews on Bengali books is developed. A comparative analysis\nwith various approaches (such as logistic regression, naive Bayes, SVM, and\nSGD) also performed by taking into consideration of the unigram, bigram, and\ntrigram features, respectively. Experimental result reveals that the\nmultinomial Naive Bayes with unigram feature outperforms the other techniques\nwith 84% accuracy on the test set."}, "authors": ["Eftekhar Hossain", "Omar Sharif", "Mohammed Moshiul Hoque"], "author_detail": {"name": "Mohammed Moshiul Hoque"}, "author": "Mohammed Moshiul Hoque", "arxiv_comment": "12 pages, ICACIE 2020, Will be published by Advances in Intelligent\n  Systems and Computing (AISC) series of Springer", "links": [{"href": "http://arxiv.org/abs/2007.02758v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2007.02758v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2007.02758v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2007.02758v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/2008.12995v3", "guidislink": true, "updated": "2020-09-20T22:48:57Z", "updated_parsed": [2020, 9, 20, 22, 48, 57, 6, 264, 0], "published": "2020-08-29T15:22:00Z", "published_parsed": [2020, 8, 29, 15, 22, 0, 5, 242, 0], "title": "AKHCRNet: Bengali Handwritten Character Recognition Using Deep Learning", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=50&max_results=10&sortBy=relevance&sortOrder=descending", "value": "AKHCRNet: Bengali Handwritten Character Recognition Using Deep Learning"}, "summary": "I propose a state of the art deep neural architectural solution for\nhandwritten character recognition for Bengali alphabets, compound characters as\nwell as numerical digits that achieves state-of-the-art accuracy 96.8% in just\n11 epochs. Similar work has been done before by Chatterjee, Swagato, et al. but\nthey achieved 96.12% accuracy in about 47 epochs. The deep neural architecture\nused in that paper was fairly large considering the inclusion of the weights of\nthe ResNet 50 model which is a 50 layer Residual Network. This proposed model\nachieves higher accuracy as compared to any previous work & in a little number\nof epochs. ResNet50 is a good model trained on the ImageNet dataset, but I\npropose an HCR network that is trained from the scratch on Bengali characters\nwithout the \"Ensemble Learning\" that can outperform previous architectures.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=50&max_results=10&sortBy=relevance&sortOrder=descending", "value": "I propose a state of the art deep neural architectural solution for\nhandwritten character recognition for Bengali alphabets, compound characters as\nwell as numerical digits that achieves state-of-the-art accuracy 96.8% in just\n11 epochs. Similar work has been done before by Chatterjee, Swagato, et al. but\nthey achieved 96.12% accuracy in about 47 epochs. The deep neural architecture\nused in that paper was fairly large considering the inclusion of the weights of\nthe ResNet 50 model which is a 50 layer Residual Network. This proposed model\nachieves higher accuracy as compared to any previous work & in a little number\nof epochs. ResNet50 is a good model trained on the ImageNet dataset, but I\npropose an HCR network that is trained from the scratch on Bengali characters\nwithout the \"Ensemble Learning\" that can outperform previous architectures."}, "authors": ["Akash Roy"], "author_detail": {"name": "Akash Roy"}, "author": "Akash Roy", "arxiv_comment": "language ambiguity cleared, typos corrected", "links": [{"href": "http://arxiv.org/abs/2008.12995v3", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2008.12995v3", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2008.12995v3", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2008.12995v3", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/2010.06906v1", "guidislink": true, "updated": "2020-10-14T09:37:51Z", "updated_parsed": [2020, 10, 14, 9, 37, 51, 2, 288, 0], "published": "2020-10-14T09:37:51Z", "published_parsed": [2020, 10, 14, 9, 37, 51, 2, 288, 0], "title": "No Rumours Please! A Multi-Indic-Lingual Approach for COVID Fake-Tweet\n  Detection", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=50&max_results=10&sortBy=relevance&sortOrder=descending", "value": "No Rumours Please! A Multi-Indic-Lingual Approach for COVID Fake-Tweet\n  Detection"}, "summary": "The sudden widespread menace created by the present global pandemic COVID-19\nhas had an unprecedented effect on our lives. Man-kind is going through\nhumongous fear and dependence on social media like never before. Fear\ninevitably leads to panic, speculations, and the spread of misinformation. Many\ngovernments have taken measures to curb the spread of such misinformation for\npublic well being. Besides global measures, to have effective outreach, systems\nfor demographically local languages have an important role to play in this\neffort. Towards this, we propose an approach to detect fake news about COVID-19\nearly on from social media, such as tweets, for multiple Indic-Languages\nbesides English. In addition, we also create an annotated dataset of Hindi and\nBengali tweet for fake news detection. We propose a BERT based model augmented\nwith additional relevant features extracted from Twitter to identify fake\ntweets. To expand our approach to multiple Indic languages, we resort to mBERT\nbased model which is fine-tuned over created dataset in Hindi and Bengali. We\nalso propose a zero-shot learning approach to alleviate the data scarcity issue\nfor such low resource languages. Through rigorous experiments, we show that our\napproach reaches around 89% F-Score in fake tweet detection which supercedes\nthe state-of-the-art (SOTA) results. Moreover, we establish the first benchmark\nfor two Indic-Languages, Hindi and Bengali. Using our annotated data, our model\nachieves about 79% F-Score in Hindi and 81% F-Score for Bengali Tweets. Our\nzero-shot model achieves about 81% F-Score in Hindi and 78% F-Score for Bengali\nTweets without any annotated data, which clearly indicates the efficacy of our\napproach.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=50&max_results=10&sortBy=relevance&sortOrder=descending", "value": "The sudden widespread menace created by the present global pandemic COVID-19\nhas had an unprecedented effect on our lives. Man-kind is going through\nhumongous fear and dependence on social media like never before. Fear\ninevitably leads to panic, speculations, and the spread of misinformation. Many\ngovernments have taken measures to curb the spread of such misinformation for\npublic well being. Besides global measures, to have effective outreach, systems\nfor demographically local languages have an important role to play in this\neffort. Towards this, we propose an approach to detect fake news about COVID-19\nearly on from social media, such as tweets, for multiple Indic-Languages\nbesides English. In addition, we also create an annotated dataset of Hindi and\nBengali tweet for fake news detection. We propose a BERT based model augmented\nwith additional relevant features extracted from Twitter to identify fake\ntweets. To expand our approach to multiple Indic languages, we resort to mBERT\nbased model which is fine-tuned over created dataset in Hindi and Bengali. We\nalso propose a zero-shot learning approach to alleviate the data scarcity issue\nfor such low resource languages. Through rigorous experiments, we show that our\napproach reaches around 89% F-Score in fake tweet detection which supercedes\nthe state-of-the-art (SOTA) results. Moreover, we establish the first benchmark\nfor two Indic-Languages, Hindi and Bengali. Using our annotated data, our model\nachieves about 79% F-Score in Hindi and 81% F-Score for Bengali Tweets. Our\nzero-shot model achieves about 81% F-Score in Hindi and 78% F-Score for Bengali\nTweets without any annotated data, which clearly indicates the efficacy of our\napproach."}, "authors": ["Debanjana Kar", "Mohit Bhardwaj", "Suranjana Samanta", "Amar Prakash Azad"], "author_detail": {"name": "Amar Prakash Azad"}, "author": "Amar Prakash Azad", "arxiv_comment": "6 pages, 4 figures", "links": [{"href": "http://arxiv.org/abs/2010.06906v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2010.06906v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.SI", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2010.06906v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2010.06906v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1910.10758v1", "guidislink": true, "updated": "2019-10-23T18:24:55Z", "updated_parsed": [2019, 10, 23, 18, 24, 55, 2, 296, 0], "published": "2019-10-23T18:24:55Z", "published_parsed": [2019, 10, 23, 18, 24, 55, 2, 296, 0], "title": "A Novel Approach for Automatic Bengali Question Answering System using\n  Semantic Similarity Analysis", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=50&max_results=10&sortBy=relevance&sortOrder=descending", "value": "A Novel Approach for Automatic Bengali Question Answering System using\n  Semantic Similarity Analysis"}, "summary": "Finding the semantically accurate answer is one of the key challenges in\nadvanced searching. In contrast to keyword-based searching, the meaning of a\nquestion or query is important here and answers are ranked according to\nrelevance. It is very natural that there is almost no common word between the\nquestion sentence and the answer sentence. In this paper, an approach is\ndescribed to find out the semantically relevant answers in the Bengali dataset.\nIn the first part of the algorithm, a set of statistical parameters like\nfrequency, index, part-of-speech (POS), etc. is matched between a question and\nthe probable answers. In the second phase, entropy and similarity are\ncalculated in different modules. Finally, a sense score is generated to rank\nthe answers. The algorithm is tested on a repository containing a total of\n275000 sentences. This Bengali repository is a product of Technology\nDevelopment for Indian Languages (TDIL) project sponsored by Govt. of India and\nprovided by the Language Research Unit of Indian Statistical Institute,\nKolkata. The shallow parser, developed by the LTRC group of IIIT Hyderabad is\nused for POS tagging. The actual answer is ranked as 1st in 82.3% cases. The\nactual answer is ranked within 1st to 5th in 90.0% cases. The accuracy of the\nsystem is coming as 97.32% and precision of the system is coming as 98.14%\nusing confusion matrix. The challenges and pitfalls of the work are reported at\nlast in this paper.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=50&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Finding the semantically accurate answer is one of the key challenges in\nadvanced searching. In contrast to keyword-based searching, the meaning of a\nquestion or query is important here and answers are ranked according to\nrelevance. It is very natural that there is almost no common word between the\nquestion sentence and the answer sentence. In this paper, an approach is\ndescribed to find out the semantically relevant answers in the Bengali dataset.\nIn the first part of the algorithm, a set of statistical parameters like\nfrequency, index, part-of-speech (POS), etc. is matched between a question and\nthe probable answers. In the second phase, entropy and similarity are\ncalculated in different modules. Finally, a sense score is generated to rank\nthe answers. The algorithm is tested on a repository containing a total of\n275000 sentences. This Bengali repository is a product of Technology\nDevelopment for Indian Languages (TDIL) project sponsored by Govt. of India and\nprovided by the Language Research Unit of Indian Statistical Institute,\nKolkata. The shallow parser, developed by the LTRC group of IIIT Hyderabad is\nused for POS tagging. The actual answer is ranked as 1st in 82.3% cases. The\nactual answer is ranked within 1st to 5th in 90.0% cases. The accuracy of the\nsystem is coming as 97.32% and precision of the system is coming as 98.14%\nusing confusion matrix. The challenges and pitfalls of the work are reported at\nlast in this paper."}, "authors": ["Arijit Das", "Jaydeep Mandal", "Zargham Danial", "Alok Ranjan Pal", "Diganta Saha"], "author_detail": {"name": "Diganta Saha"}, "author": "Diganta Saha", "arxiv_comment": "14 pages, 5 figures", "links": [{"href": "http://arxiv.org/abs/1910.10758v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1910.10758v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.IR", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.IR", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.CY", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.HC", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1910.10758v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1910.10758v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1912.00127v3", "guidislink": true, "updated": "2020-03-03T03:53:55Z", "updated_parsed": [2020, 3, 3, 3, 53, 55, 1, 63, 0], "published": "2019-11-30T04:00:31Z", "published_parsed": [2019, 11, 30, 4, 0, 31, 5, 334, 0], "title": "A Hybrid Approach Towards Two Stage Bengali Question Classification\n  Utilizing Smart Data Balancing Technique", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=50&max_results=10&sortBy=relevance&sortOrder=descending", "value": "A Hybrid Approach Towards Two Stage Bengali Question Classification\n  Utilizing Smart Data Balancing Technique"}, "summary": "Question classification (QC) is the primary step of the Question Answering\n(QA) system. Question Classification (QC) system classifies the questions in\nparticular classes so that Question Answering (QA) System can provide correct\nanswers for the questions. Our system categorizes the factoid type questions\nasked in natural language after extracting features of the questions. We\npresent a two stage QC system for Bengali. It utilizes one dimensional\nconvolutional neural network for classifying questions into coarse classes in\nthe first stage. Word2vec representation of existing words of the question\ncorpus have been constructed and used for assisting 1D CNN. A smart data\nbalancing technique has been employed for giving data hungry convolutional\nneural network the advantage of a greater number of effective samples to learn\nfrom. For each coarse class, a separate Stochastic Gradient Descent (SGD) based\nclassifier has been used in order to differentiate among the finer classes\nwithin that coarse class. TF-IDF representation of each word has been used as\nfeature for the SGD classifiers implemented as part of second stage\nclassification. Experiments show the effectiveness of our proposed method for\nBengali question classification.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=50&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Question classification (QC) is the primary step of the Question Answering\n(QA) system. Question Classification (QC) system classifies the questions in\nparticular classes so that Question Answering (QA) System can provide correct\nanswers for the questions. Our system categorizes the factoid type questions\nasked in natural language after extracting features of the questions. We\npresent a two stage QC system for Bengali. It utilizes one dimensional\nconvolutional neural network for classifying questions into coarse classes in\nthe first stage. Word2vec representation of existing words of the question\ncorpus have been constructed and used for assisting 1D CNN. A smart data\nbalancing technique has been employed for giving data hungry convolutional\nneural network the advantage of a greater number of effective samples to learn\nfrom. For each coarse class, a separate Stochastic Gradient Descent (SGD) based\nclassifier has been used in order to differentiate among the finer classes\nwithin that coarse class. TF-IDF representation of each word has been used as\nfeature for the SGD classifiers implemented as part of second stage\nclassification. Experiments show the effectiveness of our proposed method for\nBengali question classification."}, "authors": ["Md. Hasibur Rahman", "Chowdhury Rafeed Rahman", "Ruhul Amin", "Md. Habibur Rahman Sifat", "Afra Anika"], "author_detail": {"name": "Afra Anika"}, "author": "Afra Anika", "links": [{"title": "doi", "href": "http://dx.doi.org/10.1007/978-3-030-52856-0_36", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/1912.00127v3", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1912.00127v3", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.IR", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.ML", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1912.00127v3", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1912.00127v3", "arxiv_comment": null, "journal_reference": null, "doi": "10.1007/978-3-030-52856-0_36"}
{"id": "http://arxiv.org/abs/2008.13597v2", "guidislink": true, "updated": "2020-09-06T14:47:12Z", "updated_parsed": [2020, 9, 6, 14, 47, 12, 6, 250, 0], "published": "2020-08-31T13:39:04Z", "published_parsed": [2020, 8, 31, 13, 39, 4, 0, 244, 0], "title": "Classifier Combination Approach for Question Classification for Bengali\n  Question Answering System", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=50&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Classifier Combination Approach for Question Classification for Bengali\n  Question Answering System"}, "summary": "Question classification (QC) is a prime constituent of automated question\nanswering system. The work presented here demonstrates that the combination of\nmultiple models achieve better classification performance than those obtained\nwith existing individual models for the question classification task in\nBengali. We have exploited state-of-the-art multiple model combination\ntechniques, i.e., ensemble, stacking and voting, to increase QC accuracy.\nLexical, syntactic and semantic features of Bengali questions are used for four\nwell-known classifiers, namely Na\\\"{\\i}ve Bayes, kernel Na\\\"{\\i}ve Bayes, Rule\nInduction, and Decision Tree, which serve as our base learners. Single-layer\nquestion-class taxonomy with 8 coarse-grained classes is extended to two-layer\ntaxonomy by adding 69 fine-grained classes. We carried out the experiments both\non single-layer and two-layer taxonomies. Experimental results confirmed that\nclassifier combination approaches outperform single classifier classification\napproaches by 4.02% for coarse-grained question classes. Overall, the stacking\napproach produces the best results for fine-grained classification and achieves\n87.79% of accuracy. The approach presented here could be used in other\nIndo-Aryan or Indic languages to develop a question answering system.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=50&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Question classification (QC) is a prime constituent of automated question\nanswering system. The work presented here demonstrates that the combination of\nmultiple models achieve better classification performance than those obtained\nwith existing individual models for the question classification task in\nBengali. We have exploited state-of-the-art multiple model combination\ntechniques, i.e., ensemble, stacking and voting, to increase QC accuracy.\nLexical, syntactic and semantic features of Bengali questions are used for four\nwell-known classifiers, namely Na\\\"{\\i}ve Bayes, kernel Na\\\"{\\i}ve Bayes, Rule\nInduction, and Decision Tree, which serve as our base learners. Single-layer\nquestion-class taxonomy with 8 coarse-grained classes is extended to two-layer\ntaxonomy by adding 69 fine-grained classes. We carried out the experiments both\non single-layer and two-layer taxonomies. Experimental results confirmed that\nclassifier combination approaches outperform single classifier classification\napproaches by 4.02% for coarse-grained question classes. Overall, the stacking\napproach produces the best results for fine-grained classification and achieves\n87.79% of accuracy. The approach presented here could be used in other\nIndo-Aryan or Indic languages to develop a question answering system."}, "authors": ["Somnath Banerjee", "Sudip Kumar Naskar", "Paolo Rosso", "Sivaji Bandyopadhyay"], "author_detail": {"name": "Sivaji Bandyopadhyay"}, "author": "Sivaji Bandyopadhyay", "links": [{"title": "doi", "href": "http://dx.doi.org/10.1007/s12046-019-1224-8", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/2008.13597v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2008.13597v2", "rel": "related", "type": "application/pdf"}], "arxiv_comment": "16 pages, to be published in Sadhana", "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2008.13597v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2008.13597v2", "journal_reference": "Sadhana, Springer, 2019", "doi": "10.1007/s12046-019-1224-8"}
{"id": "http://arxiv.org/abs/2012.01747v1", "guidislink": true, "updated": "2020-12-03T08:17:31Z", "updated_parsed": [2020, 12, 3, 8, 17, 31, 3, 338, 0], "published": "2020-12-03T08:17:31Z", "published_parsed": [2020, 12, 3, 8, 17, 31, 3, 338, 0], "title": "Bengali Abstractive News Summarization(BANS): A Neural Attention\n  Approach", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=50&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Bengali Abstractive News Summarization(BANS): A Neural Attention\n  Approach"}, "summary": "Abstractive summarization is the process of generating novel sentences based\non the information extracted from the original text document while retaining\nthe context. Due to abstractive summarization's underlying complexities, most\nof the past research work has been done on the extractive summarization\napproach. Nevertheless, with the triumph of the sequence-to-sequence (seq2seq)\nmodel, abstractive summarization becomes more viable. Although a significant\nnumber of notable research has been done in the English language based on\nabstractive summarization, only a couple of works have been done on Bengali\nabstractive news summarization (BANS). In this article, we presented a seq2seq\nbased Long Short-Term Memory (LSTM) network model with attention at\nencoder-decoder. Our proposed system deploys a local attention-based model that\nproduces a long sequence of words with lucid and human-like generated sentences\nwith noteworthy information of the original document. We also prepared a\ndataset of more than 19k articles and corresponding human-written summaries\ncollected from bangla.bdnews24.com1 which is till now the most extensive\ndataset for Bengali news document summarization and publicly published in\nKaggle2. We evaluated our model qualitatively and quantitatively and compared\nit with other published results. It showed significant improvement in terms of\nhuman evaluation scores with state-of-the-art approaches for BANS.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=50&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Abstractive summarization is the process of generating novel sentences based\non the information extracted from the original text document while retaining\nthe context. Due to abstractive summarization's underlying complexities, most\nof the past research work has been done on the extractive summarization\napproach. Nevertheless, with the triumph of the sequence-to-sequence (seq2seq)\nmodel, abstractive summarization becomes more viable. Although a significant\nnumber of notable research has been done in the English language based on\nabstractive summarization, only a couple of works have been done on Bengali\nabstractive news summarization (BANS). In this article, we presented a seq2seq\nbased Long Short-Term Memory (LSTM) network model with attention at\nencoder-decoder. Our proposed system deploys a local attention-based model that\nproduces a long sequence of words with lucid and human-like generated sentences\nwith noteworthy information of the original document. We also prepared a\ndataset of more than 19k articles and corresponding human-written summaries\ncollected from bangla.bdnews24.com1 which is till now the most extensive\ndataset for Bengali news document summarization and publicly published in\nKaggle2. We evaluated our model qualitatively and quantitatively and compared\nit with other published results. It showed significant improvement in terms of\nhuman evaluation scores with state-of-the-art approaches for BANS."}, "authors": ["Prithwiraj Bhattacharjee", "Avi Mallick", "Md Saiful Islam", "Marium-E-Jannat"], "author_detail": {"name": "Marium-E-Jannat"}, "author": "Marium-E-Jannat", "links": [{"title": "doi", "href": "http://dx.doi.org/10.1007/978-981-33-4673-4_4", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/2012.01747v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2012.01747v1", "rel": "related", "type": "application/pdf"}], "arxiv_comment": "10 Pages, 2 figures, 4 tables, 2nd International Conference on Trends\n  in Computational and Cognitive Engineering(TCCE-2020)", "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2012.01747v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2012.01747v1", "journal_reference": "2nd International Conference on Trends in Computational and\n  Cognitive Engineering, 2020", "doi": "10.1007/978-981-33-4673-4_4"}
{"id": "http://arxiv.org/abs/1107.2723v1", "guidislink": true, "updated": "2011-07-14T04:05:23Z", "updated_parsed": [2011, 7, 14, 4, 5, 23, 3, 195, 0], "published": "2011-07-14T04:05:23Z", "published_parsed": [2011, 7, 14, 4, 5, 23, 3, 195, 0], "title": "Topographic Feature Extraction for Bengali and Hindi Character Images", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=50&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Topographic Feature Extraction for Bengali and Hindi Character Images"}, "summary": "Feature selection and extraction plays an important role in different\nclassification based problems such as face recognition, signature verification,\noptical character recognition (OCR) etc. The performance of OCR highly depends\non the proper selection and extraction of feature set. In this paper, we\npresent novel features based on the topography of a character as visible from\ndifferent viewing directions on a 2D plane. By topography of a character we\nmean the structural features of the strokes and their spatial relations. In\nthis work we develop topographic features of strokes visible with respect to\nviews from different directions (e.g. North, South, East, and West). We\nconsider three types of topographic features: closed region, convexity of\nstrokes, and straight line strokes. These features are represented as a\nshape-based graph which acts as an invariant feature set for discriminating\nvery similar type characters efficiently. We have tested the proposed method on\nprinted and handwritten Bengali and Hindi character images. Initial results\ndemonstrate the efficacy of our approach.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=50&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Feature selection and extraction plays an important role in different\nclassification based problems such as face recognition, signature verification,\noptical character recognition (OCR) etc. The performance of OCR highly depends\non the proper selection and extraction of feature set. In this paper, we\npresent novel features based on the topography of a character as visible from\ndifferent viewing directions on a 2D plane. By topography of a character we\nmean the structural features of the strokes and their spatial relations. In\nthis work we develop topographic features of strokes visible with respect to\nviews from different directions (e.g. North, South, East, and West). We\nconsider three types of topographic features: closed region, convexity of\nstrokes, and straight line strokes. These features are represented as a\nshape-based graph which acts as an invariant feature set for discriminating\nvery similar type characters efficiently. We have tested the proposed method on\nprinted and handwritten Bengali and Hindi character images. Initial results\ndemonstrate the efficacy of our approach."}, "authors": ["Soumen Bag", "Gaurav Harit"], "author_detail": {"name": "Gaurav Harit"}, "author": "Gaurav Harit", "links": [{"title": "doi", "href": "http://dx.doi.org/10.5121/sipij.2011.2215", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/1107.2723v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1107.2723v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1107.2723v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1107.2723v1", "arxiv_comment": null, "journal_reference": "Signal & Image Processing : An International Journal (SIPIJ),\n  vol.2, no.2, pp. 181-196, June 2011", "doi": "10.5121/sipij.2011.2215"}
{"id": "http://arxiv.org/abs/1701.08694v1", "guidislink": true, "updated": "2017-01-27T13:08:08Z", "updated_parsed": [2017, 1, 27, 13, 8, 8, 4, 27, 0], "published": "2017-01-27T13:08:08Z", "published_parsed": [2017, 1, 27, 13, 8, 8, 4, 27, 0], "title": "A Comparative Study on Different Types of Approaches to Bengali document\n  Categorization", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=60&max_results=10&sortBy=relevance&sortOrder=descending", "value": "A Comparative Study on Different Types of Approaches to Bengali document\n  Categorization"}, "summary": "Document categorization is a technique where the category of a document is\ndetermined. In this paper three well-known supervised learning techniques which\nare Support Vector Machine(SVM), Na\\\"ive Bayes(NB) and Stochastic Gradient\nDescent(SGD) compared for Bengali document categorization. Besides classifier,\nclassification also depends on how feature is selected from dataset. For\nanalyzing those classifier performances on predicting a document against twelve\ncategories several feature selection techniques are also applied in this\narticle namely Chi square distribution, normalized TFIDF (term\nfrequency-inverse document frequency) with word analyzer. So, we attempt to\nexplore the efficiency of those three-classification algorithms by using two\ndifferent feature selection techniques in this article.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=60&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Document categorization is a technique where the category of a document is\ndetermined. In this paper three well-known supervised learning techniques which\nare Support Vector Machine(SVM), Na\\\"ive Bayes(NB) and Stochastic Gradient\nDescent(SGD) compared for Bengali document categorization. Besides classifier,\nclassification also depends on how feature is selected from dataset. For\nanalyzing those classifier performances on predicting a document against twelve\ncategories several feature selection techniques are also applied in this\narticle namely Chi square distribution, normalized TFIDF (term\nfrequency-inverse document frequency) with word analyzer. So, we attempt to\nexplore the efficiency of those three-classification algorithms by using two\ndifferent feature selection techniques in this article."}, "authors": ["Md. Saiful Islam", "Fazla Elahi Md Jubayer", "Syed Ikhtiar Ahmed"], "author_detail": {"name": "Syed Ikhtiar Ahmed"}, "author": "Syed Ikhtiar Ahmed", "arxiv_comment": "6 pages", "links": [{"href": "http://arxiv.org/abs/1701.08694v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1701.08694v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1701.08694v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1701.08694v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1712.03579v1", "guidislink": true, "updated": "2017-12-10T19:52:51Z", "updated_parsed": [2017, 12, 10, 19, 52, 51, 6, 344, 0], "published": "2017-12-10T19:52:51Z", "published_parsed": [2017, 12, 10, 19, 52, 51, 6, 344, 0], "title": "Prodorshok I: A Bengali Isolated Speech Dataset for Voice-Based\n  Assistive Technologies - A comparative analysis of the effects of data\n  augmentation on HMM-GMM and DNN classifiers", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=60&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Prodorshok I: A Bengali Isolated Speech Dataset for Voice-Based\n  Assistive Technologies - A comparative analysis of the effects of data\n  augmentation on HMM-GMM and DNN classifiers"}, "summary": "Prodorshok I is a Bengali isolated word dataset tailored to help create\nspeaker-independent, voice-command driven automated speech recognition (ASR)\nbased assistive technologies to help improve human-computer interaction (HCI).\nThis paper presents the results of an objective analysis that was undertaken\nusing a subset of words from Prodorshok I to assess its reliability in ASR\nsystems that utilize Hidden Markov Models (HMM) with Gaussian emissions and\nDeep Neural Networks (DNN). The results show that simple data augmentation\ninvolving a small pitch shift can make surprisingly tangible improvements to\naccuracy levels in speech recognition.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=60&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Prodorshok I is a Bengali isolated word dataset tailored to help create\nspeaker-independent, voice-command driven automated speech recognition (ASR)\nbased assistive technologies to help improve human-computer interaction (HCI).\nThis paper presents the results of an objective analysis that was undertaken\nusing a subset of words from Prodorshok I to assess its reliability in ASR\nsystems that utilize Hidden Markov Models (HMM) with Gaussian emissions and\nDeep Neural Networks (DNN). The results show that simple data augmentation\ninvolving a small pitch shift can make surprisingly tangible improvements to\naccuracy levels in speech recognition."}, "authors": ["Mohi Reza", "Warida Rashid", "Moin Mostakim"], "author_detail": {"name": "Moin Mostakim"}, "author": "Moin Mostakim", "arxiv_comment": "4 pages, accepted for oral presentation at the 5th IEEE R10 HTC 2017", "links": [{"href": "http://arxiv.org/abs/1712.03579v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1712.03579v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.SD", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.SD", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.HC", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "eess.AS", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1712.03579v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1712.03579v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1803.04000v1", "guidislink": true, "updated": "2018-03-11T18:13:01Z", "updated_parsed": [2018, 3, 11, 18, 13, 1, 6, 70, 0], "published": "2018-03-11T18:13:01Z", "published_parsed": [2018, 3, 11, 18, 13, 1, 6, 70, 0], "title": "Preparing Bengali-English Code-Mixed Corpus for Sentiment Analysis of\n  Indian Languages", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=60&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Preparing Bengali-English Code-Mixed Corpus for Sentiment Analysis of\n  Indian Languages"}, "summary": "Analysis of informative contents and sentiments of social users has been\nattempted quite intensively in the recent past. Most of the systems are usable\nonly for monolingual data and fails or gives poor results when used on data\nwith code-mixing property. To gather attention and encourage researchers to\nwork on this crisis, we prepared gold standard Bengali-English code-mixed data\nwith language and polarity tag for sentiment analysis purposes. In this paper,\nwe discuss the systems we prepared to collect and filter raw Twitter data. In\norder to reduce manual work while annotation, hybrid systems combining rule\nbased and supervised models were developed for both language and sentiment\ntagging. The final corpus was annotated by a group of annotators following a\nfew guidelines. The gold standard corpus thus obtained has impressive\ninter-annotator agreement obtained in terms of Kappa values. Various metrics\nlike Code-Mixed Index (CMI), Code-Mixed Factor (CF) along with various aspects\n(language and emotion) also qualitatively polled the code-mixed and sentiment\nproperties of the corpus.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=60&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Analysis of informative contents and sentiments of social users has been\nattempted quite intensively in the recent past. Most of the systems are usable\nonly for monolingual data and fails or gives poor results when used on data\nwith code-mixing property. To gather attention and encourage researchers to\nwork on this crisis, we prepared gold standard Bengali-English code-mixed data\nwith language and polarity tag for sentiment analysis purposes. In this paper,\nwe discuss the systems we prepared to collect and filter raw Twitter data. In\norder to reduce manual work while annotation, hybrid systems combining rule\nbased and supervised models were developed for both language and sentiment\ntagging. The final corpus was annotated by a group of annotators following a\nfew guidelines. The gold standard corpus thus obtained has impressive\ninter-annotator agreement obtained in terms of Kappa values. Various metrics\nlike Code-Mixed Index (CMI), Code-Mixed Factor (CF) along with various aspects\n(language and emotion) also qualitatively polled the code-mixed and sentiment\nproperties of the corpus."}, "authors": ["Soumil Mandal", "Sainik Kumar Mahata", "Dipankar Das"], "author_detail": {"name": "Dipankar Das"}, "author": "Dipankar Das", "arxiv_comment": "The 13th Workshop on Asian Language Resources (ALR), collocated with\n  LREC 2018", "links": [{"href": "http://arxiv.org/abs/1803.04000v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1803.04000v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1803.04000v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1803.04000v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1805.01442v2", "guidislink": true, "updated": "2018-09-16T16:40:02Z", "updated_parsed": [2018, 9, 16, 16, 40, 2, 6, 259, 0], "published": "2018-05-03T17:35:45Z", "published_parsed": [2018, 5, 3, 17, 35, 45, 3, 123, 0], "title": "InceptB: A CNN Based Classification Approach for Recognizing Traditional\n  Bengali Games", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=60&max_results=10&sortBy=relevance&sortOrder=descending", "value": "InceptB: A CNN Based Classification Approach for Recognizing Traditional\n  Bengali Games"}, "summary": "Sports activities are an integral part of our day to day life. Introducing\nautonomous decision making and predictive models to recognize and analyze\ndifferent sports events and activities has become an emerging trend in computer\nvision arena. Albeit the advances and vivid applications of artificial\nintelligence and computer vision in recognizing different popular western\ngames, there remains a very minimal amount of efforts in the application of\ncomputer vision in recognizing traditional Bangladeshi games. We, in this\npaper, have described a novel Deep Learning based approach for recognizing\ntraditional Bengali games. We have retrained the final layer of the renowned\nInception V3 architecture developed by Google for our classification approach.\nOur approach shows promising results with an average accuracy of 80%\napproximately in correctly recognizing among 5 traditional Bangladeshi sports\nevents.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=60&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Sports activities are an integral part of our day to day life. Introducing\nautonomous decision making and predictive models to recognize and analyze\ndifferent sports events and activities has become an emerging trend in computer\nvision arena. Albeit the advances and vivid applications of artificial\nintelligence and computer vision in recognizing different popular western\ngames, there remains a very minimal amount of efforts in the application of\ncomputer vision in recognizing traditional Bangladeshi games. We, in this\npaper, have described a novel Deep Learning based approach for recognizing\ntraditional Bengali games. We have retrained the final layer of the renowned\nInception V3 architecture developed by Google for our classification approach.\nOur approach shows promising results with an average accuracy of 80%\napproximately in correctly recognizing among 5 traditional Bangladeshi sports\nevents."}, "authors": ["Mohammad Shakirul Islam", "Ferdouse Ahmed Foysal", "Nafis Neehal", "Enamul Karim", "Syed Akhter Hossain"], "author_detail": {"name": "Syed Akhter Hossain"}, "author": "Syed Akhter Hossain", "arxiv_comment": "8 pages, 8 sections (including reference), 5 images, 2 tables", "links": [{"href": "http://arxiv.org/abs/1805.01442v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1805.01442v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1805.01442v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1805.01442v2", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1812.04898v1", "guidislink": true, "updated": "2018-12-12T11:11:08Z", "updated_parsed": [2018, 12, 12, 11, 11, 8, 2, 346, 0], "published": "2018-12-12T11:11:08Z", "published_parsed": [2018, 12, 12, 11, 11, 8, 2, 346, 0], "title": "SMT vs NMT: A Comparison over Hindi & Bengali Simple Sentences", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=60&max_results=10&sortBy=relevance&sortOrder=descending", "value": "SMT vs NMT: A Comparison over Hindi & Bengali Simple Sentences"}, "summary": "In the present article, we identified the qualitative differences between\nStatistical Machine Translation (SMT) and Neural Machine Translation (NMT)\noutputs. We have tried to answer two important questions: 1. Does NMT perform\nequivalently well with respect to SMT and 2. Does it add extra flavor in\nimproving the quality of MT output by employing simple sentences as training\nunits. In order to obtain insights, we have developed three core models viz.,\nSMT model based on Moses toolkit, followed by character and word level NMT\nmodels. All of the systems use English-Hindi and English-Bengali language pairs\ncontaining simple sentences as well as sentences of other complexity. In order\nto preserve the translations semantics with respect to the target words of a\nsentence, we have employed soft-attention into our word level NMT model. We\nhave further evaluated all the systems with respect to the scenarios where they\nsucceed and fail. Finally, the quality of translation has been validated using\nBLEU and TER metrics along with manual parameters like fluency, adequacy etc.\nWe observed that NMT outperforms SMT in case of simple sentences whereas SMT\noutperforms in case of all types of sentence.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=60&max_results=10&sortBy=relevance&sortOrder=descending", "value": "In the present article, we identified the qualitative differences between\nStatistical Machine Translation (SMT) and Neural Machine Translation (NMT)\noutputs. We have tried to answer two important questions: 1. Does NMT perform\nequivalently well with respect to SMT and 2. Does it add extra flavor in\nimproving the quality of MT output by employing simple sentences as training\nunits. In order to obtain insights, we have developed three core models viz.,\nSMT model based on Moses toolkit, followed by character and word level NMT\nmodels. All of the systems use English-Hindi and English-Bengali language pairs\ncontaining simple sentences as well as sentences of other complexity. In order\nto preserve the translations semantics with respect to the target words of a\nsentence, we have employed soft-attention into our word level NMT model. We\nhave further evaluated all the systems with respect to the scenarios where they\nsucceed and fail. Finally, the quality of translation has been validated using\nBLEU and TER metrics along with manual parameters like fluency, adequacy etc.\nWe observed that NMT outperforms SMT in case of simple sentences whereas SMT\noutperforms in case of all types of sentence."}, "authors": ["Sainik Kumar Mahata", "Soumil Mandal", "Dipankar Das", "Sivaji Bandyopadhyay"], "author_detail": {"name": "Sivaji Bandyopadhyay"}, "author": "Sivaji Bandyopadhyay", "links": [{"href": "http://arxiv.org/abs/1812.04898v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1812.04898v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1812.04898v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1812.04898v1", "arxiv_comment": null, "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1910.09233v1", "guidislink": true, "updated": "2019-10-21T09:37:46Z", "updated_parsed": [2019, 10, 21, 9, 37, 46, 0, 294, 0], "published": "2019-10-21T09:37:46Z", "published_parsed": [2019, 10, 21, 9, 37, 46, 0, 294, 0], "title": "CNN based Extraction of Panels/Characters from Bengali Comic Book Page\n  Images", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=70&max_results=10&sortBy=relevance&sortOrder=descending", "value": "CNN based Extraction of Panels/Characters from Bengali Comic Book Page\n  Images"}, "summary": "Peoples nowadays prefer to use digital gadgets like cameras or mobile phones\nfor capturing documents. Automatic extraction of panels/characters from the\nimages of a comic document is challenging due to the wide variety of drawing\nstyles adopted by writers, beneficial for readers to read them on mobile\ndevices at any time and useful for automatic digitization. Most of the methods\nfor localization of panel/character rely on the connected component analysis or\npage background mask and are applicable only for a limited comic dataset. This\nwork proposes a panel/character localization architecture based on the features\nof YOLO and CNN for extraction of both panels and characters from comic book\nimages. The method achieved remarkable results on Bengali Comic Book Image\ndataset (BCBId) consisting of total $4130$ images, developed by us as well as\non a variety of publicly available comic datasets in other languages, i.e.\neBDtheque, Manga 109 and DCM dataset.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=70&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Peoples nowadays prefer to use digital gadgets like cameras or mobile phones\nfor capturing documents. Automatic extraction of panels/characters from the\nimages of a comic document is challenging due to the wide variety of drawing\nstyles adopted by writers, beneficial for readers to read them on mobile\ndevices at any time and useful for automatic digitization. Most of the methods\nfor localization of panel/character rely on the connected component analysis or\npage background mask and are applicable only for a limited comic dataset. This\nwork proposes a panel/character localization architecture based on the features\nof YOLO and CNN for extraction of both panels and characters from comic book\nimages. The method achieved remarkable results on Bengali Comic Book Image\ndataset (BCBId) consisting of total $4130$ images, developed by us as well as\non a variety of publicly available comic datasets in other languages, i.e.\neBDtheque, Manga 109 and DCM dataset."}, "authors": ["Arpita Dutta", "Samit Biswas"], "author_detail": {"name": "Samit Biswas"}, "author": "Samit Biswas", "arxiv_comment": "6 pages, 3 tables and 3 figures. Accepted at GREC 2019 in conjunction\n  with ICDAR 2019", "links": [{"href": "http://arxiv.org/abs/1910.09233v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1910.09233v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1910.09233v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1910.09233v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/2002.06701v1", "guidislink": true, "updated": "2020-02-16T23:03:32Z", "updated_parsed": [2020, 2, 16, 23, 3, 32, 6, 47, 0], "published": "2020-02-16T23:03:32Z", "published_parsed": [2020, 2, 16, 23, 3, 32, 6, 47, 0], "title": "Gaussian Smoothen Semantic Features (GSSF) -- Exploring the Linguistic\n  Aspects of Visual Captioning in Indian Languages (Bengali) Using MSCOCO\n  Framework", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=70&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Gaussian Smoothen Semantic Features (GSSF) -- Exploring the Linguistic\n  Aspects of Visual Captioning in Indian Languages (Bengali) Using MSCOCO\n  Framework"}, "summary": "In this work, we have introduced Gaussian Smoothen Semantic Features (GSSF)\nfor Better Semantic Selection for Indian regional language-based image\ncaptioning and introduced a procedure where we used the existing translation\nand English crowd-sourced sentences for training. We have shown that this\narchitecture is a promising alternative source, where there is a crunch in\nresources. Our main contribution of this work is the development of deep\nlearning architectures for the Bengali language (is the fifth widely spoken\nlanguage in the world) with a completely different grammar and language\nattributes. We have shown that these are working well for complex applications\nlike language generation from image contexts and can diversify the\nrepresentation through introducing constraints, more extensive features, and\nunique feature spaces. We also established that we could achieve absolute\nprecision and diversity when we use smoothened semantic tensor with the\ntraditional LSTM and feature decomposition networks. With better learning\narchitecture, we succeeded in establishing an automated algorithm and\nassessment procedure that can help in the evaluation of competent applications\nwithout the requirement for expertise and human intervention.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=70&max_results=10&sortBy=relevance&sortOrder=descending", "value": "In this work, we have introduced Gaussian Smoothen Semantic Features (GSSF)\nfor Better Semantic Selection for Indian regional language-based image\ncaptioning and introduced a procedure where we used the existing translation\nand English crowd-sourced sentences for training. We have shown that this\narchitecture is a promising alternative source, where there is a crunch in\nresources. Our main contribution of this work is the development of deep\nlearning architectures for the Bengali language (is the fifth widely spoken\nlanguage in the world) with a completely different grammar and language\nattributes. We have shown that these are working well for complex applications\nlike language generation from image contexts and can diversify the\nrepresentation through introducing constraints, more extensive features, and\nunique feature spaces. We also established that we could achieve absolute\nprecision and diversity when we use smoothened semantic tensor with the\ntraditional LSTM and feature decomposition networks. With better learning\narchitecture, we succeeded in establishing an automated algorithm and\nassessment procedure that can help in the evaluation of competent applications\nwithout the requirement for expertise and human intervention."}, "authors": ["Chiranjib Sur"], "author_detail": {"name": "Chiranjib Sur"}, "author": "Chiranjib Sur", "links": [{"href": "http://arxiv.org/abs/2002.06701v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2002.06701v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.NE", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2002.06701v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2002.06701v1", "arxiv_comment": null, "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/2010.05324v1", "guidislink": true, "updated": "2020-10-11T19:17:24Z", "updated_parsed": [2020, 10, 11, 19, 17, 24, 6, 285, 0], "published": "2020-10-11T19:17:24Z", "published_parsed": [2020, 10, 11, 19, 17, 24, 6, 285, 0], "title": "Multilingual Offensive Language Identification with Cross-lingual\n  Embeddings", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=70&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Multilingual Offensive Language Identification with Cross-lingual\n  Embeddings"}, "summary": "Offensive content is pervasive in social media and a reason for concern to\ncompanies and government organizations. Several studies have been recently\npublished investigating methods to detect the various forms of such content\n(e.g. hate speech, cyberbulling, and cyberaggression). The clear majority of\nthese studies deal with English partially because most annotated datasets\navailable contain English data. In this paper, we take advantage of English\ndata available by applying cross-lingual contextual word embeddings and\ntransfer learning to make predictions in languages with less resources. We\nproject predictions on comparable data in Bengali, Hindi, and Spanish and we\nreport results of 0.8415 F1 macro for Bengali, 0.8568 F1 macro for Hindi, and\n0.7513 F1 macro for Spanish. Finally, we show that our approach compares\nfavorably to the best systems submitted to recent shared tasks on these three\nlanguages, confirming the robustness of cross-lingual contextual embeddings and\ntransfer learning for this task.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=70&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Offensive content is pervasive in social media and a reason for concern to\ncompanies and government organizations. Several studies have been recently\npublished investigating methods to detect the various forms of such content\n(e.g. hate speech, cyberbulling, and cyberaggression). The clear majority of\nthese studies deal with English partially because most annotated datasets\navailable contain English data. In this paper, we take advantage of English\ndata available by applying cross-lingual contextual word embeddings and\ntransfer learning to make predictions in languages with less resources. We\nproject predictions on comparable data in Bengali, Hindi, and Spanish and we\nreport results of 0.8415 F1 macro for Bengali, 0.8568 F1 macro for Hindi, and\n0.7513 F1 macro for Spanish. Finally, we show that our approach compares\nfavorably to the best systems submitted to recent shared tasks on these three\nlanguages, confirming the robustness of cross-lingual contextual embeddings and\ntransfer learning for this task."}, "authors": ["Tharindu Ranasinghe", "Marcos Zampieri"], "author_detail": {"name": "Marcos Zampieri"}, "author": "Marcos Zampieri", "arxiv_comment": "Accepted to EMNLP 2020", "links": [{"href": "http://arxiv.org/abs/2010.05324v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2010.05324v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2010.05324v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2010.05324v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/2011.02323v1", "guidislink": true, "updated": "2020-11-04T14:43:43Z", "updated_parsed": [2020, 11, 4, 14, 43, 43, 2, 309, 0], "published": "2020-11-04T14:43:43Z", "published_parsed": [2020, 11, 4, 14, 43, 43, 2, 309, 0], "title": "Indic-Transformers: An Analysis of Transformer Language Models for\n  Indian Languages", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=70&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Indic-Transformers: An Analysis of Transformer Language Models for\n  Indian Languages"}, "summary": "Language models based on the Transformer architecture have achieved\nstate-of-the-art performance on a wide range of NLP tasks such as text\nclassification, question-answering, and token classification. However, this\nperformance is usually tested and reported on high-resource languages, like\nEnglish, French, Spanish, and German. Indian languages, on the other hand, are\nunderrepresented in such benchmarks. Despite some Indian languages being\nincluded in training multilingual Transformer models, they have not been the\nprimary focus of such work. In order to evaluate the performance on Indian\nlanguages specifically, we analyze these language models through extensive\nexperiments on multiple downstream tasks in Hindi, Bengali, and Telugu\nlanguage. Here, we compare the efficacy of fine-tuning model parameters of\npre-trained models against that of training a language model from scratch.\nMoreover, we empirically argue against the strict dependency between the\ndataset size and model performance, but rather encourage task-specific model\nand method selection. We achieve state-of-the-art performance on Hindi and\nBengali languages for text classification task. Finally, we present effective\nstrategies for handling the modeling of Indian languages and we release our\nmodel checkpoints for the community :\nhttps://huggingface.co/neuralspace-reverie.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=70&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Language models based on the Transformer architecture have achieved\nstate-of-the-art performance on a wide range of NLP tasks such as text\nclassification, question-answering, and token classification. However, this\nperformance is usually tested and reported on high-resource languages, like\nEnglish, French, Spanish, and German. Indian languages, on the other hand, are\nunderrepresented in such benchmarks. Despite some Indian languages being\nincluded in training multilingual Transformer models, they have not been the\nprimary focus of such work. In order to evaluate the performance on Indian\nlanguages specifically, we analyze these language models through extensive\nexperiments on multiple downstream tasks in Hindi, Bengali, and Telugu\nlanguage. Here, we compare the efficacy of fine-tuning model parameters of\npre-trained models against that of training a language model from scratch.\nMoreover, we empirically argue against the strict dependency between the\ndataset size and model performance, but rather encourage task-specific model\nand method selection. We achieve state-of-the-art performance on Hindi and\nBengali languages for text classification task. Finally, we present effective\nstrategies for handling the modeling of Indian languages and we release our\nmodel checkpoints for the community :\nhttps://huggingface.co/neuralspace-reverie."}, "authors": ["Kushal Jain", "Adwait Deshpande", "Kumar Shridhar", "Felix Laumann", "Ayushman Dash"], "author_detail": {"name": "Ayushman Dash"}, "author": "Ayushman Dash", "arxiv_comment": "Accepted at ML-RSA @ NeurIPS 2020", "links": [{"href": "http://arxiv.org/abs/2011.02323v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2011.02323v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2011.02323v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2011.02323v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/2012.01494v1", "guidislink": true, "updated": "2020-12-02T19:57:29Z", "updated_parsed": [2020, 12, 2, 19, 57, 29, 2, 337, 0], "published": "2020-12-02T19:57:29Z", "published_parsed": [2020, 12, 2, 19, 57, 29, 2, 337, 0], "title": "Braille to Text Translation for Bengali Language: A Geometric Approach", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=70&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Braille to Text Translation for Bengali Language: A Geometric Approach"}, "summary": "Braille is the only system to visually impaired people for reading and\nwriting. However, general people cannot read Braille. So, teachers and\nrelatives find it hard to assist them with learning. Almost every major\nlanguage has software solutions for this translation purpose. However, in\nBengali there is an absence of this useful tool. Here, we propose Braille to\nText Translator, which takes image of these tactile alphabets, and translates\nthem to plain text. Image deterioration, scan-time page rotation, and braille\ndot deformation are the principal issues in this scheme. All of these\nchallenges are directly checked using special image processing and geometric\nstructure analysis. The technique yields 97.25% accuracy in recognizing Braille\ncharacters.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=70&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Braille is the only system to visually impaired people for reading and\nwriting. However, general people cannot read Braille. So, teachers and\nrelatives find it hard to assist them with learning. Almost every major\nlanguage has software solutions for this translation purpose. However, in\nBengali there is an absence of this useful tool. Here, we propose Braille to\nText Translator, which takes image of these tactile alphabets, and translates\nthem to plain text. Image deterioration, scan-time page rotation, and braille\ndot deformation are the principal issues in this scheme. All of these\nchallenges are directly checked using special image processing and geometric\nstructure analysis. The technique yields 97.25% accuracy in recognizing Braille\ncharacters."}, "authors": ["Minhas Kamal", "Dr. Amin Ahsan Ali", "Dr. Muhammad Asif Hossain Khan", "Dr. Mohammad Shoyaib"], "author_detail": {"name": "Dr. Mohammad Shoyaib"}, "author": "Dr. Mohammad Shoyaib", "links": [{"href": "http://arxiv.org/abs/2012.01494v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2012.01494v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2012.01494v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2012.01494v1", "arxiv_comment": null, "journal_reference": "In Jahangirnagar University Journal of Information Technology\n  (JJIT), pp. 93-111, 2018", "doi": null}
{"id": "http://arxiv.org/abs/1508.01346v1", "guidislink": true, "updated": "2015-08-06T10:15:51Z", "updated_parsed": [2015, 8, 6, 10, 15, 51, 3, 218, 0], "published": "2015-08-06T10:15:51Z", "published_parsed": [2015, 8, 6, 10, 15, 51, 3, 218, 0], "title": "Word sense disambiguation: a survey", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=70&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Word sense disambiguation: a survey"}, "summary": "In this paper, we made a survey on Word Sense Disambiguation (WSD). Near\nabout in all major languages around the world, research in WSD has been\nconducted upto different extents. In this paper, we have gone through a survey\nregarding the different approaches adopted in different research works, the\nState of the Art in the performance in this domain, recent works in different\nIndian languages and finally a survey in Bengali language. We have made a\nsurvey on different competitions in this field and the bench mark results,\nobtained from those competitions.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=70&max_results=10&sortBy=relevance&sortOrder=descending", "value": "In this paper, we made a survey on Word Sense Disambiguation (WSD). Near\nabout in all major languages around the world, research in WSD has been\nconducted upto different extents. In this paper, we have gone through a survey\nregarding the different approaches adopted in different research works, the\nState of the Art in the performance in this domain, recent works in different\nIndian languages and finally a survey in Bengali language. We have made a\nsurvey on different competitions in this field and the bench mark results,\nobtained from those competitions."}, "authors": ["Alok Ranjan Pal", "Diganta Saha"], "author_detail": {"name": "Diganta Saha"}, "author": "Diganta Saha", "links": [{"title": "doi", "href": "http://dx.doi.org/10.5121/ijctcm.2015.5301", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/1508.01346v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1508.01346v1", "rel": "related", "type": "application/pdf"}], "arxiv_comment": "International Journal of Control Theory and Computer Modeling\n  (IJCTCM) Vol.5, No.3, July 2015", "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1508.01346v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1508.01346v1", "journal_reference": null, "doi": "10.5121/ijctcm.2015.5301"}
{"id": "http://arxiv.org/abs/1907.13356v2", "guidislink": true, "updated": "2019-11-12T08:56:48Z", "updated_parsed": [2019, 11, 12, 8, 56, 48, 1, 316, 0], "published": "2019-07-31T07:53:57Z", "published_parsed": [2019, 7, 31, 7, 53, 57, 2, 212, 0], "title": "Normalyzing Numeronyms -- A NLP approach", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=80&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Normalyzing Numeronyms -- A NLP approach"}, "summary": "This paper presents a method to apply Natural Language Processing for\nnormalizing numeronyms to make them understandable by humans. We approach the\nproblem through a two-step mechanism. We make use of the state of the art\nLevenshtein distance of words. We then apply Cosine Similarity for selection of\nthe normalized text and reach greater accuracy in solving the problem. Our\napproach garners accuracy figures of 71\\% and 72\\% for Bengali and English\nlanguage, respectively.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=80&max_results=10&sortBy=relevance&sortOrder=descending", "value": "This paper presents a method to apply Natural Language Processing for\nnormalizing numeronyms to make them understandable by humans. We approach the\nproblem through a two-step mechanism. We make use of the state of the art\nLevenshtein distance of words. We then apply Cosine Similarity for selection of\nthe normalized text and reach greater accuracy in solving the problem. Our\napproach garners accuracy figures of 71\\% and 72\\% for Bengali and English\nlanguage, respectively."}, "authors": ["Avishek Garain", "Sainik Kumar Mahata", "Subhabrata Dutta"], "author_detail": {"name": "Subhabrata Dutta"}, "author": "Subhabrata Dutta", "links": [{"href": "http://arxiv.org/abs/1907.13356v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1907.13356v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1907.13356v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1907.13356v2", "arxiv_comment": null, "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1911.02989v1", "guidislink": true, "updated": "2019-11-08T02:19:52Z", "updated_parsed": [2019, 11, 8, 2, 19, 52, 4, 312, 0], "published": "2019-11-08T02:19:52Z", "published_parsed": [2019, 11, 8, 2, 19, 52, 4, 312, 0], "title": "Cross-Lingual Relevance Transfer for Document Retrieval", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=80&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Cross-Lingual Relevance Transfer for Document Retrieval"}, "summary": "Recent work has shown the surprising ability of multi-lingual BERT to serve\nas a zero-shot cross-lingual transfer model for a number of language processing\ntasks. We combine this finding with a similarly-recently proposal on\nsentence-level relevance modeling for document retrieval to demonstrate the\nability of multi-lingual BERT to transfer models of relevance across languages.\nExperiments on test collections in five different languages from diverse\nlanguage families (Chinese, Arabic, French, Hindi, and Bengali) show that\nmodels trained with English data improve ranking quality, without any special\nprocessing, both for (non-English) mono-lingual retrieval as well as\ncross-lingual retrieval.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=80&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Recent work has shown the surprising ability of multi-lingual BERT to serve\nas a zero-shot cross-lingual transfer model for a number of language processing\ntasks. We combine this finding with a similarly-recently proposal on\nsentence-level relevance modeling for document retrieval to demonstrate the\nability of multi-lingual BERT to transfer models of relevance across languages.\nExperiments on test collections in five different languages from diverse\nlanguage families (Chinese, Arabic, French, Hindi, and Bengali) show that\nmodels trained with English data improve ranking quality, without any special\nprocessing, both for (non-English) mono-lingual retrieval as well as\ncross-lingual retrieval."}, "authors": ["Peng Shi", "Jimmy Lin"], "author_detail": {"name": "Jimmy Lin"}, "author": "Jimmy Lin", "links": [{"href": "http://arxiv.org/abs/1911.02989v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1911.02989v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.IR", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.IR", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1911.02989v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1911.02989v1", "arxiv_comment": null, "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/2007.14074v1", "guidislink": true, "updated": "2020-07-28T09:04:47Z", "updated_parsed": [2020, 7, 28, 9, 4, 47, 1, 210, 0], "published": "2020-07-28T09:04:47Z", "published_parsed": [2020, 7, 28, 9, 4, 47, 1, 210, 0], "title": "Preparation of Sentiment tagged Parallel Corpus and Testing its effect\n  on Machine Translation", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=80&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Preparation of Sentiment tagged Parallel Corpus and Testing its effect\n  on Machine Translation"}, "summary": "In the current work, we explore the enrichment in the machine translation\noutput when the training parallel corpus is augmented with the introduction of\nsentiment analysis. The paper discusses the preparation of the same sentiment\ntagged English-Bengali parallel corpus. The preparation of raw parallel corpus,\nsentiment analysis of the sentences and the training of a Character Based\nNeural Machine Translation model using the same has been discussed extensively\nin this paper. The output of the translation model has been compared with a\nbase-line translation model using automated metrics such as BLEU and TER as\nwell as manually.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=80&max_results=10&sortBy=relevance&sortOrder=descending", "value": "In the current work, we explore the enrichment in the machine translation\noutput when the training parallel corpus is augmented with the introduction of\nsentiment analysis. The paper discusses the preparation of the same sentiment\ntagged English-Bengali parallel corpus. The preparation of raw parallel corpus,\nsentiment analysis of the sentences and the training of a Character Based\nNeural Machine Translation model using the same has been discussed extensively\nin this paper. The output of the translation model has been compared with a\nbase-line translation model using automated metrics such as BLEU and TER as\nwell as manually."}, "authors": ["Sainik Kumar Mahata", "Amrita Chandra", "Dipankar Das", "Sivaji Bandyopadhyay"], "author_detail": {"name": "Sivaji Bandyopadhyay"}, "author": "Sivaji Bandyopadhyay", "links": [{"href": "http://arxiv.org/abs/2007.14074v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2007.14074v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2007.14074v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2007.14074v1", "arxiv_comment": null, "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/0909.0247v2", "guidislink": true, "updated": "2009-09-08T02:47:43Z", "updated_parsed": [2009, 9, 8, 2, 47, 43, 1, 251, 0], "published": "2009-09-01T19:54:57Z", "published_parsed": [2009, 9, 1, 19, 54, 57, 1, 244, 0], "title": "An Enhanced Static Data Compression Scheme Of Bengali Short Message", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=80&max_results=10&sortBy=relevance&sortOrder=descending", "value": "An Enhanced Static Data Compression Scheme Of Bengali Short Message"}, "summary": "This paper concerns a modified approach of compressing Short Bengali Text\nMessage for small devices. The prime objective of this research technique is to\nestablish a low complexity compression scheme suitable for small devices having\nsmall memory and relatively lower processing speed. The basic aim is not to\ncompress text of any size up to its maximum level without having any constraint\non space and time, rather than the main target is to compress short messages up\nto an optimal level which needs minimum space, consume less time and the\nprocessor requirement is lower. We have implemented Character Masking,\nDictionary Matching, Associative rule of data mining and Hyphenation algorithm\nfor syllable based compression in hierarchical steps to achieve low complexity\nlossless compression of text message for any mobile devices. The scheme to\nchoose the diagrams are performed on the basis of extensive statistical model\nand the static Huffman coding is done through the same context.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=80&max_results=10&sortBy=relevance&sortOrder=descending", "value": "This paper concerns a modified approach of compressing Short Bengali Text\nMessage for small devices. The prime objective of this research technique is to\nestablish a low complexity compression scheme suitable for small devices having\nsmall memory and relatively lower processing speed. The basic aim is not to\ncompress text of any size up to its maximum level without having any constraint\non space and time, rather than the main target is to compress short messages up\nto an optimal level which needs minimum space, consume less time and the\nprocessor requirement is lower. We have implemented Character Masking,\nDictionary Matching, Associative rule of data mining and Hyphenation algorithm\nfor syllable based compression in hierarchical steps to achieve low complexity\nlossless compression of text message for any mobile devices. The scheme to\nchoose the diagrams are performed on the basis of extensive statistical model\nand the static Huffman coding is done through the same context."}, "authors": ["Abu Shamim Mohammad Arif", "Asif Mahamud", "Rashedul Islam"], "author_detail": {"name": "Rashedul Islam"}, "author": "Rashedul Islam", "arxiv_comment": "7 Pages IEEE format, International Journal of Computer Science and\n  Information Security, IJCSIS 2009, ISSN 1947 5500, Impact factor 0.423\n  http://sites.google.com/site/ijcsis/", "links": [{"href": "http://arxiv.org/abs/0909.0247v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/0909.0247v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.IT", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.IT", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "math.IT", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/0909.0247v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/0909.0247v2", "journal_reference": "International Journal of Computer Science and Information\n  Security, IJCSIS, Vol. 4, No. 1 & 2, August 2009, USA", "doi": null}
{"id": "http://arxiv.org/abs/1911.01256v2", "guidislink": true, "updated": "2020-02-25T18:22:01Z", "updated_parsed": [2020, 2, 25, 18, 22, 1, 1, 56, 0], "published": "2019-11-04T14:47:33Z", "published_parsed": [2019, 11, 4, 14, 47, 33, 0, 308, 0], "title": "A Novel Approach to Enhance the Performance of Semantic Search in\n  Bengali using Neural Net and other Classification Techniques", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=80&max_results=10&sortBy=relevance&sortOrder=descending", "value": "A Novel Approach to Enhance the Performance of Semantic Search in\n  Bengali using Neural Net and other Classification Techniques"}, "summary": "Search has for a long time been an important tool for users to retrieve\ninformation. Syntactic search is matching documents or objects containing\nspecific keywords like user-history, location, preference etc. to improve the\nresults. However, it is often possible that the query and the best answer have\nno term or very less number of terms in common and syntactic search can not\nperform properly in such cases. Semantic search, on the other hand, resolves\nthese issues but suffers from lack of annotation, absence of WordNet in case of\nlow resource languages. In this work, we have demonstrated an end to end\nprocedure to improve the performance of semantic search using semi-supervised\nand unsupervised learning algorithms. An available Bengali repository was\nchosen to have seven types of semantic properties primarily to develop the\nsystem. Performance has been tested using Support Vector Machine, Naive Bayes,\nDecision Tree and Artificial Neural Network (ANN). Our system has achieved the\nefficiency to predict the correct semantics using knowledge base over the time\nof learning. A repository containing around a million sentences, a product of\nTDIL project of Govt. of India, was used to test our system at first instance.\nThen the testing has been done for other languages. Being a cognitive system it\nmay be very useful for improving user satisfaction in e-Governance or\nm-Governance in the multilingual environment and also for other applications.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=80&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Search has for a long time been an important tool for users to retrieve\ninformation. Syntactic search is matching documents or objects containing\nspecific keywords like user-history, location, preference etc. to improve the\nresults. However, it is often possible that the query and the best answer have\nno term or very less number of terms in common and syntactic search can not\nperform properly in such cases. Semantic search, on the other hand, resolves\nthese issues but suffers from lack of annotation, absence of WordNet in case of\nlow resource languages. In this work, we have demonstrated an end to end\nprocedure to improve the performance of semantic search using semi-supervised\nand unsupervised learning algorithms. An available Bengali repository was\nchosen to have seven types of semantic properties primarily to develop the\nsystem. Performance has been tested using Support Vector Machine, Naive Bayes,\nDecision Tree and Artificial Neural Network (ANN). Our system has achieved the\nefficiency to predict the correct semantics using knowledge base over the time\nof learning. A repository containing around a million sentences, a product of\nTDIL project of Govt. of India, was used to test our system at first instance.\nThen the testing has been done for other languages. Being a cognitive system it\nmay be very useful for improving user satisfaction in e-Governance or\nm-Governance in the multilingual environment and also for other applications."}, "authors": ["Arijit Das", "Diganta Saha"], "author_detail": {"name": "Diganta Saha"}, "author": "Diganta Saha", "arxiv_comment": "12 pages, 5 figures", "links": [{"href": "http://arxiv.org/abs/1911.01256v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1911.01256v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.IR", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1911.01256v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1911.01256v2", "journal_reference": "IJEAT Vol 9 Issue 3 year 2020 ISSN 2249-8958", "doi": null}
{"id": "http://arxiv.org/abs/cs/0306055v1", "guidislink": true, "updated": "2003-06-12T15:53:17Z", "updated_parsed": [2003, 6, 12, 15, 53, 17, 3, 163, 0], "published": "2003-06-12T15:53:17Z", "published_parsed": [2003, 6, 12, 15, 53, 17, 3, 163, 0], "title": "BlueOx: A Java Framework for Distributed Data Analysis", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=90&max_results=10&sortBy=relevance&sortOrder=descending", "value": "BlueOx: A Java Framework for Distributed Data Analysis"}, "summary": "High energy physics experiments including those at the Tevatron and the\nupcoming LHC require analysis of large data sets which are best handled by\ndistributed computation. We present the design and development of a distributed\ndata analysis framework based on Java. Analysis jobs run through three phases:\ndiscovery of data sets available, brokering/assignment of data sets to analysis\nservers, and job execution. Each phase is represented by a set of abstract\ninterfaces. These interfaces allow different techniques to be used without\nmodification to the framework. For example, the communications interface has\nbeen implemented by both a packet protocol and a SOAP-based scheme. User\nauthentication can be provided either through simple passwords or through a GSI\ncertificates system. Data from CMS HCAL Testbeams, the L3 LEP experiment, and a\nhypothetical high-energy linear collider experiment have been interfaced with\nthe framework.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=90&max_results=10&sortBy=relevance&sortOrder=descending", "value": "High energy physics experiments including those at the Tevatron and the\nupcoming LHC require analysis of large data sets which are best handled by\ndistributed computation. We present the design and development of a distributed\ndata analysis framework based on Java. Analysis jobs run through three phases:\ndiscovery of data sets available, brokering/assignment of data sets to analysis\nservers, and job execution. Each phase is represented by a set of abstract\ninterfaces. These interfaces allow different techniques to be used without\nmodification to the framework. For example, the communications interface has\nbeen implemented by both a packet protocol and a SOAP-based scheme. User\nauthentication can be provided either through simple passwords or through a GSI\ncertificates system. Data from CMS HCAL Testbeams, the L3 LEP experiment, and a\nhypothetical high-energy linear collider experiment have been interfaced with\nthe framework."}, "authors": ["Jeremiah Mans", "David Bengali"], "author_detail": {"name": "David Bengali"}, "author": "David Bengali", "arxiv_comment": "Talk from the 2003 Computing in High Energy and Nuclear Physics\n  (CHEP03), La Jolla, Ca, USA, March 2003, 5 pages, LaTeX, 1 eps figure. PSN\n  TULT006", "links": [{"href": "http://arxiv.org/abs/cs/0306055v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/cs/0306055v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.DC", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.DC", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "J.2;D.4.7", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/cs/0306055v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/cs/0306055v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/cs/0308019v1", "guidislink": true, "updated": "2003-08-07T09:40:04Z", "updated_parsed": [2003, 8, 7, 9, 40, 4, 3, 219, 0], "published": "2003-08-07T09:40:04Z", "published_parsed": [2003, 8, 7, 9, 40, 4, 3, 219, 0], "title": "Language Access: An Information Based Approach", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=90&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Language Access: An Information Based Approach"}, "summary": "The anusaaraka system (a kind of machine translation system) makes text in\none Indian language accessible through another Indian language. The machine\npresents an image of the source text in a language close to the target\nlanguage. In the image, some constructions of the source language (which do not\nhave equivalents in the target language) spill over to the output. Some special\nnotation is also devised.\n  Anusaarakas have been built from five pairs of languages: Telugu,Kannada,\nMarathi, Bengali and Punjabi to Hindi. They are available for use through Email\nservers.\n  Anusaarkas follows the principle of substitutibility and reversibility of\nstrings produced. This implies preservation of information while going from a\nsource language to a target language.\n  For narrow subject areas, specialized modules can be built by putting subject\ndomain knowledge into the system, which produce good quality grammatical\noutput. However, it should be remembered, that such modules will work only in\nnarrow areas, and will sometimes go wrong. In such a situation, anusaaraka\noutput will still remain useful.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=90&max_results=10&sortBy=relevance&sortOrder=descending", "value": "The anusaaraka system (a kind of machine translation system) makes text in\none Indian language accessible through another Indian language. The machine\npresents an image of the source text in a language close to the target\nlanguage. In the image, some constructions of the source language (which do not\nhave equivalents in the target language) spill over to the output. Some special\nnotation is also devised.\n  Anusaarakas have been built from five pairs of languages: Telugu,Kannada,\nMarathi, Bengali and Punjabi to Hindi. They are available for use through Email\nservers.\n  Anusaarkas follows the principle of substitutibility and reversibility of\nstrings produced. This implies preservation of information while going from a\nsource language to a target language.\n  For narrow subject areas, specialized modules can be built by putting subject\ndomain knowledge into the system, which produce good quality grammatical\noutput. However, it should be remembered, that such modules will work only in\nnarrow areas, and will sometimes go wrong. In such a situation, anusaaraka\noutput will still remain useful."}, "authors": ["Akshar Bharati", "Vineet Chaitanya", "Amba P. Kulkarni", "Rajeev Sangal"], "author_detail": {"name": "Rajeev Sangal"}, "author": "Rajeev Sangal", "arxiv_comment": "Published in the proceedings of Knowledge Based Computer Systems\n  conference, 2000, Tata McGraw-Hill, New Delhi, Dec. 2000", "links": [{"href": "http://arxiv.org/abs/cs/0308019v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/cs/0308019v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "I,2,7", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/cs/0308019v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/cs/0308019v1", "journal_reference": "Published in the proceedings of Knowledge Based Computer Systems\n  Conference, 2000, Tata McGraw Hill, New Delhi 2000", "doi": null}
{"id": "http://arxiv.org/abs/1103.0738v1", "guidislink": true, "updated": "2011-03-03T17:31:48Z", "updated_parsed": [2011, 3, 3, 17, 31, 48, 3, 62, 0], "published": "2011-03-03T17:31:48Z", "published_parsed": [2011, 3, 3, 17, 31, 48, 3, 62, 0], "title": "A Medial Axis Based Thinning Strategy for Character Images", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=90&max_results=10&sortBy=relevance&sortOrder=descending", "value": "A Medial Axis Based Thinning Strategy for Character Images"}, "summary": "Thinning of character images is a big challenge. Removal of strokes or\ndeformities in thinning is a difficult problem. In this paper, we have proposed\na medial axis based thinning strategy used for performing skeletonization of\nprinted and handwritten character images. In this method, we have used shape\ncharacteristics of text to get skeleton of nearly same as the true character\nshape. This approach helps to preserve the local features and true shape of the\ncharacter images. The proposed algorithm produces one pixel width thin\nskeleton. As a by-product of our thinning approach, the skeleton also gets\nsegmented into strokes in vector form. Hence further stroke segmentation is not\nrequired. Experiment is done on printed English and Bengali characters and we\nobtain less spurious branches comparing with other thinning methods without any\npost processing.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=90&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Thinning of character images is a big challenge. Removal of strokes or\ndeformities in thinning is a difficult problem. In this paper, we have proposed\na medial axis based thinning strategy used for performing skeletonization of\nprinted and handwritten character images. In this method, we have used shape\ncharacteristics of text to get skeleton of nearly same as the true character\nshape. This approach helps to preserve the local features and true shape of the\ncharacter images. The proposed algorithm produces one pixel width thin\nskeleton. As a by-product of our thinning approach, the skeleton also gets\nsegmented into strokes in vector form. Hence further stroke segmentation is not\nrequired. Experiment is done on printed English and Bengali characters and we\nobtain less spurious branches comparing with other thinning methods without any\npost processing."}, "authors": ["Soumen Bag", "Gaurav Harit"], "author_detail": {"name": "Gaurav Harit"}, "author": "Gaurav Harit", "arxiv_comment": "6 pages, 5 figures. In proceedings of the second National Conference\n  on Computer Vision, Pattern Recognition, Image Processing and Graphics\n  (NCVPRIPG), pp. 67-72, Jaipur, India, 2010", "links": [{"href": "http://arxiv.org/abs/1103.0738v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1103.0738v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.DL", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1103.0738v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1103.0738v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1207.3932v1", "guidislink": true, "updated": "2012-07-17T10:14:24Z", "updated_parsed": [2012, 7, 17, 10, 14, 24, 1, 199, 0], "published": "2012-07-17T10:14:24Z", "published_parsed": [2012, 7, 17, 10, 14, 24, 1, 199, 0], "title": "Automatic Segmentation of Manipuri (Meiteilon) Word into Syllabic Units", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=90&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Automatic Segmentation of Manipuri (Meiteilon) Word into Syllabic Units"}, "summary": "The work of automatic segmentation of a Manipuri language (or Meiteilon) word\ninto syllabic units is demonstrated in this paper. This language is a scheduled\nIndian language of Tibeto-Burman origin, which is also a very highly\nagglutinative language. This language usages two script: a Bengali script and\nMeitei Mayek (Script). The present work is based on the second script. An\nalgorithm is designed so as to identify mainly the syllables of Manipuri origin\nword. The result of the algorithm shows a Recall of 74.77, Precision of 91.21\nand F-Score of 82.18 which is a reasonable score with the first attempt of such\nkind for this language.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=90&max_results=10&sortBy=relevance&sortOrder=descending", "value": "The work of automatic segmentation of a Manipuri language (or Meiteilon) word\ninto syllabic units is demonstrated in this paper. This language is a scheduled\nIndian language of Tibeto-Burman origin, which is also a very highly\nagglutinative language. This language usages two script: a Bengali script and\nMeitei Mayek (Script). The present work is based on the second script. An\nalgorithm is designed so as to identify mainly the syllables of Manipuri origin\nword. The result of the algorithm shows a Recall of 74.77, Precision of 91.21\nand F-Score of 82.18 which is a reasonable score with the first attempt of such\nkind for this language."}, "authors": ["Kishorjit Nongmeikapam", "Vidya Raj RK", "Oinam Imocha Singh", "Sivaji Bandyopadhyay"], "author_detail": {"name": "Sivaji Bandyopadhyay"}, "author": "Sivaji Bandyopadhyay", "links": [{"title": "doi", "href": "http://dx.doi.org/10.5121/ijcsit.2012.4311", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/1207.3932v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1207.3932v1", "rel": "related", "type": "application/pdf"}], "arxiv_comment": "12 Pages, 5 Tables See the link\n  http://airccse.org/journal/jcsit/0612csit11.pdf", "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "I.2.7", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1207.3932v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1207.3932v1", "journal_reference": null, "doi": "10.5121/ijcsit.2012.4311"}
{"id": "http://arxiv.org/abs/1310.1590v1", "guidislink": true, "updated": "2013-10-06T14:37:05Z", "updated_parsed": [2013, 10, 6, 14, 37, 5, 6, 279, 0], "published": "2013-10-06T14:37:05Z", "published_parsed": [2013, 10, 6, 14, 37, 5, 6, 279, 0], "title": "Evolution of the Modern Phase of Written Bangla: A Statistical Study", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=90&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Evolution of the Modern Phase of Written Bangla: A Statistical Study"}, "summary": "Active languages such as Bangla (or Bengali) evolve over time due to a\nvariety of social, cultural, economic, and political issues. In this paper, we\nanalyze the change in the written form of the modern phase of Bangla\nquantitatively in terms of character-level, syllable-level, morpheme-level and\nword-level features. We collect three different types of corpora---classical,\nnewspapers and blogs---and test whether the differences in their features are\nstatistically significant. Results suggest that there are significant changes\nin the length of a word when measured in terms of characters, but there is not\nmuch difference in usage of different characters, syllables and morphemes in a\nword or of different words in a sentence. To the best of our knowledge, this is\nthe first work on Bangla of this kind.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=90&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Active languages such as Bangla (or Bengali) evolve over time due to a\nvariety of social, cultural, economic, and political issues. In this paper, we\nanalyze the change in the written form of the modern phase of Bangla\nquantitatively in terms of character-level, syllable-level, morpheme-level and\nword-level features. We collect three different types of corpora---classical,\nnewspapers and blogs---and test whether the differences in their features are\nstatistically significant. Results suggest that there are significant changes\nin the length of a word when measured in terms of characters, but there is not\nmuch difference in usage of different characters, syllables and morphemes in a\nword or of different words in a sentence. To the best of our knowledge, this is\nthe first work on Bangla of this kind."}, "authors": ["Paheli Bhattacharya", "Arnab Bhattacharya"], "author_detail": {"name": "Arnab Bhattacharya"}, "author": "Arnab Bhattacharya", "arxiv_comment": "LCC 2013", "links": [{"href": "http://arxiv.org/abs/1310.1590v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1310.1590v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "I.2.7", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1310.1590v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1310.1590v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1502.05213v1", "guidislink": true, "updated": "2015-02-18T13:15:13Z", "updated_parsed": [2015, 2, 18, 13, 15, 13, 2, 49, 0], "published": "2015-02-18T13:15:13Z", "published_parsed": [2015, 2, 18, 13, 15, 13, 2, 49, 0], "title": "F0 Modeling In Hmm-Based Speech Synthesis System Using Deep Belief\n  Network", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=90&max_results=10&sortBy=relevance&sortOrder=descending", "value": "F0 Modeling In Hmm-Based Speech Synthesis System Using Deep Belief\n  Network"}, "summary": "In recent years multilayer perceptrons (MLPs) with many hid- den layers Deep\nNeural Network (DNN) has performed sur- prisingly well in many speech tasks,\ni.e. speech recognition, speaker verification, speech synthesis etc. Although\nin the context of F0 modeling these techniques has not been ex- ploited\nproperly. In this paper, Deep Belief Network (DBN), a class of DNN family has\nbeen employed and applied to model the F0 contour of synthesized speech which\nwas generated by HMM-based speech synthesis system. The experiment was done on\nBengali language. Several DBN-DNN architectures ranging from four to seven\nhidden layers and up to 200 hid- den units per hidden layer was presented and\nevaluated. The results were compared against clustering tree techniques pop-\nularly found in statistical parametric speech synthesis. We show that from\ntextual inputs DBN-DNN learns a high level structure which in turn improves F0\ncontour in terms of ob- jective and subjective tests.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=90&max_results=10&sortBy=relevance&sortOrder=descending", "value": "In recent years multilayer perceptrons (MLPs) with many hid- den layers Deep\nNeural Network (DNN) has performed sur- prisingly well in many speech tasks,\ni.e. speech recognition, speaker verification, speech synthesis etc. Although\nin the context of F0 modeling these techniques has not been ex- ploited\nproperly. In this paper, Deep Belief Network (DBN), a class of DNN family has\nbeen employed and applied to model the F0 contour of synthesized speech which\nwas generated by HMM-based speech synthesis system. The experiment was done on\nBengali language. Several DBN-DNN architectures ranging from four to seven\nhidden layers and up to 200 hid- den units per hidden layer was presented and\nevaluated. The results were compared against clustering tree techniques pop-\nularly found in statistical parametric speech synthesis. We show that from\ntextual inputs DBN-DNN learns a high level structure which in turn improves F0\ncontour in terms of ob- jective and subjective tests."}, "authors": ["Sankar Mukherjee", "Shyamal Kumar Das Mandal"], "author_detail": {"name": "Shyamal Kumar Das Mandal"}, "author": "Shyamal Kumar Das Mandal", "arxiv_comment": "OCOCOSDA 2014", "links": [{"href": "http://arxiv.org/abs/1502.05213v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1502.05213v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.NE", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1502.05213v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1502.05213v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1607.08883v1", "guidislink": true, "updated": "2016-07-29T18:20:24Z", "updated_parsed": [2016, 7, 29, 18, 20, 24, 4, 211, 0], "published": "2016-07-29T18:20:24Z", "published_parsed": [2016, 7, 29, 18, 20, 24, 4, 211, 0], "title": "Labeling of Query Words using Conditional Random Field", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=100&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Labeling of Query Words using Conditional Random Field"}, "summary": "This paper describes our approach on Query Word Labeling as an attempt in the\nshared task on Mixed Script Information Retrieval at Forum for Information\nRetrieval Evaluation (FIRE) 2015. The query is written in Roman script and the\nwords were in English or transliterated from Indian regional languages. A total\nof eight Indian languages were present in addition to English. We also\nidentified the Named Entities and special symbols as part of our task. A CRF\nbased machine learning framework was used for labeling the individual words\nwith their corresponding language labels. We used a dictionary based approach\nfor language identification. We also took into account the context of the word\nwhile identifying the language. Our system demonstrated an overall accuracy of\n75.5% for token level language identification. The strict F-measure scores for\nthe identification of token level language labels for Bengali, English and\nHindi are 0.7486, 0.892 and 0.7972 respectively. The overall weighted F-measure\nof our system was 0.7498.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=100&max_results=10&sortBy=relevance&sortOrder=descending", "value": "This paper describes our approach on Query Word Labeling as an attempt in the\nshared task on Mixed Script Information Retrieval at Forum for Information\nRetrieval Evaluation (FIRE) 2015. The query is written in Roman script and the\nwords were in English or transliterated from Indian regional languages. A total\nof eight Indian languages were present in addition to English. We also\nidentified the Named Entities and special symbols as part of our task. A CRF\nbased machine learning framework was used for labeling the individual words\nwith their corresponding language labels. We used a dictionary based approach\nfor language identification. We also took into account the context of the word\nwhile identifying the language. Our system demonstrated an overall accuracy of\n75.5% for token level language identification. The strict F-measure scores for\nthe identification of token level language labels for Bengali, English and\nHindi are 0.7486, 0.892 and 0.7972 respectively. The overall weighted F-measure\nof our system was 0.7498."}, "authors": ["Satanu Ghosh", "Souvick Ghosh", "Dipankar Das"], "author_detail": {"name": "Dipankar Das"}, "author": "Dipankar Das", "arxiv_comment": "4 pages in Technical Report, FIRE 2015", "links": [{"href": "http://arxiv.org/abs/1607.08883v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1607.08883v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.IR", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.IR", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1607.08883v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1607.08883v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1609.01409v1", "guidislink": true, "updated": "2016-09-06T06:29:32Z", "updated_parsed": [2016, 9, 6, 6, 29, 32, 1, 250, 0], "published": "2016-09-06T06:29:32Z", "published_parsed": [2016, 9, 6, 6, 29, 32, 1, 250, 0], "title": "Android Assistant EyeMate for Blind and Blind Tracker", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=100&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Android Assistant EyeMate for Blind and Blind Tracker"}, "summary": "At present many blind assistive systems have been implemented but there is no\nsuch kind of good system to navigate a blind person and also to track the\nmovement of a blind person and rescue him/her if he/she is lost. In this paper,\nwe have presented a blind assistive and tracking embedded system. In this\nsystem the blind person is navigated through a spectacle interfaced with an\nandroid application. The blind person is guided through Bengali/English voice\ncommands generated by the application according to the obstacle position. Using\nvoice command a blind person can establish voice call to a predefined number\nwithout touching the phone just by pressing the headset button. The blind\nassistive application gets the latitude and longitude using GPS and then sends\nthem to a server. The movement of the blind person is tracked through another\nandroid application that points out the current position in Google map. We took\ndistances from several surfaces like concrete and tiles floor in our experiment\nwhere the error rate is 5%.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=100&max_results=10&sortBy=relevance&sortOrder=descending", "value": "At present many blind assistive systems have been implemented but there is no\nsuch kind of good system to navigate a blind person and also to track the\nmovement of a blind person and rescue him/her if he/she is lost. In this paper,\nwe have presented a blind assistive and tracking embedded system. In this\nsystem the blind person is navigated through a spectacle interfaced with an\nandroid application. The blind person is guided through Bengali/English voice\ncommands generated by the application according to the obstacle position. Using\nvoice command a blind person can establish voice call to a predefined number\nwithout touching the phone just by pressing the headset button. The blind\nassistive application gets the latitude and longitude using GPS and then sends\nthem to a server. The movement of the blind person is tracked through another\nandroid application that points out the current position in Google map. We took\ndistances from several surfaces like concrete and tiles floor in our experiment\nwhere the error rate is 5%."}, "authors": ["Md. Siddiqur Rahman Tanveer", "M. M. A. Hashem", "Md. Kowsar Hossain"], "author_detail": {"name": "Md. Kowsar Hossain"}, "author": "Md. Kowsar Hossain", "links": [{"title": "doi", "href": "http://dx.doi.org/10.1109/ICCITechn.2015.7488080", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/1609.01409v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1609.01409v1", "rel": "related", "type": "application/pdf"}], "arxiv_comment": "arXiv admin note: text overlap with arXiv:1611.09480 by other author", "arxiv_primary_category": {"term": "cs.HC", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.HC", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.CY", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1609.01409v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1609.01409v1", "journal_reference": "2015 18th International Conference on Computer and Information\n  Technology (ICCIT)", "doi": "10.1109/ICCITechn.2015.7488080"}
{"id": "http://arxiv.org/abs/1610.07418v1", "guidislink": true, "updated": "2016-10-24T14:06:31Z", "updated_parsed": [2016, 10, 24, 14, 6, 31, 0, 298, 0], "published": "2016-10-24T14:06:31Z", "published_parsed": [2016, 10, 24, 14, 6, 31, 0, 298, 0], "title": "Statistical Machine Translation for Indian Languages: Mission Hindi", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=100&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Statistical Machine Translation for Indian Languages: Mission Hindi"}, "summary": "This paper discusses Centre for Development of Advanced Computing Mumbai's\n(CDACM) submission to the NLP Tools Contest on Statistical Machine Translation\nin Indian Languages (ILSMT) 2014 (collocated with ICON 2014). The objective of\nthe contest was to explore the effectiveness of Statistical Machine Translation\n(SMT) for Indian language to Indian language and English-Hindi machine\ntranslation. In this paper, we have proposed that suffix separation and word\nsplitting for SMT from agglutinative languages to Hindi significantly improves\nover the baseline (BL). We have also shown that the factored model with\nreordering outperforms the phrase-based SMT for English-Hindi (\\enhi). We\nreport our work on all five pairs of languages, namely Bengali-Hindi (\\bnhi),\nMarathi-Hindi (\\mrhi), Tamil-Hindi (\\tahi), Telugu-Hindi (\\tehi), and \\enhi for\nHealth, Tourism, and General domains.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=100&max_results=10&sortBy=relevance&sortOrder=descending", "value": "This paper discusses Centre for Development of Advanced Computing Mumbai's\n(CDACM) submission to the NLP Tools Contest on Statistical Machine Translation\nin Indian Languages (ILSMT) 2014 (collocated with ICON 2014). The objective of\nthe contest was to explore the effectiveness of Statistical Machine Translation\n(SMT) for Indian language to Indian language and English-Hindi machine\ntranslation. In this paper, we have proposed that suffix separation and word\nsplitting for SMT from agglutinative languages to Hindi significantly improves\nover the baseline (BL). We have also shown that the factored model with\nreordering outperforms the phrase-based SMT for English-Hindi (\\enhi). We\nreport our work on all five pairs of languages, namely Bengali-Hindi (\\bnhi),\nMarathi-Hindi (\\mrhi), Tamil-Hindi (\\tahi), Telugu-Hindi (\\tehi), and \\enhi for\nHealth, Tourism, and General domains."}, "authors": ["Raj Nath Patel", "Prakash B. Pimpale", "Sasikumar M"], "author_detail": {"name": "Sasikumar M"}, "author": "Sasikumar M", "arxiv_comment": "5 pages, Published at NLP Tools Contest: Statistical Machine\n  Translation in Indian Languages, ICON-2015", "links": [{"href": "http://arxiv.org/abs/1610.07418v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1610.07418v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1610.07418v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1610.07418v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1610.08000v1", "guidislink": true, "updated": "2016-10-25T18:20:08Z", "updated_parsed": [2016, 10, 25, 18, 20, 8, 1, 299, 0], "published": "2016-10-25T18:20:08Z", "published_parsed": [2016, 10, 25, 18, 20, 8, 1, 299, 0], "title": "Statistical Machine Translation for Indian Languages: Mission Hindi 2", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=100&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Statistical Machine Translation for Indian Languages: Mission Hindi 2"}, "summary": "This paper presents Centre for Development of Advanced Computing Mumbai's\n(CDACM) submission to NLP Tools Contest on Statistical Machine Translation in\nIndian Languages (ILSMT) 2015 (collocated with ICON 2015). The aim of the\ncontest was to collectively explore the effectiveness of Statistical Machine\nTranslation (SMT) while translating within Indian languages and between English\nand Indian languages. In this paper, we report our work on all five language\npairs, namely Bengali-Hindi (\\bnhi), Marathi-Hindi (\\mrhi), Tamil-Hindi\n(\\tahi), Telugu-Hindi (\\tehi), and English-Hindi (\\enhi) for Health, Tourism,\nand General domains. We have used suffix separation, compound splitting and\npreordering prior to SMT training and testing.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=100&max_results=10&sortBy=relevance&sortOrder=descending", "value": "This paper presents Centre for Development of Advanced Computing Mumbai's\n(CDACM) submission to NLP Tools Contest on Statistical Machine Translation in\nIndian Languages (ILSMT) 2015 (collocated with ICON 2015). The aim of the\ncontest was to collectively explore the effectiveness of Statistical Machine\nTranslation (SMT) while translating within Indian languages and between English\nand Indian languages. In this paper, we report our work on all five language\npairs, namely Bengali-Hindi (\\bnhi), Marathi-Hindi (\\mrhi), Tamil-Hindi\n(\\tahi), Telugu-Hindi (\\tehi), and English-Hindi (\\enhi) for Health, Tourism,\nand General domains. We have used suffix separation, compound splitting and\npreordering prior to SMT training and testing."}, "authors": ["Raj Nath Patel", "Prakash B. Pimpale"], "author_detail": {"name": "Prakash B. Pimpale"}, "author": "Prakash B. Pimpale", "arxiv_comment": "4 pages, Published in the Proceedings of NLP Tools Contest:\n  Statistical Machine Translation in Indian Languages", "links": [{"href": "http://arxiv.org/abs/1610.08000v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1610.08000v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1610.08000v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1610.08000v1", "journal_reference": "In the Proceedings of the 12th International Conference on Natural\n  Language Processing (ICON 2015)", "doi": null}
{"id": "http://arxiv.org/abs/1610.09799v1", "guidislink": true, "updated": "2016-10-31T06:13:31Z", "updated_parsed": [2016, 10, 31, 6, 13, 31, 0, 305, 0], "published": "2016-10-31T06:13:31Z", "published_parsed": [2016, 10, 31, 6, 13, 31, 0, 305, 0], "title": "Experiments with POS Tagging Code-mixed Indian Social Media Text", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=100&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Experiments with POS Tagging Code-mixed Indian Social Media Text"}, "summary": "This paper presents Centre for Development of Advanced Computing Mumbai's\n(CDACM) submission to the NLP Tools Contest on Part-Of-Speech (POS) Tagging For\nCode-mixed Indian Social Media Text (POSCMISMT) 2015 (collocated with ICON\n2015). We submitted results for Hindi (hi), Bengali (bn), and Telugu (te)\nlanguages mixed with English (en). In this paper, we have described our\napproaches to the POS tagging techniques, we exploited for this task. Machine\nlearning has been used to POS tag the mixed language text. For POS tagging,\ndistributed representations of words in vector space (word2vec) for feature\nextraction and Log-linear models have been tried. We report our work on all\nthree languages hi, bn, and te mixed with en.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=100&max_results=10&sortBy=relevance&sortOrder=descending", "value": "This paper presents Centre for Development of Advanced Computing Mumbai's\n(CDACM) submission to the NLP Tools Contest on Part-Of-Speech (POS) Tagging For\nCode-mixed Indian Social Media Text (POSCMISMT) 2015 (collocated with ICON\n2015). We submitted results for Hindi (hi), Bengali (bn), and Telugu (te)\nlanguages mixed with English (en). In this paper, we have described our\napproaches to the POS tagging techniques, we exploited for this task. Machine\nlearning has been used to POS tag the mixed language text. For POS tagging,\ndistributed representations of words in vector space (word2vec) for feature\nextraction and Log-linear models have been tried. We report our work on all\nthree languages hi, bn, and te mixed with en."}, "authors": ["Prakash B. Pimpale", "Raj Nath Patel"], "author_detail": {"name": "Raj Nath Patel"}, "author": "Raj Nath Patel", "arxiv_comment": "3 Pages, Published in the Proceedings of the Tool Contest on POS\n  Tagging for Code-mixed Indian Social Media (Facebook, Twitter, and Whatsapp)\n  Text", "links": [{"href": "http://arxiv.org/abs/1610.09799v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1610.09799v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1610.09799v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1610.09799v1", "journal_reference": "In the Proceedings of the 12th International Conference on Natural\n  Language Processing (ICON 2015)", "doi": null}
{"id": "http://arxiv.org/abs/1702.00167v2", "guidislink": true, "updated": "2017-02-02T08:12:13Z", "updated_parsed": [2017, 2, 2, 8, 12, 13, 3, 33, 0], "published": "2017-02-01T09:04:54Z", "published_parsed": [2017, 2, 1, 9, 4, 54, 2, 32, 0], "title": "SMPOST: Parts of Speech Tagger for Code-Mixed Indic Social Media Text", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=100&max_results=10&sortBy=relevance&sortOrder=descending", "value": "SMPOST: Parts of Speech Tagger for Code-Mixed Indic Social Media Text"}, "summary": "Use of social media has grown dramatically during the last few years. Users\nfollow informal languages in communicating through social media. The language\nof communication is often mixed in nature, where people transcribe their\nregional language with English and this technique is found to be extremely\npopular. Natural language processing (NLP) aims to infer the information from\nthese text where Part-of-Speech (PoS) tagging plays an important role in\ngetting the prosody of the written text. For the task of PoS tagging on\nCode-Mixed Indian Social Media Text, we develop a supervised system based on\nConditional Random Field classifier. In order to tackle the problem\neffectively, we have focused on extracting rich linguistic features. We\nparticipate in three different language pairs, ie. English-Hindi,\nEnglish-Bengali and English-Telugu on three different social media platforms,\nTwitter, Facebook & WhatsApp. The proposed system is able to successfully\nassign coarse as well as fine-grained PoS tag labels for a given a code-mixed\nsentence. Experiments show that our system is quite generic that shows\nencouraging performance levels on all the three language pairs in all the\ndomains.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=100&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Use of social media has grown dramatically during the last few years. Users\nfollow informal languages in communicating through social media. The language\nof communication is often mixed in nature, where people transcribe their\nregional language with English and this technique is found to be extremely\npopular. Natural language processing (NLP) aims to infer the information from\nthese text where Part-of-Speech (PoS) tagging plays an important role in\ngetting the prosody of the written text. For the task of PoS tagging on\nCode-Mixed Indian Social Media Text, we develop a supervised system based on\nConditional Random Field classifier. In order to tackle the problem\neffectively, we have focused on extracting rich linguistic features. We\nparticipate in three different language pairs, ie. English-Hindi,\nEnglish-Bengali and English-Telugu on three different social media platforms,\nTwitter, Facebook & WhatsApp. The proposed system is able to successfully\nassign coarse as well as fine-grained PoS tag labels for a given a code-mixed\nsentence. Experiments show that our system is quite generic that shows\nencouraging performance levels on all the three language pairs in all the\ndomains."}, "authors": ["Deepak Gupta", "Shubham Tripathi", "Asif Ekbal", "Pushpak Bhattacharyya"], "author_detail": {"name": "Pushpak Bhattacharyya"}, "author": "Pushpak Bhattacharyya", "arxiv_comment": "5 pages, ICON 2016", "links": [{"href": "http://arxiv.org/abs/1702.00167v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1702.00167v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1702.00167v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1702.00167v2", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1708.03361v3", "guidislink": true, "updated": "2019-01-20T15:23:29Z", "updated_parsed": [2019, 1, 20, 15, 23, 29, 6, 20, 0], "published": "2017-08-10T19:23:11Z", "published_parsed": [2017, 8, 10, 19, 23, 11, 3, 222, 0], "title": "An Empirical Study on Writer Identification & Verification from\n  Intra-variable Individual Handwriting", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=100&max_results=10&sortBy=relevance&sortOrder=descending", "value": "An Empirical Study on Writer Identification & Verification from\n  Intra-variable Individual Handwriting"}, "summary": "The handwriting of an individual may vary substantially with factors such as\nmood, time, space, writing speed, writing medium and tool, writing topic, etc.\nIt becomes challenging to perform automated writer verification/identification\non a particular set of handwritten patterns (e.g., speedy handwriting) of a\nperson, especially when the system is trained using a different set of writing\npatterns (e.g., normal speed) of that same person. However, it would be\ninteresting to experimentally analyze if there exists any implicit\ncharacteristic of individuality which is insensitive to high intra-variable\nhandwriting. In this paper, we study some handcrafted features and auto-derived\nfeatures extracted from intra-variable writing. Here, we work on writer\nidentification/verification from offline Bengali handwriting of high\nintra-variability. To this end, we use various models mainly based on\nhandcrafted features with SVM (Support Vector Machine) and features\nauto-derived by the convolutional network. For experimentation, we have\ngenerated two handwritten databases from two different sets of 100 writers and\nenlarged the dataset by a data-augmentation technique. We have obtained some\ninteresting results.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=100&max_results=10&sortBy=relevance&sortOrder=descending", "value": "The handwriting of an individual may vary substantially with factors such as\nmood, time, space, writing speed, writing medium and tool, writing topic, etc.\nIt becomes challenging to perform automated writer verification/identification\non a particular set of handwritten patterns (e.g., speedy handwriting) of a\nperson, especially when the system is trained using a different set of writing\npatterns (e.g., normal speed) of that same person. However, it would be\ninteresting to experimentally analyze if there exists any implicit\ncharacteristic of individuality which is insensitive to high intra-variable\nhandwriting. In this paper, we study some handcrafted features and auto-derived\nfeatures extracted from intra-variable writing. Here, we work on writer\nidentification/verification from offline Bengali handwriting of high\nintra-variability. To this end, we use various models mainly based on\nhandcrafted features with SVM (Support Vector Machine) and features\nauto-derived by the convolutional network. For experimentation, we have\ngenerated two handwritten databases from two different sets of 100 writers and\nenlarged the dataset by a data-augmentation technique. We have obtained some\ninteresting results."}, "authors": ["Chandranath Adak", "Bidyut B. Chaudhuri", "Michael Blumenstein"], "author_detail": {"name": "Michael Blumenstein"}, "author": "Michael Blumenstein", "links": [{"title": "doi", "href": "http://dx.doi.org/10.1109/ACCESS.2019.2899908", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/1708.03361v3", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1708.03361v3", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1708.03361v3", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1708.03361v3", "arxiv_comment": null, "journal_reference": null, "doi": "10.1109/ACCESS.2019.2899908"}
{"id": "http://arxiv.org/abs/1601.01195v1", "guidislink": true, "updated": "2016-01-06T14:40:38Z", "updated_parsed": [2016, 1, 6, 14, 40, 38, 2, 6, 0], "published": "2016-01-06T14:40:38Z", "published_parsed": [2016, 1, 6, 14, 40, 38, 2, 6, 0], "title": "Part-of-Speech Tagging for Code-mixed Indian Social Media Text at ICON\n  2015", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=110&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Part-of-Speech Tagging for Code-mixed Indian Social Media Text at ICON\n  2015"}, "summary": "This paper discusses the experiments carried out by us at Jadavpur University\nas part of the participation in ICON 2015 task: POS Tagging for Code-mixed\nIndian Social Media Text. The tool that we have developed for the task is based\non Trigram Hidden Markov Model that utilizes information from dictionary as\nwell as some other word level features to enhance the observation probabilities\nof the known tokens as well as unknown tokens. We submitted runs for\nBengali-English, Hindi-English and Tamil-English Language pairs. Our system has\nbeen trained and tested on the datasets released for ICON 2015 shared task: POS\nTagging For Code-mixed Indian Social Media Text. In constrained mode, our\nsystem obtains average overall accuracy (averaged over all three language\npairs) of 75.60% which is very close to other participating two systems (76.79%\nfor IIITH and 75.79% for AMRITA_CEN) ranked higher than our system. In\nunconstrained mode, our system obtains average overall accuracy of 70.65% which\nis also close to the system (72.85% for AMRITA_CEN) which obtains the highest\naverage overall accuracy.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=110&max_results=10&sortBy=relevance&sortOrder=descending", "value": "This paper discusses the experiments carried out by us at Jadavpur University\nas part of the participation in ICON 2015 task: POS Tagging for Code-mixed\nIndian Social Media Text. The tool that we have developed for the task is based\non Trigram Hidden Markov Model that utilizes information from dictionary as\nwell as some other word level features to enhance the observation probabilities\nof the known tokens as well as unknown tokens. We submitted runs for\nBengali-English, Hindi-English and Tamil-English Language pairs. Our system has\nbeen trained and tested on the datasets released for ICON 2015 shared task: POS\nTagging For Code-mixed Indian Social Media Text. In constrained mode, our\nsystem obtains average overall accuracy (averaged over all three language\npairs) of 75.60% which is very close to other participating two systems (76.79%\nfor IIITH and 75.79% for AMRITA_CEN) ranked higher than our system. In\nunconstrained mode, our system obtains average overall accuracy of 70.65% which\nis also close to the system (72.85% for AMRITA_CEN) which obtains the highest\naverage overall accuracy."}, "authors": ["Kamal Sarkar"], "author_detail": {"name": "Kamal Sarkar"}, "author": "Kamal Sarkar", "arxiv_comment": "NLP Tool Contest on \"POS Tagging For Code-mixed Indian Social Media\n  Text\" held in conjunction with International Conference on Natural Language\n  Processing(ICON 2015). arXiv admin note: text overlap with arXiv:1512.03950,\n  arXiv:1405.7397", "links": [{"href": "http://arxiv.org/abs/1601.01195v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1601.01195v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "68T50", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1601.01195v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1601.01195v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1611.04989v2", "guidislink": true, "updated": "2016-11-16T04:28:06Z", "updated_parsed": [2016, 11, 16, 4, 28, 6, 2, 321, 0], "published": "2016-11-15T19:02:35Z", "published_parsed": [2016, 11, 15, 19, 2, 35, 1, 320, 0], "title": "Recurrent Neural Network based Part-of-Speech Tagger for Code-Mixed\n  Social Media Text", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=110&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Recurrent Neural Network based Part-of-Speech Tagger for Code-Mixed\n  Social Media Text"}, "summary": "This paper describes Centre for Development of Advanced Computing's (CDACM)\nsubmission to the shared task-'Tool Contest on POS tagging for Code-Mixed\nIndian Social Media (Facebook, Twitter, and Whatsapp) Text', collocated with\nICON-2016. The shared task was to predict Part of Speech (POS) tag at word\nlevel for a given text. The code-mixed text is generated mostly on social media\nby multilingual users. The presence of the multilingual words,\ntransliterations, and spelling variations make such content linguistically\ncomplex. In this paper, we propose an approach to POS tag code-mixed social\nmedia text using Recurrent Neural Network Language Model (RNN-LM) architecture.\nWe submitted the results for Hindi-English (hi-en), Bengali-English (bn-en),\nand Telugu-English (te-en) code-mixed data.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=110&max_results=10&sortBy=relevance&sortOrder=descending", "value": "This paper describes Centre for Development of Advanced Computing's (CDACM)\nsubmission to the shared task-'Tool Contest on POS tagging for Code-Mixed\nIndian Social Media (Facebook, Twitter, and Whatsapp) Text', collocated with\nICON-2016. The shared task was to predict Part of Speech (POS) tag at word\nlevel for a given text. The code-mixed text is generated mostly on social media\nby multilingual users. The presence of the multilingual words,\ntransliterations, and spelling variations make such content linguistically\ncomplex. In this paper, we propose an approach to POS tag code-mixed social\nmedia text using Recurrent Neural Network Language Model (RNN-LM) architecture.\nWe submitted the results for Hindi-English (hi-en), Bengali-English (bn-en),\nand Telugu-English (te-en) code-mixed data."}, "authors": ["Raj Nath Patel", "Prakash B. Pimpale", "M Sasikumar"], "author_detail": {"name": "M Sasikumar"}, "author": "M Sasikumar", "arxiv_comment": "7 pages, Published at the Tool Contest on POS tagging for Indian\n  Social Media Text, ICON 2016", "links": [{"href": "http://arxiv.org/abs/1611.04989v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1611.04989v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1611.04989v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1611.04989v2", "journal_reference": "In Proceedings of the Tool Contest on POS tagging for Indian\n  Social Media Text, ICON 2016", "doi": null}
{"id": "http://arxiv.org/abs/1612.07956v1", "guidislink": true, "updated": "2016-12-23T12:58:58Z", "updated_parsed": [2016, 12, 23, 12, 58, 58, 4, 358, 0], "published": "2016-12-23T12:58:58Z", "published_parsed": [2016, 12, 23, 12, 58, 58, 4, 358, 0], "title": "A CRF Based POS Tagger for Code-mixed Indian Social Media Text", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=110&max_results=10&sortBy=relevance&sortOrder=descending", "value": "A CRF Based POS Tagger for Code-mixed Indian Social Media Text"}, "summary": "In this work, we describe a conditional random fields (CRF) based system for\nPart-Of- Speech (POS) tagging of code-mixed Indian social media text as part of\nour participation in the tool contest on POS tagging for codemixed Indian\nsocial media text, held in conjunction with the 2016 International Conference\non Natural Language Processing, IIT(BHU), India. We participated only in\nconstrained mode contest for all three language pairs, Bengali-English,\nHindi-English and Telegu-English. Our system achieves the overall average F1\nscore of 79.99, which is the highest overall average F1 score among all 16\nsystems participated in constrained mode contest.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=110&max_results=10&sortBy=relevance&sortOrder=descending", "value": "In this work, we describe a conditional random fields (CRF) based system for\nPart-Of- Speech (POS) tagging of code-mixed Indian social media text as part of\nour participation in the tool contest on POS tagging for codemixed Indian\nsocial media text, held in conjunction with the 2016 International Conference\non Natural Language Processing, IIT(BHU), India. We participated only in\nconstrained mode contest for all three language pairs, Bengali-English,\nHindi-English and Telegu-English. Our system achieves the overall average F1\nscore of 79.99, which is the highest overall average F1 score among all 16\nsystems participated in constrained mode contest."}, "authors": ["Kamal Sarkar"], "author_detail": {"name": "Kamal Sarkar"}, "author": "Kamal Sarkar", "arxiv_comment": "This work is awarded the first prize in the NLP tool contest on \"POS\n  Tagging for Code-Mixed Indian Social Media Text\", held in conjunction with\n  the 13th International Conference on Natural Language Processing 2016(ICON\n  2016), Indian Institute of Technology (BHU), India", "links": [{"href": "http://arxiv.org/abs/1612.07956v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1612.07956v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1612.07956v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1612.07956v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1701.00066v1", "guidislink": true, "updated": "2016-12-31T07:09:52Z", "updated_parsed": [2016, 12, 31, 7, 9, 52, 5, 366, 0], "published": "2016-12-31T07:09:52Z", "published_parsed": [2016, 12, 31, 7, 9, 52, 5, 366, 0], "title": "A POS Tagger for Code Mixed Indian Social Media Text - ICON-2016 NLP\n  Tools Contest Entry from Surukam", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=110&max_results=10&sortBy=relevance&sortOrder=descending", "value": "A POS Tagger for Code Mixed Indian Social Media Text - ICON-2016 NLP\n  Tools Contest Entry from Surukam"}, "summary": "Building Part-of-Speech (POS) taggers for code-mixed Indian languages is a\nparticularly challenging problem in computational linguistics due to a dearth\nof accurately annotated training corpora. ICON, as part of its NLP tools\ncontest has organized this challenge as a shared task for the second\nconsecutive year to improve the state-of-the-art. This paper describes the POS\ntagger built at Surukam to predict the coarse-grained and fine-grained POS tags\nfor three language pairs - Bengali-English, Telugu-English and Hindi-English,\nwith the text spanning three popular social media platforms - Facebook,\nWhatsApp and Twitter. We employed Conditional Random Fields as the sequence\ntagging algorithm and used a library called sklearn-crfsuite - a thin wrapper\naround CRFsuite for training our model. Among the features we used include -\ncharacter n-grams, language information and patterns for emoji, number,\npunctuation and web-address. Our submissions in the constrained\nenvironment,i.e., without making any use of monolingual POS taggers or the\nlike, obtained an overall average F1-score of 76.45%, which is comparable to\nthe 2015 winning score of 76.79%.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=110&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Building Part-of-Speech (POS) taggers for code-mixed Indian languages is a\nparticularly challenging problem in computational linguistics due to a dearth\nof accurately annotated training corpora. ICON, as part of its NLP tools\ncontest has organized this challenge as a shared task for the second\nconsecutive year to improve the state-of-the-art. This paper describes the POS\ntagger built at Surukam to predict the coarse-grained and fine-grained POS tags\nfor three language pairs - Bengali-English, Telugu-English and Hindi-English,\nwith the text spanning three popular social media platforms - Facebook,\nWhatsApp and Twitter. We employed Conditional Random Fields as the sequence\ntagging algorithm and used a library called sklearn-crfsuite - a thin wrapper\naround CRFsuite for training our model. Among the features we used include -\ncharacter n-grams, language information and patterns for emoji, number,\npunctuation and web-address. Our submissions in the constrained\nenvironment,i.e., without making any use of monolingual POS taggers or the\nlike, obtained an overall average F1-score of 76.45%, which is comparable to\nthe 2015 winning score of 76.79%."}, "authors": ["Sree Harsha Ramesh", "Raveena R Kumar"], "author_detail": {"name": "Raveena R Kumar"}, "author": "Raveena R Kumar", "arxiv_comment": "4 Pages, 13th International Conference on Natural Language\n  Processing, Varanasi, India", "links": [{"href": "http://arxiv.org/abs/1701.00066v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1701.00066v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1701.00066v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1701.00066v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1701.04290v1", "guidislink": true, "updated": "2017-01-16T13:55:01Z", "updated_parsed": [2017, 1, 16, 13, 55, 1, 0, 16, 0], "published": "2017-01-16T13:55:01Z", "published_parsed": [2017, 1, 16, 13, 55, 1, 0, 16, 0], "title": "Machine Translation Approaches and Survey for Indian Languages", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=110&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Machine Translation Approaches and Survey for Indian Languages"}, "summary": "In this study, we present an analysis regarding the performance of the\nstate-of-art Phrase-based Statistical Machine Translation (SMT) on multiple\nIndian languages. We report baseline systems on several language pairs. The\nmotivation of this study is to promote the development of SMT and linguistic\nresources for these language pairs, as the current state-of-the-art is quite\nbleak due to sparse data resources. The success of an SMT system is contingent\non the availability of a large parallel corpus. Such data is necessary to\nreliably estimate translation probabilities. We report the performance of\nbaseline systems translating from Indian languages (Bengali, Guajarati, Hindi,\nMalayalam, Punjabi, Tamil, Telugu and Urdu) into English with average 10%\naccurate results for all the language pairs.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=110&max_results=10&sortBy=relevance&sortOrder=descending", "value": "In this study, we present an analysis regarding the performance of the\nstate-of-art Phrase-based Statistical Machine Translation (SMT) on multiple\nIndian languages. We report baseline systems on several language pairs. The\nmotivation of this study is to promote the development of SMT and linguistic\nresources for these language pairs, as the current state-of-the-art is quite\nbleak due to sparse data resources. The success of an SMT system is contingent\non the availability of a large parallel corpus. Such data is necessary to\nreliably estimate translation probabilities. We report the performance of\nbaseline systems translating from Indian languages (Bengali, Guajarati, Hindi,\nMalayalam, Punjabi, Tamil, Telugu and Urdu) into English with average 10%\naccurate results for all the language pairs."}, "authors": ["Nadeem Jadoon Khan", "Waqas Anwar", "Nadir Durrani"], "author_detail": {"name": "Nadir Durrani"}, "author": "Nadir Durrani", "links": [{"href": "http://arxiv.org/abs/1701.04290v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1701.04290v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1701.04290v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1701.04290v1", "arxiv_comment": null, "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1712.01434v1", "guidislink": true, "updated": "2017-12-05T01:12:25Z", "updated_parsed": [2017, 12, 5, 1, 12, 25, 1, 339, 0], "published": "2017-12-05T01:12:25Z", "published_parsed": [2017, 12, 5, 1, 12, 25, 1, 339, 0], "title": "Zone-based Keyword Spotting in Bangla and Devanagari Documents", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=110&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Zone-based Keyword Spotting in Bangla and Devanagari Documents"}, "summary": "In this paper we present a word spotting system in text lines for offline\nIndic scripts such as Bangla (Bengali) and Devanagari. Recently, it was shown\nthat zone-wise recognition method improves the word recognition performance\nthan conventional full word recognition system in Indic scripts. Inspired with\nthis idea we consider the zone segmentation approach and use middle zone\ninformation to improve the traditional word spotting performance. To avoid the\nproblem of zone segmentation using heuristic approach, we propose here an HMM\nbased approach to segment the upper and lower zone components from the text\nline images. The candidate keywords are searched from a line without segmenting\ncharacters or words. Also, we propose a novel feature combining foreground and\nbackground information of text line images for keyword-spotting by character\nfiller models. A significant improvement in performance is noted by using both\nforeground and background information than their individual one. Pyramid\nHistogram of Oriented Gradient (PHOG) feature has been used in our word\nspotting framework. From the experiment, it has been noted that the proposed\nzone-segmentation based system outperforms traditional approaches of word\nspotting.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=110&max_results=10&sortBy=relevance&sortOrder=descending", "value": "In this paper we present a word spotting system in text lines for offline\nIndic scripts such as Bangla (Bengali) and Devanagari. Recently, it was shown\nthat zone-wise recognition method improves the word recognition performance\nthan conventional full word recognition system in Indic scripts. Inspired with\nthis idea we consider the zone segmentation approach and use middle zone\ninformation to improve the traditional word spotting performance. To avoid the\nproblem of zone segmentation using heuristic approach, we propose here an HMM\nbased approach to segment the upper and lower zone components from the text\nline images. The candidate keywords are searched from a line without segmenting\ncharacters or words. Also, we propose a novel feature combining foreground and\nbackground information of text line images for keyword-spotting by character\nfiller models. A significant improvement in performance is noted by using both\nforeground and background information than their individual one. Pyramid\nHistogram of Oriented Gradient (PHOG) feature has been used in our word\nspotting framework. From the experiment, it has been noted that the proposed\nzone-segmentation based system outperforms traditional approaches of word\nspotting."}, "authors": ["Ayan Kumar Bhunia", "Partha Pratim Roy", "Umapada Pal"], "author_detail": {"name": "Umapada Pal"}, "author": "Umapada Pal", "arxiv_comment": "Preprint Submitted", "links": [{"href": "http://arxiv.org/abs/1712.01434v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1712.01434v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1712.01434v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1712.01434v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1801.02581v2", "guidislink": true, "updated": "2018-03-15T19:31:44Z", "updated_parsed": [2018, 3, 15, 19, 31, 44, 3, 74, 0], "published": "2018-01-08T17:43:12Z", "published_parsed": [2018, 1, 8, 17, 43, 12, 0, 8, 0], "title": "Analyzing Roles of Classifiers and Code-Mixed factors for Sentiment\n  Identification", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=110&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Analyzing Roles of Classifiers and Code-Mixed factors for Sentiment\n  Identification"}, "summary": "Multilingual speakers often switch between languages to express themselves on\nsocial communication platforms. Sometimes, the original script of the language\nis preserved, while using a common script for all the languages is quite\npopular as well due to convenience. On such occasions, multiple languages are\nbeing mixed with different rules of grammar, using the same script which makes\nit a challenging task for natural language processing even in case of accurate\nsentiment identification. In this paper, we report results of various\nexperiments carried out on movie reviews dataset having this code-mixing\nproperty of two languages, English and Bengali, both typed in Roman script. We\nhave tested various machine learning algorithms trained only on English\nfeatures on our code-mixed data and have achieved the maximum accuracy of\n59.00% using Naive Bayes (NB) model. We have also tested various models trained\non code-mixed data, as well as English features and the highest accuracy of\n72.50% was obtained by a Support Vector Machine (SVM) model. Finally, we have\nanalyzed the misclassified snippets and have discussed the challenges needed to\nbe resolved for better accuracy.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=110&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Multilingual speakers often switch between languages to express themselves on\nsocial communication platforms. Sometimes, the original script of the language\nis preserved, while using a common script for all the languages is quite\npopular as well due to convenience. On such occasions, multiple languages are\nbeing mixed with different rules of grammar, using the same script which makes\nit a challenging task for natural language processing even in case of accurate\nsentiment identification. In this paper, we report results of various\nexperiments carried out on movie reviews dataset having this code-mixing\nproperty of two languages, English and Bengali, both typed in Roman script. We\nhave tested various machine learning algorithms trained only on English\nfeatures on our code-mixed data and have achieved the maximum accuracy of\n59.00% using Naive Bayes (NB) model. We have also tested various models trained\non code-mixed data, as well as English features and the highest accuracy of\n72.50% was obtained by a Support Vector Machine (SVM) model. Finally, we have\nanalyzed the misclassified snippets and have discussed the challenges needed to\nbe resolved for better accuracy."}, "authors": ["Soumil Mandal", "Dipankar Das"], "author_detail": {"name": "Dipankar Das"}, "author": "Dipankar Das", "arxiv_comment": "18th International Conference on Computational Linguistics and\n  Intelligent Text Processing, CICLing 2017 (RCS)", "links": [{"href": "http://arxiv.org/abs/1801.02581v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1801.02581v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1801.02581v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1801.02581v2", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1803.06745v1", "guidislink": true, "updated": "2018-03-18T21:32:07Z", "updated_parsed": [2018, 3, 18, 21, 32, 7, 6, 77, 0], "published": "2018-03-18T21:32:07Z", "published_parsed": [2018, 3, 18, 21, 32, 7, 6, 77, 0], "title": "Sentiment Analysis of Code-Mixed Indian Languages: An Overview of\n  SAIL_Code-Mixed Shared Task @ICON-2017", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=110&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Sentiment Analysis of Code-Mixed Indian Languages: An Overview of\n  SAIL_Code-Mixed Shared Task @ICON-2017"}, "summary": "Sentiment analysis is essential in many real-world applications such as\nstance detection, review analysis, recommendation system, and so on. Sentiment\nanalysis becomes more difficult when the data is noisy and collected from\nsocial media. India is a multilingual country; people use more than one\nlanguages to communicate within themselves. The switching in between the\nlanguages is called code-switching or code-mixing, depending upon the type of\nmixing. This paper presents overview of the shared task on sentiment analysis\nof code-mixed data pairs of Hindi-English and Bengali-English collected from\nthe different social media platform. The paper describes the task, dataset,\nevaluation, baseline and participant's systems.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=110&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Sentiment analysis is essential in many real-world applications such as\nstance detection, review analysis, recommendation system, and so on. Sentiment\nanalysis becomes more difficult when the data is noisy and collected from\nsocial media. India is a multilingual country; people use more than one\nlanguages to communicate within themselves. The switching in between the\nlanguages is called code-switching or code-mixing, depending upon the type of\nmixing. This paper presents overview of the shared task on sentiment analysis\nof code-mixed data pairs of Hindi-English and Bengali-English collected from\nthe different social media platform. The paper describes the task, dataset,\nevaluation, baseline and participant's systems."}, "authors": ["Braja Gopal Patra", "Dipankar Das", "Amitava Das"], "author_detail": {"name": "Amitava Das"}, "author": "Amitava Das", "links": [{"href": "http://arxiv.org/abs/1803.06745v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1803.06745v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1803.06745v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1803.06745v1", "arxiv_comment": null, "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1812.00798v1", "guidislink": true, "updated": "2018-12-03T14:53:41Z", "updated_parsed": [2018, 12, 3, 14, 53, 41, 0, 337, 0], "published": "2018-12-03T14:53:41Z", "published_parsed": [2018, 12, 3, 14, 53, 41, 0, 337, 0], "title": "The RGNLP Machine Translation Systems for WAT 2018", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=120&max_results=10&sortBy=relevance&sortOrder=descending", "value": "The RGNLP Machine Translation Systems for WAT 2018"}, "summary": "This paper presents the system description of Machine Translation (MT)\nsystem(s) for Indic Languages Multilingual Task for the 2018 edition of the WAT\nShared Task. In our experiments, we (the RGNLP team) explore both statistical\nand neural methods across all language pairs. (We further present an extensive\ncomparison of language-related problems for both the approaches in the context\nof low-resourced settings.) Our PBSMT models were highest score on all\nautomatic evaluation metrics in the English into Telugu, Hindi, Bengali, Tamil\nportion of the shared task.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=120&max_results=10&sortBy=relevance&sortOrder=descending", "value": "This paper presents the system description of Machine Translation (MT)\nsystem(s) for Indic Languages Multilingual Task for the 2018 edition of the WAT\nShared Task. In our experiments, we (the RGNLP team) explore both statistical\nand neural methods across all language pairs. (We further present an extensive\ncomparison of language-related problems for both the approaches in the context\nof low-resourced settings.) Our PBSMT models were highest score on all\nautomatic evaluation metrics in the English into Telugu, Hindi, Bengali, Tamil\nportion of the shared task."}, "authors": ["Atul Kr. Ojha", "Koel Dutta Chowdhury", "Chao-Hong Liu", "Karan Saxena"], "author_detail": {"name": "Karan Saxena"}, "author": "Karan Saxena", "arxiv_comment": "Short-Paper at WAT Shared Task 2018, In Proceedings of the 5th\n  Workshop on Asian Translation (WAT2018), Hong Kong, China, December", "links": [{"href": "http://arxiv.org/abs/1812.00798v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1812.00798v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1812.00798v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1812.00798v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1906.05725v1", "guidislink": true, "updated": "2019-06-13T14:41:00Z", "updated_parsed": [2019, 6, 13, 14, 41, 0, 3, 164, 0], "published": "2019-06-13T14:41:00Z", "published_parsed": [2019, 6, 13, 14, 41, 0, 3, 164, 0], "title": "Improved Sentiment Detection via Label Transfer from Monolingual to\n  Synthetic Code-Switched Text", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=120&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Improved Sentiment Detection via Label Transfer from Monolingual to\n  Synthetic Code-Switched Text"}, "summary": "Multilingual writers and speakers often alternate between two languages in a\nsingle discourse, a practice called \"code-switching\". Existing sentiment\ndetection methods are usually trained on sentiment-labeled monolingual text.\nManually labeled code-switched text, especially involving minority languages,\nis extremely rare. Consequently, the best monolingual methods perform\nrelatively poorly on code-switched text. We present an effective technique for\nsynthesizing labeled code-switched text from labeled monolingual text, which is\nmore readily available. The idea is to replace carefully selected subtrees of\nconstituency parses of sentences in the resource-rich language with suitable\ntoken spans selected from automatic translations to the resource-poor language.\nBy augmenting scarce human-labeled code-switched text with plentiful synthetic\ncode-switched text, we achieve significant improvements in sentiment labeling\naccuracy (1.5%, 5.11%, 7.20%) for three different language pairs\n(English-Hindi, English-Spanish and English-Bengali). We also get significant\ngains for hate speech detection: 4% improvement using only synthetic text and\n6% if augmented with real text.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=120&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Multilingual writers and speakers often alternate between two languages in a\nsingle discourse, a practice called \"code-switching\". Existing sentiment\ndetection methods are usually trained on sentiment-labeled monolingual text.\nManually labeled code-switched text, especially involving minority languages,\nis extremely rare. Consequently, the best monolingual methods perform\nrelatively poorly on code-switched text. We present an effective technique for\nsynthesizing labeled code-switched text from labeled monolingual text, which is\nmore readily available. The idea is to replace carefully selected subtrees of\nconstituency parses of sentences in the resource-rich language with suitable\ntoken spans selected from automatic translations to the resource-poor language.\nBy augmenting scarce human-labeled code-switched text with plentiful synthetic\ncode-switched text, we achieve significant improvements in sentiment labeling\naccuracy (1.5%, 5.11%, 7.20%) for three different language pairs\n(English-Hindi, English-Spanish and English-Bengali). We also get significant\ngains for hate speech detection: 4% improvement using only synthetic text and\n6% if augmented with real text."}, "authors": ["Bidisha Samanta", "Niloy Ganguly", "Soumen Chakrabarti"], "author_detail": {"name": "Soumen Chakrabarti"}, "author": "Soumen Chakrabarti", "links": [{"href": "http://arxiv.org/abs/1906.05725v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1906.05725v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1906.05725v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1906.05725v1", "arxiv_comment": null, "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1909.03598v1", "guidislink": true, "updated": "2019-09-09T02:45:38Z", "updated_parsed": [2019, 9, 9, 2, 45, 38, 0, 252, 0], "published": "2019-09-09T02:45:38Z", "published_parsed": [2019, 9, 9, 2, 45, 38, 0, 252, 0], "title": "What Matters for Neural Cross-Lingual Named Entity Recognition: An\n  Empirical Analysis", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=120&max_results=10&sortBy=relevance&sortOrder=descending", "value": "What Matters for Neural Cross-Lingual Named Entity Recognition: An\n  Empirical Analysis"}, "summary": "Building named entity recognition (NER) models for languages that do not have\nmuch training data is a challenging task. While recent work has shown promising\nresults on cross-lingual transfer from high-resource languages to low-resource\nlanguages, it is unclear what knowledge is transferred. In this paper, we first\npropose a simple and efficient neural architecture for cross-lingual NER.\nExperiments show that our model achieves competitive performance with the\nstate-of-the-art. We further analyze how transfer learning works for\ncross-lingual NER on two transferable factors: sequential order and\nmultilingual embeddings, and investigate how model performance varies across\nentity lengths. Finally, we conduct a case-study on a non-Latin language,\nBengali, which suggests that leveraging knowledge from Wikipedia will be a\npromising direction to further improve the model performances. Our results can\nshed light on future research for improving cross-lingual NER.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=120&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Building named entity recognition (NER) models for languages that do not have\nmuch training data is a challenging task. While recent work has shown promising\nresults on cross-lingual transfer from high-resource languages to low-resource\nlanguages, it is unclear what knowledge is transferred. In this paper, we first\npropose a simple and efficient neural architecture for cross-lingual NER.\nExperiments show that our model achieves competitive performance with the\nstate-of-the-art. We further analyze how transfer learning works for\ncross-lingual NER on two transferable factors: sequential order and\nmultilingual embeddings, and investigate how model performance varies across\nentity lengths. Finally, we conduct a case-study on a non-Latin language,\nBengali, which suggests that leveraging knowledge from Wikipedia will be a\npromising direction to further improve the model performances. Our results can\nshed light on future research for improving cross-lingual NER."}, "authors": ["Xiaolei Huang", "Jonathan May", "Nanyun Peng"], "author_detail": {"name": "Nanyun Peng"}, "author": "Nanyun Peng", "arxiv_comment": "7 pages", "links": [{"href": "http://arxiv.org/abs/1909.03598v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1909.03598v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1909.03598v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1909.03598v1", "journal_reference": "published at EMNLP 2019", "doi": null}
{"id": "http://arxiv.org/abs/1909.09270v1", "guidislink": true, "updated": "2019-09-20T00:39:07Z", "updated_parsed": [2019, 9, 20, 0, 39, 7, 4, 263, 0], "published": "2019-09-20T00:39:07Z", "published_parsed": [2019, 9, 20, 0, 39, 7, 4, 263, 0], "title": "Named Entity Recognition with Partially Annotated Training Data", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=120&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Named Entity Recognition with Partially Annotated Training Data"}, "summary": "Supervised machine learning assumes the availability of fully-labeled data,\nbut in many cases, such as low-resource languages, the only data available is\npartially annotated. We study the problem of Named Entity Recognition (NER)\nwith partially annotated training data in which a fraction of the named\nentities are labeled, and all other tokens, entities or otherwise, are labeled\nas non-entity by default. In order to train on this noisy dataset, we need to\ndistinguish between the true and false negatives. To this end, we introduce a\nconstraint-driven iterative algorithm that learns to detect false negatives in\nthe noisy set and downweigh them, resulting in a weighted training set. With\nthis set, we train a weighted NER model. We evaluate our algorithm with\nweighted variants of neural and non-neural NER models on data in 8 languages\nfrom several language and script families, showing strong ability to learn from\npartial data. Finally, to show real-world efficacy, we evaluate on a Bengali\nNER corpus annotated by non-speakers, outperforming the prior state-of-the-art\nby over 5 points F1.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=120&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Supervised machine learning assumes the availability of fully-labeled data,\nbut in many cases, such as low-resource languages, the only data available is\npartially annotated. We study the problem of Named Entity Recognition (NER)\nwith partially annotated training data in which a fraction of the named\nentities are labeled, and all other tokens, entities or otherwise, are labeled\nas non-entity by default. In order to train on this noisy dataset, we need to\ndistinguish between the true and false negatives. To this end, we introduce a\nconstraint-driven iterative algorithm that learns to detect false negatives in\nthe noisy set and downweigh them, resulting in a weighted training set. With\nthis set, we train a weighted NER model. We evaluate our algorithm with\nweighted variants of neural and non-neural NER models on data in 8 languages\nfrom several language and script families, showing strong ability to learn from\npartial data. Finally, to show real-world efficacy, we evaluate on a Bengali\nNER corpus annotated by non-speakers, outperforming the prior state-of-the-art\nby over 5 points F1."}, "authors": ["Stephen Mayhew", "Snigdha Chaturvedi", "Chen-Tse Tsai", "Dan Roth"], "author_detail": {"name": "Dan Roth"}, "author": "Dan Roth", "arxiv_comment": "Accepted to CoNLL 2019", "links": [{"href": "http://arxiv.org/abs/1909.09270v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1909.09270v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1909.09270v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1909.09270v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/2007.07691v1", "guidislink": true, "updated": "2020-07-15T14:00:18Z", "updated_parsed": [2020, 7, 15, 14, 0, 18, 2, 197, 0], "published": "2020-07-15T14:00:18Z", "published_parsed": [2020, 7, 15, 14, 0, 18, 2, 197, 0], "title": "A Multilingual Parallel Corpora Collection Effort for Indian Languages", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=120&max_results=10&sortBy=relevance&sortOrder=descending", "value": "A Multilingual Parallel Corpora Collection Effort for Indian Languages"}, "summary": "We present sentence aligned parallel corpora across 10 Indian Languages -\nHindi, Telugu, Tamil, Malayalam, Gujarati, Urdu, Bengali, Oriya, Marathi,\nPunjabi, and English - many of which are categorized as low resource. The\ncorpora are compiled from online sources which have content shared across\nlanguages. The corpora presented significantly extends present resources that\nare either not large enough or are restricted to a specific domain (such as\nhealth). We also provide a separate test corpus compiled from an independent\nonline source that can be independently used for validating the performance in\n10 Indian languages. Alongside, we report on the methods of constructing such\ncorpora using tools enabled by recent advances in machine translation and\ncross-lingual retrieval using deep neural network based methods.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=120&max_results=10&sortBy=relevance&sortOrder=descending", "value": "We present sentence aligned parallel corpora across 10 Indian Languages -\nHindi, Telugu, Tamil, Malayalam, Gujarati, Urdu, Bengali, Oriya, Marathi,\nPunjabi, and English - many of which are categorized as low resource. The\ncorpora are compiled from online sources which have content shared across\nlanguages. The corpora presented significantly extends present resources that\nare either not large enough or are restricted to a specific domain (such as\nhealth). We also provide a separate test corpus compiled from an independent\nonline source that can be independently used for validating the performance in\n10 Indian languages. Alongside, we report on the methods of constructing such\ncorpora using tools enabled by recent advances in machine translation and\ncross-lingual retrieval using deep neural network based methods."}, "authors": ["Shashank Siripragada", "Jerin Philip", "Vinay P. Namboodiri", "C V Jawahar"], "author_detail": {"name": "C V Jawahar"}, "author": "C V Jawahar", "arxiv_comment": "9 pages. Accepted in LREC 2020", "links": [{"href": "http://arxiv.org/abs/2007.07691v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2007.07691v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2007.07691v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2007.07691v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/2010.08066v1", "guidislink": true, "updated": "2020-10-15T23:24:15Z", "updated_parsed": [2020, 10, 15, 23, 24, 15, 3, 289, 0], "published": "2020-10-15T23:24:15Z", "published_parsed": [2020, 10, 15, 23, 24, 15, 3, 289, 0], "title": "TextMage: The Automated Bangla Caption Generator Based On Deep Learning", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=130&max_results=10&sortBy=relevance&sortOrder=descending", "value": "TextMage: The Automated Bangla Caption Generator Based On Deep Learning"}, "summary": "Neural Networks and Deep Learning have seen an upsurge of research in the\npast decade due to the improved results. Generates text from the given image is\na crucial task that requires the combination of both sectors which are computer\nvision and natural language processing in order to understand an image and\nrepresent it using a natural language. However existing works have all been\ndone on a particular lingual domain and on the same set of data. This leads to\nthe systems being developed to perform poorly on images that belong to specific\nlocales' geographical context. TextMage is a system that is capable of\nunderstanding visual scenes that belong to the Bangladeshi geographical context\nand use its knowledge to represent what it understands in Bengali. Hence, we\nhave trained a model on our previously developed and published dataset named\nBanglaLekhaImageCaptions. This dataset contains 9,154 images along with two\nannotations for each image. In order to access performance, the proposed model\nhas been implemented and evaluated.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=130&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Neural Networks and Deep Learning have seen an upsurge of research in the\npast decade due to the improved results. Generates text from the given image is\na crucial task that requires the combination of both sectors which are computer\nvision and natural language processing in order to understand an image and\nrepresent it using a natural language. However existing works have all been\ndone on a particular lingual domain and on the same set of data. This leads to\nthe systems being developed to perform poorly on images that belong to specific\nlocales' geographical context. TextMage is a system that is capable of\nunderstanding visual scenes that belong to the Bangladeshi geographical context\nand use its knowledge to represent what it understands in Bengali. Hence, we\nhave trained a model on our previously developed and published dataset named\nBanglaLekhaImageCaptions. This dataset contains 9,154 images along with two\nannotations for each image. In order to access performance, the proposed model\nhas been implemented and evaluated."}, "authors": ["Abrar Hasin Kamal", "Md. Asifuzzaman Jishan", "Nafees Mansoor"], "author_detail": {"name": "Nafees Mansoor"}, "author": "Nafees Mansoor", "arxiv_comment": "5 pages", "links": [{"href": "http://arxiv.org/abs/2010.08066v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2010.08066v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2010.08066v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2010.08066v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/2101.06949v1", "guidislink": true, "updated": "2021-01-18T09:23:35Z", "updated_parsed": [2021, 1, 18, 9, 23, 35, 0, 18, 0], "published": "2021-01-18T09:23:35Z", "published_parsed": [2021, 1, 18, 9, 23, 35, 0, 18, 0], "title": "HinFlair: pre-trained contextual string embeddings for pos tagging and\n  text classification in the Hindi language", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=130&max_results=10&sortBy=relevance&sortOrder=descending", "value": "HinFlair: pre-trained contextual string embeddings for pos tagging and\n  text classification in the Hindi language"}, "summary": "Recent advancements in language models based on recurrent neural networks and\ntransformers architecture have achieved state-of-the-art results on a wide\nrange of natural language processing tasks such as pos tagging, named entity\nrecognition, and text classification. However, most of these language models\nare pre-trained in high resource languages like English, German, Spanish.\nMulti-lingual language models include Indian languages like Hindi, Telugu,\nBengali in their training corpus, but they often fail to represent the\nlinguistic features of these languages as they are not the primary language of\nthe study. We introduce HinFlair, which is a language representation model\n(contextual string embeddings) pre-trained on a large monolingual Hindi corpus.\nExperiments were conducted on 6 text classification datasets and a Hindi\ndependency treebank to analyze the performance of these contextualized string\nembeddings for the Hindi language. Results show that HinFlair outperforms\nprevious state-of-the-art publicly available pre-trained embeddings for\ndownstream tasks like text classification and pos tagging. Also, HinFlair when\ncombined with FastText embeddings outperforms many transformers-based language\nmodels trained particularly for the Hindi language.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=130&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Recent advancements in language models based on recurrent neural networks and\ntransformers architecture have achieved state-of-the-art results on a wide\nrange of natural language processing tasks such as pos tagging, named entity\nrecognition, and text classification. However, most of these language models\nare pre-trained in high resource languages like English, German, Spanish.\nMulti-lingual language models include Indian languages like Hindi, Telugu,\nBengali in their training corpus, but they often fail to represent the\nlinguistic features of these languages as they are not the primary language of\nthe study. We introduce HinFlair, which is a language representation model\n(contextual string embeddings) pre-trained on a large monolingual Hindi corpus.\nExperiments were conducted on 6 text classification datasets and a Hindi\ndependency treebank to analyze the performance of these contextualized string\nembeddings for the Hindi language. Results show that HinFlair outperforms\nprevious state-of-the-art publicly available pre-trained embeddings for\ndownstream tasks like text classification and pos tagging. Also, HinFlair when\ncombined with FastText embeddings outperforms many transformers-based language\nmodels trained particularly for the Hindi language."}, "authors": ["Harsh Patel"], "author_detail": {"name": "Harsh Patel"}, "author": "Harsh Patel", "links": [{"href": "http://arxiv.org/abs/2101.06949v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2101.06949v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2101.06949v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2101.06949v1", "arxiv_comment": null, "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/cs/0308018v1", "guidislink": true, "updated": "2003-08-07T09:24:46Z", "updated_parsed": [2003, 8, 7, 9, 24, 46, 3, 219, 0], "published": "2003-08-07T09:24:46Z", "published_parsed": [2003, 8, 7, 9, 24, 46, 3, 219, 0], "title": "Anusaaraka: Overcoming the Language Barrier in India", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=130&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Anusaaraka: Overcoming the Language Barrier in India"}, "summary": "The anusaaraka system makes text in one Indian language accessible in another\nIndian language. In the anusaaraka approach, the load is so divided between man\nand computer that the language load is taken by the machine, and the\ninterpretation of the text is left to the man. The machine presents an image of\nthe source text in a language close to the target language.In the image, some\nconstructions of the source language (which do not have equivalents) spill over\nto the output. Some special notation is also devised. The user after some\ntraining learns to read and understand the output. Because the Indian languages\nare close, the learning time of the output language is short, and is expected\nto be around 2 weeks.\n  The output can also be post-edited by a trained user to make it grammatically\ncorrect in the target language. Style can also be changed, if necessary. Thus,\nin this scenario, it can function as a human assisted translation system.\n  Currently, anusaarakas are being built from Telugu, Kannada, Marathi, Bengali\nand Punjabi to Hindi. They can be built for all Indian languages in the near\nfuture. Everybody must pitch in to build such systems connecting all Indian\nlanguages, using the free software model.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=130&max_results=10&sortBy=relevance&sortOrder=descending", "value": "The anusaaraka system makes text in one Indian language accessible in another\nIndian language. In the anusaaraka approach, the load is so divided between man\nand computer that the language load is taken by the machine, and the\ninterpretation of the text is left to the man. The machine presents an image of\nthe source text in a language close to the target language.In the image, some\nconstructions of the source language (which do not have equivalents) spill over\nto the output. Some special notation is also devised. The user after some\ntraining learns to read and understand the output. Because the Indian languages\nare close, the learning time of the output language is short, and is expected\nto be around 2 weeks.\n  The output can also be post-edited by a trained user to make it grammatically\ncorrect in the target language. Style can also be changed, if necessary. Thus,\nin this scenario, it can function as a human assisted translation system.\n  Currently, anusaarakas are being built from Telugu, Kannada, Marathi, Bengali\nand Punjabi to Hindi. They can be built for all Indian languages in the near\nfuture. Everybody must pitch in to build such systems connecting all Indian\nlanguages, using the free software model."}, "authors": ["Akshar Bharati", "Vineet Chaitanya", "Amba P. Kulkarni", "Rajeev Sangal", "G Umamaheshwara Rao"], "author_detail": {"name": "G Umamaheshwara Rao"}, "author": "G Umamaheshwara Rao", "arxiv_comment": "Published in \"Anuvad: Approaches to Translation\", Rukmini Bhaya Nair,\n  (editor), Sage, New Delhi, 2001", "links": [{"href": "http://arxiv.org/abs/cs/0308018v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/cs/0308018v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "I,2,7", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/cs/0308018v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/cs/0308018v1", "journal_reference": "Published in \"Anuvad: Approaches to Translation\", Rukmini Bhaya\n  Nair, (editor), Sage, New Delhi, 2001", "doi": null}
{"id": "http://arxiv.org/abs/1003.1891v1", "guidislink": true, "updated": "2010-03-09T14:56:00Z", "updated_parsed": [2010, 3, 9, 14, 56, 0, 1, 68, 0], "published": "2010-03-09T14:56:00Z", "published_parsed": [2010, 3, 9, 14, 56, 0, 1, 68, 0], "title": "Handwritten Arabic Numeral Recognition using a Multi Layer Perceptron", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=130&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Handwritten Arabic Numeral Recognition using a Multi Layer Perceptron"}, "summary": "Handwritten numeral recognition is in general a benchmark problem of Pattern\nRecognition and Artificial Intelligence. Compared to the problem of printed\nnumeral recognition, the problem of handwritten numeral recognition is\ncompounded due to variations in shapes and sizes of handwritten characters.\nConsidering all these, the problem of handwritten numeral recognition is\naddressed under the present work in respect to handwritten Arabic numerals.\nArabic is spoken throughout the Arab World and the fifth most popular language\nin the world slightly before Portuguese and Bengali. For the present work, we\nhave developed a feature set of 88 features is designed to represent samples of\nhandwritten Arabic numerals for this work. It includes 72 shadow and 16 octant\nfeatures. A Multi Layer Perceptron (MLP) based classifier is used here for\nrecognition handwritten Arabic digits represented with the said feature set. On\nexperimentation with a database of 3000 samples, the technique yields an\naverage recognition rate of 94.93% evaluated after three-fold cross validation\nof results. It is useful for applications related to OCR of handwritten Arabic\nDigit and can also be extended to include OCR of handwritten characters of\nArabic alphabet.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=130&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Handwritten numeral recognition is in general a benchmark problem of Pattern\nRecognition and Artificial Intelligence. Compared to the problem of printed\nnumeral recognition, the problem of handwritten numeral recognition is\ncompounded due to variations in shapes and sizes of handwritten characters.\nConsidering all these, the problem of handwritten numeral recognition is\naddressed under the present work in respect to handwritten Arabic numerals.\nArabic is spoken throughout the Arab World and the fifth most popular language\nin the world slightly before Portuguese and Bengali. For the present work, we\nhave developed a feature set of 88 features is designed to represent samples of\nhandwritten Arabic numerals for this work. It includes 72 shadow and 16 octant\nfeatures. A Multi Layer Perceptron (MLP) based classifier is used here for\nrecognition handwritten Arabic digits represented with the said feature set. On\nexperimentation with a database of 3000 samples, the technique yields an\naverage recognition rate of 94.93% evaluated after three-fold cross validation\nof results. It is useful for applications related to OCR of handwritten Arabic\nDigit and can also be extended to include OCR of handwritten characters of\nArabic alphabet."}, "authors": ["Nibaran Das", "Ayatullah Faruk Mollah", "Sudip Saha", "Syed Sahidul Haque"], "author_detail": {"name": "Syed Sahidul Haque"}, "author": "Syed Sahidul Haque", "arxiv_comment": "Proc. National Conference on Recent Trends in Information Systems\n  (ReTIS-06), July 14-15, 2006, Kolkata, India, pp 200-203", "links": [{"href": "http://arxiv.org/abs/1003.1891v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1003.1891v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1003.1891v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1003.1891v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1403.7783v1", "guidislink": true, "updated": "2014-03-30T16:48:31Z", "updated_parsed": [2014, 3, 30, 16, 48, 31, 6, 89, 0], "published": "2014-03-30T16:48:31Z", "published_parsed": [2014, 3, 30, 16, 48, 31, 6, 89, 0], "title": "Extraction of Line Word Character Segments Directly from Run Length\n  Compressed Printed Text Documents", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=130&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Extraction of Line Word Character Segments Directly from Run Length\n  Compressed Printed Text Documents"}, "summary": "Segmentation of a text-document into lines, words and characters, which is\nconsidered to be the crucial pre-processing stage in Optical Character\nRecognition (OCR) is traditionally carried out on uncompressed documents,\nalthough most of the documents in real life are available in compressed form,\nfor the reasons such as transmission and storage efficiency. However, this\nimplies that the compressed image should be decompressed, which indents\nadditional computing resources. This limitation has motivated us to take up\nresearch in document image analysis using compressed documents. In this paper,\nwe think in a new way to carry out segmentation at line, word and character\nlevel in run-length compressed printed-text-documents. We extract the\nhorizontal projection profile curve from the compressed file and using the\nlocal minima points perform line segmentation. However, tracing vertical\ninformation which leads to tracking words-characters in a run-length compressed\nfile is not very straight forward. Therefore, we propose a novel technique for\ncarrying out simultaneous word and character segmentation by popping out column\nruns from each row in an intelligent sequence. The proposed algorithms have\nbeen validated with 1101 text-lines, 1409 words and 7582 characters from a\ndata-set of 35 noise and skew free compressed documents of Bengali, Kannada and\nEnglish Scripts.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=130&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Segmentation of a text-document into lines, words and characters, which is\nconsidered to be the crucial pre-processing stage in Optical Character\nRecognition (OCR) is traditionally carried out on uncompressed documents,\nalthough most of the documents in real life are available in compressed form,\nfor the reasons such as transmission and storage efficiency. However, this\nimplies that the compressed image should be decompressed, which indents\nadditional computing resources. This limitation has motivated us to take up\nresearch in document image analysis using compressed documents. In this paper,\nwe think in a new way to carry out segmentation at line, word and character\nlevel in run-length compressed printed-text-documents. We extract the\nhorizontal projection profile curve from the compressed file and using the\nlocal minima points perform line segmentation. However, tracing vertical\ninformation which leads to tracking words-characters in a run-length compressed\nfile is not very straight forward. Therefore, we propose a novel technique for\ncarrying out simultaneous word and character segmentation by popping out column\nruns from each row in an intelligent sequence. The proposed algorithms have\nbeen validated with 1101 text-lines, 1409 words and 7582 characters from a\ndata-set of 35 noise and skew free compressed documents of Bengali, Kannada and\nEnglish Scripts."}, "authors": ["Mohammed Javed", "P. Nagabhushan", "B. B. Chaudhuri"], "author_detail": {"name": "B. B. Chaudhuri"}, "author": "B. B. Chaudhuri", "links": [{"title": "doi", "href": "http://dx.doi.org/10.1109/NCVPRIPG.2013.6776195", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/1403.7783v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1403.7783v1", "rel": "related", "type": "application/pdf"}], "arxiv_comment": "IEEE Proceedings in National Conference on Computer Vision, Pattern\n  Recognition, Image Processing and Graphics (NCVPRIPG 2013)", "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1403.7783v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1403.7783v1", "journal_reference": null, "doi": "10.1109/NCVPRIPG.2013.6776195"}
{"id": "http://arxiv.org/abs/1804.06254v1", "guidislink": true, "updated": "2018-04-17T13:52:59Z", "updated_parsed": [2018, 4, 17, 13, 52, 59, 1, 107, 0], "published": "2018-04-17T13:52:59Z", "published_parsed": [2018, 4, 17, 13, 52, 59, 1, 107, 0], "title": "Synthetic data generation for Indic handwritten text recognition", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=130&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Synthetic data generation for Indic handwritten text recognition"}, "summary": "This paper presents a novel approach to generate synthetic dataset for\nhandwritten word recognition systems. It is difficult to recognize handwritten\nscripts for which sufficient training data is not readily available or it may\nbe expensive to collect such data. Hence, it becomes hard to train recognition\nsystems owing to lack of proper dataset. To overcome such problems, synthetic\ndata could be used to create or expand the existing training dataset to improve\nrecognition performance. Any available digital data from online newspaper and\nsuch sources can be used to generate synthetic data. In this paper, we propose\nto add distortion/deformation to digital data in such a way that the underlying\npattern is preserved, so that the image so produced bears a close similarity to\nactual handwritten samples. The images thus produced can be used independently\nto train the system or be combined with natural handwritten data to augment the\noriginal dataset and improve the recognition system. We experimented using\nsynthetic data to improve the recognition accuracy of isolated characters and\nwords. The framework is tested on 2 Indic scripts - Devanagari (Hindi) and\nBengali (Bangla), for numeral, character and word recognition. We have obtained\nencouraging results from the experiment. Finally, the experiment with Latin\ntext verifies the utility of the approach.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=130&max_results=10&sortBy=relevance&sortOrder=descending", "value": "This paper presents a novel approach to generate synthetic dataset for\nhandwritten word recognition systems. It is difficult to recognize handwritten\nscripts for which sufficient training data is not readily available or it may\nbe expensive to collect such data. Hence, it becomes hard to train recognition\nsystems owing to lack of proper dataset. To overcome such problems, synthetic\ndata could be used to create or expand the existing training dataset to improve\nrecognition performance. Any available digital data from online newspaper and\nsuch sources can be used to generate synthetic data. In this paper, we propose\nto add distortion/deformation to digital data in such a way that the underlying\npattern is preserved, so that the image so produced bears a close similarity to\nactual handwritten samples. The images thus produced can be used independently\nto train the system or be combined with natural handwritten data to augment the\noriginal dataset and improve the recognition system. We experimented using\nsynthetic data to improve the recognition accuracy of isolated characters and\nwords. The framework is tested on 2 Indic scripts - Devanagari (Hindi) and\nBengali (Bangla), for numeral, character and word recognition. We have obtained\nencouraging results from the experiment. Finally, the experiment with Latin\ntext verifies the utility of the approach."}, "authors": ["Partha Pratim Roy", "Akash Mohta", "Bidyut B. Chaudhuri"], "author_detail": {"name": "Bidyut B. Chaudhuri"}, "author": "Bidyut B. Chaudhuri", "links": [{"href": "http://arxiv.org/abs/1804.06254v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1804.06254v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1804.06254v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1804.06254v1", "arxiv_comment": null, "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1707.07150v2", "guidislink": true, "updated": "2017-10-04T16:48:08Z", "updated_parsed": [2017, 10, 4, 16, 48, 8, 2, 277, 0], "published": "2017-07-22T12:09:28Z", "published_parsed": [2017, 7, 22, 12, 9, 28, 5, 203, 0], "title": "Multi-Oriented Text Detection and Verification in Video Frames and Scene\n  Images", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=130&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Multi-Oriented Text Detection and Verification in Video Frames and Scene\n  Images"}, "summary": "In this paper, we bring forth a novel approach of video text detection using\nFourier-Laplacian filtering in the frequency domain that includes a\nverification technique using Hidden Markov Model (HMM). The proposed approach\ndeals with the text region appearing not only in horizontal or vertical\ndirections, but also in any other oblique or curved orientation in the image.\nUntil now only a few methods have been proposed that look into curved text\ndetection in video frames, wherein lies our novelty. In our approach, we first\napply Fourier-Laplacian transform on the image followed by an ideal\nLaplacian-Gaussian filtering. Thereafter K-means clustering is employed to\nobtain the asserted text areas depending on a maximum difference map. Next, the\nobtained connected components (CC) are skeletonized to distinguish various text\nstrings. Complex components are disintegrated into simpler ones according to a\njunction removal algorithm followed by a concatenation performed on possible\ncombination of the disjoint skeletons to obtain the corresponding text area.\nFinally these text hypotheses are verified using HMM-based text/non-text\nclassification system. False positives are thus eliminated giving us a robust\ntext detection performance. We have tested our framework in multi-oriented text\nlines in four scripts, namely, English, Chinese, Devanagari and Bengali, in\nvideo frames and scene texts. The results obtained show that proposed approach\nsurpasses existing methods on text detection.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=130&max_results=10&sortBy=relevance&sortOrder=descending", "value": "In this paper, we bring forth a novel approach of video text detection using\nFourier-Laplacian filtering in the frequency domain that includes a\nverification technique using Hidden Markov Model (HMM). The proposed approach\ndeals with the text region appearing not only in horizontal or vertical\ndirections, but also in any other oblique or curved orientation in the image.\nUntil now only a few methods have been proposed that look into curved text\ndetection in video frames, wherein lies our novelty. In our approach, we first\napply Fourier-Laplacian transform on the image followed by an ideal\nLaplacian-Gaussian filtering. Thereafter K-means clustering is employed to\nobtain the asserted text areas depending on a maximum difference map. Next, the\nobtained connected components (CC) are skeletonized to distinguish various text\nstrings. Complex components are disintegrated into simpler ones according to a\njunction removal algorithm followed by a concatenation performed on possible\ncombination of the disjoint skeletons to obtain the corresponding text area.\nFinally these text hypotheses are verified using HMM-based text/non-text\nclassification system. False positives are thus eliminated giving us a robust\ntext detection performance. We have tested our framework in multi-oriented text\nlines in four scripts, namely, English, Chinese, Devanagari and Bengali, in\nvideo frames and scene texts. The results obtained show that proposed approach\nsurpasses existing methods on text detection."}, "authors": ["Aneeshan Sain", "Ayan Kumar Bhunia", "Partha Pratim Roy", "Umapada Pal"], "author_detail": {"name": "Umapada Pal"}, "author": "Umapada Pal", "links": [{"title": "doi", "href": "http://dx.doi.org/10.1016/j.neucom.2017.09.089", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/1707.07150v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1707.07150v2", "rel": "related", "type": "application/pdf"}], "arxiv_comment": "Accepted in Neurocomputing, Elsevier", "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1707.07150v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1707.07150v2", "journal_reference": null, "doi": "10.1016/j.neucom.2017.09.089"}
{"id": "http://arxiv.org/abs/1806.05997v1", "guidislink": true, "updated": "2018-06-15T14:28:50Z", "updated_parsed": [2018, 6, 15, 14, 28, 50, 4, 166, 0], "published": "2018-06-15T14:28:50Z", "published_parsed": [2018, 6, 15, 14, 28, 50, 4, 166, 0], "title": "A Dataset for Building Code-Mixed Goal Oriented Conversation Systems", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=130&max_results=10&sortBy=relevance&sortOrder=descending", "value": "A Dataset for Building Code-Mixed Goal Oriented Conversation Systems"}, "summary": "There is an increasing demand for goal-oriented conversation systems which\ncan assist users in various day-to-day activities such as booking tickets,\nrestaurant reservations, shopping, etc. Most of the existing datasets for\nbuilding such conversation systems focus on monolingual conversations and there\nis hardly any work on multilingual and/or code-mixed conversations. Such\ndatasets and systems thus do not cater to the multilingual regions of the\nworld, such as India, where it is very common for people to speak more than one\nlanguage and seamlessly switch between them resulting in code-mixed\nconversations. For example, a Hindi speaking user looking to book a restaurant\nwould typically ask, \"Kya tum is restaurant mein ek table book karne mein meri\nhelp karoge?\" (\"Can you help me in booking a table at this restaurant?\"). To\nfacilitate the development of such code-mixed conversation models, we build a\ngoal-oriented dialog dataset containing code-mixed conversations. Specifically,\nwe take the text from the DSTC2 restaurant reservation dataset and create\ncode-mixed versions of it in Hindi-English, Bengali-English, Gujarati-English\nand Tamil-English. We also establish initial baselines on this dataset using\nexisting state of the art models. This dataset along with our baseline\nimplementations is made publicly available for research purposes.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=130&max_results=10&sortBy=relevance&sortOrder=descending", "value": "There is an increasing demand for goal-oriented conversation systems which\ncan assist users in various day-to-day activities such as booking tickets,\nrestaurant reservations, shopping, etc. Most of the existing datasets for\nbuilding such conversation systems focus on monolingual conversations and there\nis hardly any work on multilingual and/or code-mixed conversations. Such\ndatasets and systems thus do not cater to the multilingual regions of the\nworld, such as India, where it is very common for people to speak more than one\nlanguage and seamlessly switch between them resulting in code-mixed\nconversations. For example, a Hindi speaking user looking to book a restaurant\nwould typically ask, \"Kya tum is restaurant mein ek table book karne mein meri\nhelp karoge?\" (\"Can you help me in booking a table at this restaurant?\"). To\nfacilitate the development of such code-mixed conversation models, we build a\ngoal-oriented dialog dataset containing code-mixed conversations. Specifically,\nwe take the text from the DSTC2 restaurant reservation dataset and create\ncode-mixed versions of it in Hindi-English, Bengali-English, Gujarati-English\nand Tamil-English. We also establish initial baselines on this dataset using\nexisting state of the art models. This dataset along with our baseline\nimplementations is made publicly available for research purposes."}, "authors": ["Suman Banerjee", "Nikita Moghe", "Siddhartha Arora", "Mitesh M. Khapra"], "author_detail": {"name": "Mitesh M. Khapra"}, "author": "Mitesh M. Khapra", "arxiv_comment": "15 pages, 2 figures, 10 tables, Accepted in COLING - 2018", "links": [{"href": "http://arxiv.org/abs/1806.05997v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1806.05997v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1806.05997v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1806.05997v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1708.05529v6", "guidislink": true, "updated": "2018-07-30T10:41:30Z", "updated_parsed": [2018, 7, 30, 10, 41, 30, 0, 211, 0], "published": "2017-08-18T07:47:05Z", "published_parsed": [2017, 8, 18, 7, 47, 5, 4, 230, 0], "title": "Word Searching in Scene Image and Video Frame in Multi-Script Scenario\n  using Dynamic Shape Coding", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=130&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Word Searching in Scene Image and Video Frame in Multi-Script Scenario\n  using Dynamic Shape Coding"}, "summary": "Retrieval of text information from natural scene images and video frames is a\nchallenging task due to its inherent problems like complex character shapes,\nlow resolution, background noise, etc. Available OCR systems often fail to\nretrieve such information in scene/video frames. Keyword spotting, an\nalternative way to retrieve information, performs efficient text searching in\nsuch scenarios. However, current word spotting techniques in scene/video images\nare script-specific and they are mainly developed for Latin script. This paper\npresents a novel word spotting framework using dynamic shape coding for text\nretrieval in natural scene image and video frames. The framework is designed to\nsearch query keyword from multiple scripts with the help of on-the-fly\nscript-wise keyword generation for the corresponding script. We have used a\ntwo-stage word spotting approach using Hidden Markov Model (HMM) to detect the\ntranslated keyword in a given text line by identifying the script of the line.\nA novel unsupervised dynamic shape coding based scheme has been used to group\nsimilar shape characters to avoid confusion and to improve text alignment.\nNext, the hypotheses locations are verified to improve retrieval performance.\nTo evaluate the proposed system for searching keyword from natural scene image\nand video frames, we have considered two popular Indic scripts such as Bangla\n(Bengali) and Devanagari along with English. Inspired by the zone-wise\nrecognition approach in Indic scripts[1], zone-wise text information has been\nused to improve the traditional word spotting performance in Indic scripts. For\nour experiment, a dataset consisting of images of different scenes and video\nframes of English, Bangla and Devanagari scripts were considered. The results\nobtained showed the effectiveness of our proposed word spotting approach.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=130&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Retrieval of text information from natural scene images and video frames is a\nchallenging task due to its inherent problems like complex character shapes,\nlow resolution, background noise, etc. Available OCR systems often fail to\nretrieve such information in scene/video frames. Keyword spotting, an\nalternative way to retrieve information, performs efficient text searching in\nsuch scenarios. However, current word spotting techniques in scene/video images\nare script-specific and they are mainly developed for Latin script. This paper\npresents a novel word spotting framework using dynamic shape coding for text\nretrieval in natural scene image and video frames. The framework is designed to\nsearch query keyword from multiple scripts with the help of on-the-fly\nscript-wise keyword generation for the corresponding script. We have used a\ntwo-stage word spotting approach using Hidden Markov Model (HMM) to detect the\ntranslated keyword in a given text line by identifying the script of the line.\nA novel unsupervised dynamic shape coding based scheme has been used to group\nsimilar shape characters to avoid confusion and to improve text alignment.\nNext, the hypotheses locations are verified to improve retrieval performance.\nTo evaluate the proposed system for searching keyword from natural scene image\nand video frames, we have considered two popular Indic scripts such as Bangla\n(Bengali) and Devanagari along with English. Inspired by the zone-wise\nrecognition approach in Indic scripts[1], zone-wise text information has been\nused to improve the traditional word spotting performance in Indic scripts. For\nour experiment, a dataset consisting of images of different scenes and video\nframes of English, Bangla and Devanagari scripts were considered. The results\nobtained showed the effectiveness of our proposed word spotting approach."}, "authors": ["Partha Pratim Roy", "Ayan Kumar Bhunia", "Avirup Bhattacharyya", "Umapada Pal"], "author_detail": {"name": "Umapada Pal"}, "author": "Umapada Pal", "arxiv_comment": "Multimedia Tools and Applications, Springer", "links": [{"href": "http://arxiv.org/abs/1708.05529v6", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1708.05529v6", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.IR", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1708.05529v6", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1708.05529v6", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1901.05613v1", "guidislink": true, "updated": "2019-01-17T04:27:34Z", "updated_parsed": [2019, 1, 17, 4, 27, 34, 3, 17, 0], "published": "2019-01-17T04:27:34Z", "published_parsed": [2019, 1, 17, 4, 27, 34, 3, 17, 0], "title": "Hand Sign to Bangla Speech: A Deep Learning in Vision based system for\n  Recognizing Hand Sign Digits and Generating Bangla Speech", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=130&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Hand Sign to Bangla Speech: A Deep Learning in Vision based system for\n  Recognizing Hand Sign Digits and Generating Bangla Speech"}, "summary": "Recent advancements in the field of computer vision with the help of deep\nneural networks have led us to explore and develop many existing challenges\nthat were once unattended due to the lack of necessary technologies. Hand\nSign/Gesture Recognition is one of the significant areas where the deep neural\nnetwork is making a substantial impact. In the last few years, a large number\nof researches has been conducted to recognize hand signs and hand gestures,\nwhich we aim to extend to our mother-tongue, Bangla (also known as Bengali).\nThe primary goal of our work is to make an automated tool to aid the people who\nare unable to speak. We developed a system that automatically detects hand sign\nbased digits and speaks out the result in Bangla language. According to the\nreport of the World Health Organization (WHO), 15% of people in the world live\nwith some kind of disabilities. Among them, individuals with communication\nimpairment such as speech disabilities experience substantial barrier in social\ninteraction. The proposed system can be invaluable to mitigate such a barrier.\nThe core of the system is built with a deep learning model which is based on\nconvolutional neural networks (CNN). The model classifies hand sign based\ndigits with 92% accuracy over validation data which ensures it a highly\ntrustworthy system. Upon classification of the digits, the resulting output is\nfed to the text to speech engine and the translator unit eventually which\ngenerates audio output in Bangla language. A web application to demonstrate our\ntool is available at http://bit.ly/signdigits2banglaspeech.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=130&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Recent advancements in the field of computer vision with the help of deep\nneural networks have led us to explore and develop many existing challenges\nthat were once unattended due to the lack of necessary technologies. Hand\nSign/Gesture Recognition is one of the significant areas where the deep neural\nnetwork is making a substantial impact. In the last few years, a large number\nof researches has been conducted to recognize hand signs and hand gestures,\nwhich we aim to extend to our mother-tongue, Bangla (also known as Bengali).\nThe primary goal of our work is to make an automated tool to aid the people who\nare unable to speak. We developed a system that automatically detects hand sign\nbased digits and speaks out the result in Bangla language. According to the\nreport of the World Health Organization (WHO), 15% of people in the world live\nwith some kind of disabilities. Among them, individuals with communication\nimpairment such as speech disabilities experience substantial barrier in social\ninteraction. The proposed system can be invaluable to mitigate such a barrier.\nThe core of the system is built with a deep learning model which is based on\nconvolutional neural networks (CNN). The model classifies hand sign based\ndigits with 92% accuracy over validation data which ensures it a highly\ntrustworthy system. Upon classification of the digits, the resulting output is\nfed to the text to speech engine and the translator unit eventually which\ngenerates audio output in Bangla language. A web application to demonstrate our\ntool is available at http://bit.ly/signdigits2banglaspeech."}, "authors": ["Shahjalal Ahmed", "Md. Rafiqul Islam", "Jahid Hassan", "Minhaz Uddin Ahmed", "Bilkis Jamal Ferdosi", "Sanjay Saha", "Md. Shopon"], "author_detail": {"name": "Md. Shopon"}, "author": "Md. Shopon", "links": [{"href": "http://arxiv.org/abs/1901.05613v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1901.05613v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1901.05613v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1901.05613v1", "arxiv_comment": null, "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/2004.00104v1", "guidislink": true, "updated": "2020-03-31T20:58:14Z", "updated_parsed": [2020, 3, 31, 20, 58, 14, 1, 91, 0], "published": "2020-03-31T20:58:14Z", "published_parsed": [2020, 3, 31, 20, 58, 14, 1, 91, 0], "title": "Improvement of electronic Governance and mobile Governance in\n  Multilingual Countries with Digital Etymology using Sanskrit Grammar", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=140&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Improvement of electronic Governance and mobile Governance in\n  Multilingual Countries with Digital Etymology using Sanskrit Grammar"}, "summary": "With huge improvement of digital connectivity (Wifi,3G,4G) and digital\ndevices access to internet has reached in the remotest corners now a days.\nRural people can easily access web or apps from PDAs, laptops, smartphones etc.\nThis is an opportunity of the Government to reach to the citizen in large\nnumber, get their feedback, associate them in policy decision with e governance\nwithout deploying huge man, material or resourses. But the Government of\nmultilingual countries face a lot of problem in successful implementation of\nGovernment to Citizen (G2C) and Citizen to Government (C2G) governance as the\nrural people tend and prefer to interact in their native languages. Presenting\nequal experience over web or app to different language group of speakers is a\nreal challenge. In this research we have sorted out the problems faced by Indo\nAryan speaking netizens which is in general also applicable to any language\nfamily groups or subgroups. Then we have tried to give probable solutions using\nEtymology. Etymology is used to correlate the words using their ROOT forms. In\n5th century BC Panini wrote Astadhyayi where he depicted sutras or rules -- how\na word is changed according to person,tense,gender,number etc. Later this book\nwas followed in Western countries also to derive their grammar of comparatively\nnew languages. We have trained our system for automatic root extraction from\nthe surface level or morphed form of words using Panian Gramatical rules. We\nhave tested our system over 10000 bengali Verbs and extracted the root form\nwith 98% accuracy. We are now working to extend the program to successfully\nlemmatize any words of any language and correlate them by applying those rule\nsets in Artificial Neural Network.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=140&max_results=10&sortBy=relevance&sortOrder=descending", "value": "With huge improvement of digital connectivity (Wifi,3G,4G) and digital\ndevices access to internet has reached in the remotest corners now a days.\nRural people can easily access web or apps from PDAs, laptops, smartphones etc.\nThis is an opportunity of the Government to reach to the citizen in large\nnumber, get their feedback, associate them in policy decision with e governance\nwithout deploying huge man, material or resourses. But the Government of\nmultilingual countries face a lot of problem in successful implementation of\nGovernment to Citizen (G2C) and Citizen to Government (C2G) governance as the\nrural people tend and prefer to interact in their native languages. Presenting\nequal experience over web or app to different language group of speakers is a\nreal challenge. In this research we have sorted out the problems faced by Indo\nAryan speaking netizens which is in general also applicable to any language\nfamily groups or subgroups. Then we have tried to give probable solutions using\nEtymology. Etymology is used to correlate the words using their ROOT forms. In\n5th century BC Panini wrote Astadhyayi where he depicted sutras or rules -- how\na word is changed according to person,tense,gender,number etc. Later this book\nwas followed in Western countries also to derive their grammar of comparatively\nnew languages. We have trained our system for automatic root extraction from\nthe surface level or morphed form of words using Panian Gramatical rules. We\nhave tested our system over 10000 bengali Verbs and extracted the root form\nwith 98% accuracy. We are now working to extend the program to successfully\nlemmatize any words of any language and correlate them by applying those rule\nsets in Artificial Neural Network."}, "authors": ["Arijit Das", "Diganta Saha"], "author_detail": {"name": "Diganta Saha"}, "author": "Diganta Saha", "links": [{"title": "doi", "href": "http://dx.doi.org/10.1109/R10-HTC.2017.8289008", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/2004.00104v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2004.00104v1", "rel": "related", "type": "application/pdf"}], "arxiv_comment": "7 pages. 2017 IEEE Region 10 Humanitarian Technology Conference\n  (R10-HTC), Dhaka, 2017", "arxiv_primary_category": {"term": "cs.CY", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CY", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2004.00104v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2004.00104v1", "journal_reference": null, "doi": "10.1109/R10-HTC.2017.8289008"}
{"id": "http://arxiv.org/abs/2101.03916v1", "guidislink": true, "updated": "2021-01-05T03:16:34Z", "updated_parsed": [2021, 1, 5, 3, 16, 34, 1, 5, 0], "published": "2021-01-05T03:16:34Z", "published_parsed": [2021, 1, 5, 3, 16, 34, 1, 5, 0], "title": "edATLAS: An Efficient Disambiguation Algorithm for Texting in Languages\n  with Abugida Scripts", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=140&max_results=10&sortBy=relevance&sortOrder=descending", "value": "edATLAS: An Efficient Disambiguation Algorithm for Texting in Languages\n  with Abugida Scripts"}, "summary": "Abugida refers to a phonogram writing system where each syllable is\nrepresented using a single consonant or typographic ligature, along with a\ndefault vowel or optional diacritic(s) to denote other vowels. However, texting\nin these languages has some unique challenges in spite of the advent of devices\nwith soft keyboard supporting custom key layouts. The number of characters in\nthese languages is large enough to require characters to be spread over\nmultiple views in the layout. Having to switch between views many times to type\na single word hinders the natural thought process. This prevents popular usage\nof native keyboard layouts. On the other hand, supporting romanized scripts\n(native words transcribed using Latin characters) with language model based\nsuggestions is also set back by the lack of uniform romanization rules.\n  To this end, we propose a disambiguation algorithm and showcase its\nusefulness in two novel mutually non-exclusive input methods for languages\nnatively using the abugida writing system: (a) disambiguation of ambiguous\ninput for abugida scripts, and (b) disambiguation of word variants in romanized\nscripts. We benchmark these approaches using public datasets, and show an\nimprovement in typing speed by 19.49%, 25.13%, and 14.89%, in Hindi, Bengali,\nand Thai, respectively, using Ambiguous Input, owing to the human ease of\nlocating keys combined with the efficiency of our inference method. Our Word\nVariant Disambiguation (WDA) maps valid variants of romanized words, previously\ntreated as Out-of-Vocab, to a vocabulary of 100k words with high accuracy,\nleading to an increase in Error Correction F1 score by 10.03% and Next Word\nPrediction (NWP) by 62.50% on average.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bengali&id_list=&start=140&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Abugida refers to a phonogram writing system where each syllable is\nrepresented using a single consonant or typographic ligature, along with a\ndefault vowel or optional diacritic(s) to denote other vowels. However, texting\nin these languages has some unique challenges in spite of the advent of devices\nwith soft keyboard supporting custom key layouts. The number of characters in\nthese languages is large enough to require characters to be spread over\nmultiple views in the layout. Having to switch between views many times to type\na single word hinders the natural thought process. This prevents popular usage\nof native keyboard layouts. On the other hand, supporting romanized scripts\n(native words transcribed using Latin characters) with language model based\nsuggestions is also set back by the lack of uniform romanization rules.\n  To this end, we propose a disambiguation algorithm and showcase its\nusefulness in two novel mutually non-exclusive input methods for languages\nnatively using the abugida writing system: (a) disambiguation of ambiguous\ninput for abugida scripts, and (b) disambiguation of word variants in romanized\nscripts. We benchmark these approaches using public datasets, and show an\nimprovement in typing speed by 19.49%, 25.13%, and 14.89%, in Hindi, Bengali,\nand Thai, respectively, using Ambiguous Input, owing to the human ease of\nlocating keys combined with the efficiency of our inference method. Our Word\nVariant Disambiguation (WDA) maps valid variants of romanized words, previously\ntreated as Out-of-Vocab, to a vocabulary of 100k words with high accuracy,\nleading to an increase in Error Correction F1 score by 10.03% and Next Word\nPrediction (NWP) by 62.50% on average."}, "authors": ["Sourav Ghosh", "Sourabh Vasant Gothe", "Chandramouli Sanchi", "Barath Raj Kandur Raja"], "author_detail": {"name": "Barath Raj Kandur Raja"}, "author": "Barath Raj Kandur Raja", "arxiv_comment": "Accepted for publication in the 15th IEEE International Conference on\n  Semantic Computing (IEEE ICSC 2021)", "links": [{"href": "http://arxiv.org/abs/2101.03916v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2101.03916v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2101.03916v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2101.03916v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/2002.08627v2", "guidislink": true, "updated": "2020-02-28T11:02:16Z", "updated_parsed": [2020, 2, 28, 11, 2, 16, 4, 59, 0], "published": "2020-02-20T09:04:38Z", "published_parsed": [2020, 2, 20, 9, 4, 38, 3, 51, 0], "title": "A Comprehensive Scoping Review of Bayesian Networks in Healthcare: Past,\n  Present and Future", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=0&max_results=10&sortBy=relevance&sortOrder=descending", "value": "A Comprehensive Scoping Review of Bayesian Networks in Healthcare: Past,\n  Present and Future"}, "summary": "No comprehensive review of Bayesian networks (BNs) in healthcare has been\npublished in the past, making it difficult to organize the research\ncontributions in the present and identify challenges and neglected areas that\nneed to be addressed in the future. This unique and novel scoping review of BNs\nin healthcare provides an analytical framework for comprehensively\ncharacterizing the domain and its current state. The review shows that: (1) BNs\nin healthcare are not used to their full potential; (2) a generic BN\ndevelopment process is lacking; (3) limitations exists in the way BNs in\nhealthcare are presented in the literature, which impacts understanding,\nconsensus towards systematic methodologies, practice and adoption of BNs; and\n(4) a gap exists between having an accurate BN and a useful BN that impacts\nclinical practice. This review empowers researchers and clinicians with an\nanalytical framework and findings that will enable understanding of the need to\naddress the problems of restricted aims of BNs, ad hoc BN development methods,\nand the lack of BN adoption in practice. To map the way forward, the paper\nproposes future research directions and makes recommendations regarding BN\ndevelopment methods and adoption in practice.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=0&max_results=10&sortBy=relevance&sortOrder=descending", "value": "No comprehensive review of Bayesian networks (BNs) in healthcare has been\npublished in the past, making it difficult to organize the research\ncontributions in the present and identify challenges and neglected areas that\nneed to be addressed in the future. This unique and novel scoping review of BNs\nin healthcare provides an analytical framework for comprehensively\ncharacterizing the domain and its current state. The review shows that: (1) BNs\nin healthcare are not used to their full potential; (2) a generic BN\ndevelopment process is lacking; (3) limitations exists in the way BNs in\nhealthcare are presented in the literature, which impacts understanding,\nconsensus towards systematic methodologies, practice and adoption of BNs; and\n(4) a gap exists between having an accurate BN and a useful BN that impacts\nclinical practice. This review empowers researchers and clinicians with an\nanalytical framework and findings that will enable understanding of the need to\naddress the problems of restricted aims of BNs, ad hoc BN development methods,\nand the lack of BN adoption in practice. To map the way forward, the paper\nproposes future research directions and makes recommendations regarding BN\ndevelopment methods and adoption in practice."}, "authors": ["Evangelia Kyrimi", "Scott McLachlan", "Kudakwashe Dube", "Mariana R. Neves", "Ali Fahmi", "Norman Fenton"], "author_detail": {"name": "Norman Fenton"}, "author": "Norman Fenton", "links": [{"href": "http://arxiv.org/abs/2002.08627v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2002.08627v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2002.08627v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2002.08627v2", "arxiv_comment": null, "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1809.00846v4", "guidislink": true, "updated": "2019-04-24T05:23:45Z", "updated_parsed": [2019, 4, 24, 5, 23, 45, 2, 114, 0], "published": "2018-09-04T09:01:10Z", "published_parsed": [2018, 9, 4, 9, 1, 10, 1, 247, 0], "title": "Towards Understanding Regularization in Batch Normalization", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=0&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Towards Understanding Regularization in Batch Normalization"}, "summary": "Batch Normalization (BN) improves both convergence and generalization in\ntraining neural networks. This work understands these phenomena theoretically.\nWe analyze BN by using a basic block of neural networks, consisting of a kernel\nlayer, a BN layer, and a nonlinear activation function. This basic network\nhelps us understand the impacts of BN in three aspects. First, by viewing BN as\nan implicit regularizer, BN can be decomposed into population normalization\n(PN) and gamma decay as an explicit regularization. Second, learning dynamics\nof BN and the regularization show that training converged with large maximum\nand effective learning rate. Third, generalization of BN is explored by using\nstatistical mechanics. Experiments demonstrate that BN in convolutional neural\nnetworks share the same traits of regularization as the above analyses.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=0&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Batch Normalization (BN) improves both convergence and generalization in\ntraining neural networks. This work understands these phenomena theoretically.\nWe analyze BN by using a basic block of neural networks, consisting of a kernel\nlayer, a BN layer, and a nonlinear activation function. This basic network\nhelps us understand the impacts of BN in three aspects. First, by viewing BN as\nan implicit regularizer, BN can be decomposed into population normalization\n(PN) and gamma decay as an explicit regularization. Second, learning dynamics\nof BN and the regularization show that training converged with large maximum\nand effective learning rate. Third, generalization of BN is explored by using\nstatistical mechanics. Experiments demonstrate that BN in convolutional neural\nnetworks share the same traits of regularization as the above analyses."}, "authors": ["Ping Luo", "Xinjiang Wang", "Wenqi Shao", "Zhanglin Peng"], "author_detail": {"name": "Zhanglin Peng"}, "author": "Zhanglin Peng", "arxiv_comment": "International Conference on Learning Representations (ICLR)", "links": [{"href": "http://arxiv.org/abs/1809.00846v4", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1809.00846v4", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.SY", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.ML", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1809.00846v4", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1809.00846v4", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1709.08145v2", "guidislink": true, "updated": "2017-10-07T23:28:57Z", "updated_parsed": [2017, 10, 7, 23, 28, 57, 5, 280, 0], "published": "2017-09-24T04:33:53Z", "published_parsed": [2017, 9, 24, 4, 33, 53, 6, 267, 0], "title": "Comparison of Batch Normalization and Weight Normalization Algorithms\n  for the Large-scale Image Classification", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=0&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Comparison of Batch Normalization and Weight Normalization Algorithms\n  for the Large-scale Image Classification"}, "summary": "Batch normalization (BN) has become a de facto standard for training deep\nconvolutional networks. However, BN accounts for a significant fraction of\ntraining run-time and is difficult to accelerate, since it is a\nmemory-bandwidth bounded operation. Such a drawback of BN motivates us to\nexplore recently proposed weight normalization algorithms (WN algorithms), i.e.\nweight normalization, normalization propagation and weight normalization with\ntranslated ReLU. These algorithms don't slow-down training iterations and were\nexperimentally shown to outperform BN on relatively small networks and\ndatasets. However, it is not clear if these algorithms could replace BN in\npractical, large-scale applications. We answer this question by providing a\ndetailed comparison of BN and WN algorithms using ResNet-50 network trained on\nImageNet. We found that although WN achieves better training accuracy, the\nfinal test accuracy is significantly lower ($\\approx 6\\%$) than that of BN.\nThis result demonstrates the surprising strength of the BN regularization\neffect which we were unable to compensate for using standard regularization\ntechniques like dropout and weight decay. We also found that training of deep\nnetworks with WN algorithms is significantly less stable compared to BN,\nlimiting their practical applications.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=0&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Batch normalization (BN) has become a de facto standard for training deep\nconvolutional networks. However, BN accounts for a significant fraction of\ntraining run-time and is difficult to accelerate, since it is a\nmemory-bandwidth bounded operation. Such a drawback of BN motivates us to\nexplore recently proposed weight normalization algorithms (WN algorithms), i.e.\nweight normalization, normalization propagation and weight normalization with\ntranslated ReLU. These algorithms don't slow-down training iterations and were\nexperimentally shown to outperform BN on relatively small networks and\ndatasets. However, it is not clear if these algorithms could replace BN in\npractical, large-scale applications. We answer this question by providing a\ndetailed comparison of BN and WN algorithms using ResNet-50 network trained on\nImageNet. We found that although WN achieves better training accuracy, the\nfinal test accuracy is significantly lower ($\\approx 6\\%$) than that of BN.\nThis result demonstrates the surprising strength of the BN regularization\neffect which we were unable to compensate for using standard regularization\ntechniques like dropout and weight decay. We also found that training of deep\nnetworks with WN algorithms is significantly less stable compared to BN,\nlimiting their practical applications."}, "authors": ["Igor Gitman", "Boris Ginsburg"], "author_detail": {"name": "Boris Ginsburg"}, "author": "Boris Ginsburg", "links": [{"href": "http://arxiv.org/abs/1709.08145v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1709.08145v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1709.08145v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1709.08145v2", "arxiv_comment": null, "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1912.04259v3", "guidislink": true, "updated": "2020-04-23T05:00:20Z", "updated_parsed": [2020, 4, 23, 5, 0, 20, 3, 114, 0], "published": "2019-12-09T18:52:30Z", "published_parsed": [2019, 12, 9, 18, 52, 30, 0, 343, 0], "title": "An Empirical Study on Position of the Batch Normalization Layer in\n  Convolutional Neural Networks", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=10&max_results=10&sortBy=relevance&sortOrder=descending", "value": "An Empirical Study on Position of the Batch Normalization Layer in\n  Convolutional Neural Networks"}, "summary": "In this paper, we have studied how the training of the convolutional neural\nnetworks (CNNs) can be affected by changing the position of the batch\nnormalization (BN) layer. Three different convolutional neural networks have\nbeen chosen for our experiments. These networks are AlexNet, VGG-16, and\nResNet- 20. We show that the speed up in training provided by the BN algorithm\ncan be improved by using other positions for the BN layer than the one\nsuggested by its original paper. Also, we discuss how the BN layer in a certain\nposition can aid the training of one network but not the other. Three different\npositions for the BN layer have been studied in this research. These positions\nare: the BN layer between the convolution layer and the non-linear activation\nfunction, the BN layer after the non-linear activation function and finally,\nthe BN layer before each of the convolutional layers.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=10&max_results=10&sortBy=relevance&sortOrder=descending", "value": "In this paper, we have studied how the training of the convolutional neural\nnetworks (CNNs) can be affected by changing the position of the batch\nnormalization (BN) layer. Three different convolutional neural networks have\nbeen chosen for our experiments. These networks are AlexNet, VGG-16, and\nResNet- 20. We show that the speed up in training provided by the BN algorithm\ncan be improved by using other positions for the BN layer than the one\nsuggested by its original paper. Also, we discuss how the BN layer in a certain\nposition can aid the training of one network but not the other. Three different\npositions for the BN layer have been studied in this research. These positions\nare: the BN layer between the convolution layer and the non-linear activation\nfunction, the BN layer after the non-linear activation function and finally,\nthe BN layer before each of the convolutional layers."}, "authors": ["Moein Hasani", "Hassan Khotanlou"], "author_detail": {"name": "Hassan Khotanlou"}, "author": "Hassan Khotanlou", "links": [{"title": "doi", "href": "http://dx.doi.org/10.1109/ICSPIS48872.2019.9066113", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/1912.04259v3", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1912.04259v3", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1912.04259v3", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1912.04259v3", "arxiv_comment": null, "journal_reference": null, "doi": "10.1109/ICSPIS48872.2019.9066113"}
{"id": "http://arxiv.org/abs/1207.5293v2", "guidislink": true, "updated": "2012-10-07T19:24:39Z", "updated_parsed": [2012, 10, 7, 19, 24, 39, 6, 281, 0], "published": "2012-07-23T04:56:33Z", "published_parsed": [2012, 7, 23, 4, 56, 33, 0, 205, 0], "title": "Probability Bracket Notation, Multivariable Systems and Static Bayesian\n  Networks", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=10&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Probability Bracket Notation, Multivariable Systems and Static Bayesian\n  Networks"}, "summary": "Probability Bracket Notation (PBN) is applied to systems of multiple random\nvariables for preliminary study of static Bayesian Networks (BN) and\nProbabilistic Graphic Models (PGM). The famous Student BN Example is explored\nto show the local independences and reasoning power of a BN. Software package\nElvira is used to graphically display the student BN. Our investigation shows\nthat PBN provides a consistent and convenient alternative to manipulate many\nexpressions related to joint, marginal and conditional probability\ndistributions in static BN.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=10&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Probability Bracket Notation (PBN) is applied to systems of multiple random\nvariables for preliminary study of static Bayesian Networks (BN) and\nProbabilistic Graphic Models (PGM). The famous Student BN Example is explored\nto show the local independences and reasoning power of a BN. Software package\nElvira is used to graphically display the student BN. Our investigation shows\nthat PBN provides a consistent and convenient alternative to manipulate many\nexpressions related to joint, marginal and conditional probability\ndistributions in static BN."}, "authors": ["Xing M. Wang"], "author_detail": {"name": "Xing M. Wang"}, "author": "Xing M. Wang", "links": [{"href": "http://arxiv.org/abs/1207.5293v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1207.5293v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "math.PR", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "62F15", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "G.3; I.2.3", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1207.5293v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1207.5293v2", "arxiv_comment": null, "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1301.2297v1", "guidislink": true, "updated": "2013-01-10T16:25:34Z", "updated_parsed": [2013, 1, 10, 16, 25, 34, 3, 10, 0], "published": "2013-01-10T16:25:34Z", "published_parsed": [2013, 1, 10, 16, 25, 34, 3, 10, 0], "title": "A Case Study in Knowledge Discovery and Elicitation in an Intelligent\n  Tutoring Application", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=10&max_results=10&sortBy=relevance&sortOrder=descending", "value": "A Case Study in Knowledge Discovery and Elicitation in an Intelligent\n  Tutoring Application"}, "summary": "Most successful Bayesian network (BN) applications to datehave been built\nthrough knowledge elicitation from experts.This is difficult and time\nconsuming, which has lead to recentinterest in automated methods for learning\nBNs from data. We present a case study in the construction of a BN in\nanintelligent tutoring application, specifically decimal misconceptions.\nWedescribe the BN construction using expert elicitation and then investigate\nhow certainexisting automated knowledge discovery methods might support the BN\nknowledge engineering process.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=10&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Most successful Bayesian network (BN) applications to datehave been built\nthrough knowledge elicitation from experts.This is difficult and time\nconsuming, which has lead to recentinterest in automated methods for learning\nBNs from data. We present a case study in the construction of a BN in\nanintelligent tutoring application, specifically decimal misconceptions.\nWedescribe the BN construction using expert elicitation and then investigate\nhow certainexisting automated knowledge discovery methods might support the BN\nknowledge engineering process."}, "authors": ["Ann Nicholson", "Tal Boneh", "Tim Wilkin", "Kaye Stacey", "Liz Sonenberg", "Vicki Steinle"], "author_detail": {"name": "Vicki Steinle"}, "author": "Vicki Steinle", "arxiv_comment": "Appears in Proceedings of the Seventeenth Conference on Uncertainty\n  in Artificial Intelligence (UAI2001)", "links": [{"href": "http://arxiv.org/abs/1301.2297v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1301.2297v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1301.2297v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1301.2297v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/2010.09278v2", "guidislink": true, "updated": "2020-10-24T01:50:11Z", "updated_parsed": [2020, 10, 24, 1, 50, 11, 5, 298, 0], "published": "2020-10-19T07:42:41Z", "published_parsed": [2020, 10, 19, 7, 42, 41, 0, 293, 0], "title": "MimicNorm: Weight Mean and Last BN Layer Mimic the Dynamic of Batch\n  Normalization", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=10&max_results=10&sortBy=relevance&sortOrder=descending", "value": "MimicNorm: Weight Mean and Last BN Layer Mimic the Dynamic of Batch\n  Normalization"}, "summary": "Substantial experiments have validated the success of Batch Normalization\n(BN) Layer in benefiting convergence and generalization. However, BN requires\nextra memory and float-point calculation. Moreover, BN would be inaccurate on\nmicro-batch, as it depends on batch statistics. In this paper, we address these\nproblems by simplifying BN regularization while keeping two fundamental impacts\nof BN layers, i.e., data decorrelation and adaptive learning rate. We propose a\nnovel normalization method, named MimicNorm, to improve the convergence and\nefficiency in network training. MimicNorm consists of only two light\noperations, including modified weight mean operations (subtract mean values\nfrom weight parameter tensor) and one BN layer before loss function (last BN\nlayer). We leverage the neural tangent kernel (NTK) theory to prove that our\nweight mean operation whitens activations and transits network into the chaotic\nregime like BN layer, and consequently, leads to an enhanced convergence. The\nlast BN layer provides autotuned learning rates and also improves accuracy.\nExperimental results show that MimicNorm achieves similar accuracy for various\nnetwork structures, including ResNets and lightweight networks like ShuffleNet,\nwith a reduction of about 20% memory consumption. The code is publicly\navailable at https://github.com/Kid-key/MimicNorm.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=10&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Substantial experiments have validated the success of Batch Normalization\n(BN) Layer in benefiting convergence and generalization. However, BN requires\nextra memory and float-point calculation. Moreover, BN would be inaccurate on\nmicro-batch, as it depends on batch statistics. In this paper, we address these\nproblems by simplifying BN regularization while keeping two fundamental impacts\nof BN layers, i.e., data decorrelation and adaptive learning rate. We propose a\nnovel normalization method, named MimicNorm, to improve the convergence and\nefficiency in network training. MimicNorm consists of only two light\noperations, including modified weight mean operations (subtract mean values\nfrom weight parameter tensor) and one BN layer before loss function (last BN\nlayer). We leverage the neural tangent kernel (NTK) theory to prove that our\nweight mean operation whitens activations and transits network into the chaotic\nregime like BN layer, and consequently, leads to an enhanced convergence. The\nlast BN layer provides autotuned learning rates and also improves accuracy.\nExperimental results show that MimicNorm achieves similar accuracy for various\nnetwork structures, including ResNets and lightweight networks like ShuffleNet,\nwith a reduction of about 20% memory consumption. The code is publicly\navailable at https://github.com/Kid-key/MimicNorm."}, "authors": ["Wen Fei", "Wenrui Dai", "Chenglin Li", "Junni Zou", "Hongkai Xiong"], "author_detail": {"name": "Hongkai Xiong"}, "author": "Hongkai Xiong", "links": [{"href": "http://arxiv.org/abs/2010.09278v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2010.09278v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2010.09278v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2010.09278v2", "arxiv_comment": null, "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1807.01702v2", "guidislink": true, "updated": "2019-03-01T08:27:20Z", "updated_parsed": [2019, 3, 1, 8, 27, 20, 4, 60, 0], "published": "2018-07-04T02:00:19Z", "published_parsed": [2018, 7, 4, 2, 0, 19, 2, 185, 0], "title": "Restructuring Batch Normalization to Accelerate CNN Training", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=20&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Restructuring Batch Normalization to Accelerate CNN Training"}, "summary": "Batch Normalization (BN) has become a core design block of modern\nConvolutional Neural Networks (CNNs). A typical modern CNN has a large number\nof BN layers in its lean and deep architecture. BN requires mean and variance\ncalculations over each mini-batch during training. Therefore, the existing\nmemory access reduction techniques, such as fusing multiple CONV layers, are\nnot effective for accelerating BN due to their inability to optimize mini-batch\nrelated calculations during training. To address this increasingly important\nproblem, we propose to restructure BN layers by first splitting a BN layer into\ntwo sub-layers (fission) and then combining the first sub-layer with its\npreceding CONV layer and the second sub-layer with the following activation and\nCONV layers (fusion). The proposed solution can significantly reduce\nmain-memory accesses while training the latest CNN models, and the experiments\non a chip multiprocessor show that the proposed BN restructuring can improve\nthe performance of DenseNet-121 by 25.7%.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=20&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Batch Normalization (BN) has become a core design block of modern\nConvolutional Neural Networks (CNNs). A typical modern CNN has a large number\nof BN layers in its lean and deep architecture. BN requires mean and variance\ncalculations over each mini-batch during training. Therefore, the existing\nmemory access reduction techniques, such as fusing multiple CONV layers, are\nnot effective for accelerating BN due to their inability to optimize mini-batch\nrelated calculations during training. To address this increasingly important\nproblem, we propose to restructure BN layers by first splitting a BN layer into\ntwo sub-layers (fission) and then combining the first sub-layer with its\npreceding CONV layer and the second sub-layer with the following activation and\nCONV layers (fusion). The proposed solution can significantly reduce\nmain-memory accesses while training the latest CNN models, and the experiments\non a chip multiprocessor show that the proposed BN restructuring can improve\nthe performance of DenseNet-121 by 25.7%."}, "authors": ["Wonkyung Jung", "Daejin Jung", "and Byeongho Kim", "Sunjung Lee", "Wonjong Rhee", "Jung Ho Ahn"], "author_detail": {"name": "Jung Ho Ahn"}, "author": "Jung Ho Ahn", "arxiv_comment": "13 pages, 8 figures, to appear in SysML 2019, added ResNet-50 results", "links": [{"href": "http://arxiv.org/abs/1807.01702v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1807.01702v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.PF", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1807.01702v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1807.01702v2", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1912.10092v2", "guidislink": true, "updated": "2020-05-19T17:52:00Z", "updated_parsed": [2020, 5, 19, 17, 52, 0, 1, 140, 0], "published": "2019-12-20T20:39:28Z", "published_parsed": [2019, 12, 20, 20, 39, 28, 4, 354, 0], "title": "Sum-Product Network Decompilation", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=20&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Sum-Product Network Decompilation"}, "summary": "There exists a dichotomy between classical probabilistic graphical models,\nsuch as Bayesian networks (BNs), and modern tractable models, such as\nsum-product networks (SPNs). The former generally have intractable inference,\nbut provide a high level of interpretability, while the latter admits a wide\nrange of tractable inference routines, but are typically harder to interpret.\nDue to this dichotomy, tools to convert between BNs and SPNs are desirable.\nWhile one direction -- compiling BNs into SPNs -- is well discussed in\nDarwiche's seminal work on arithmetic circuit compilation, the converse\ndirection -- decompiling SPNs into BNs -- has received surprisingly little\nattention.\n  In this paper, we fill this gap by proposing SPN2BN, an algorithm that\ndecompiles an SPN into a BN. SPN2BN has several salient features when compared\nto the only other two works decompiling SPNs. Most significantly, the BNs\nreturned by SPN2BN are minimal independence-maps that are more parsimonious\nwith respect to the introduction of latent variables. Secondly, the output BN\nproduced by SPN2BN can be precisely characterized with respect to a compiled\nBN. More specifically, a certain set of directed edges will be added to the\ninput BN, giving what we will call the moral-closure. Lastly, it is established\nthat our compilation-decompilation process is idempotent. This has practical\nsignificance as it limits the size of the decompiled SPN.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=20&max_results=10&sortBy=relevance&sortOrder=descending", "value": "There exists a dichotomy between classical probabilistic graphical models,\nsuch as Bayesian networks (BNs), and modern tractable models, such as\nsum-product networks (SPNs). The former generally have intractable inference,\nbut provide a high level of interpretability, while the latter admits a wide\nrange of tractable inference routines, but are typically harder to interpret.\nDue to this dichotomy, tools to convert between BNs and SPNs are desirable.\nWhile one direction -- compiling BNs into SPNs -- is well discussed in\nDarwiche's seminal work on arithmetic circuit compilation, the converse\ndirection -- decompiling SPNs into BNs -- has received surprisingly little\nattention.\n  In this paper, we fill this gap by proposing SPN2BN, an algorithm that\ndecompiles an SPN into a BN. SPN2BN has several salient features when compared\nto the only other two works decompiling SPNs. Most significantly, the BNs\nreturned by SPN2BN are minimal independence-maps that are more parsimonious\nwith respect to the introduction of latent variables. Secondly, the output BN\nproduced by SPN2BN can be precisely characterized with respect to a compiled\nBN. More specifically, a certain set of directed edges will be added to the\ninput BN, giving what we will call the moral-closure. Lastly, it is established\nthat our compilation-decompilation process is idempotent. This has practical\nsignificance as it limits the size of the decompiled SPN."}, "authors": ["Cory J. Butz", "Jhonatan S. Oliveira", "Robert Peharz"], "author_detail": {"name": "Robert Peharz"}, "author": "Robert Peharz", "links": [{"href": "http://arxiv.org/abs/1912.10092v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1912.10092v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1912.10092v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1912.10092v2", "arxiv_comment": null, "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1101.6009v1", "guidislink": true, "updated": "2011-01-31T16:32:06Z", "updated_parsed": [2011, 1, 31, 16, 32, 6, 0, 31, 0], "published": "2011-01-31T16:32:06Z", "published_parsed": [2011, 1, 31, 16, 32, 6, 0, 31, 0], "title": "Solving the Satisfiability Problem Through Boolean Networks", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=20&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Solving the Satisfiability Problem Through Boolean Networks"}, "summary": "In this paper we present a new approach to solve the satisfiability problem\n(SAT), based on boolean networks (BN). We define a mapping between a SAT\ninstance and a BN, and we solve SAT problem by simulating the BN dynamics. We\nprove that BN fixed points correspond to the SAT solutions. The mapping\npresented allows to develop a new class of algorithms to solve SAT. Moreover,\nthis new approach suggests new ways to combine symbolic and connectionist\ncomputation and provides a general framework for local search algorithms.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=20&max_results=10&sortBy=relevance&sortOrder=descending", "value": "In this paper we present a new approach to solve the satisfiability problem\n(SAT), based on boolean networks (BN). We define a mapping between a SAT\ninstance and a BN, and we solve SAT problem by simulating the BN dynamics. We\nprove that BN fixed points correspond to the SAT solutions. The mapping\npresented allows to develop a new class of algorithms to solve SAT. Moreover,\nthis new approach suggests new ways to combine symbolic and connectionist\ncomputation and provides a general framework for local search algorithms."}, "authors": ["Andrea Roli", "Michela Milano"], "author_detail": {"name": "Michela Milano"}, "author": "Michela Milano", "arxiv_comment": "12 pages, 6 figures, 2 tables", "links": [{"href": "http://arxiv.org/abs/1101.6009v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1101.6009v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.NE", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "nlin.CG", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1101.6009v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1101.6009v1", "journal_reference": "In Evelina Lamma and Paola Mello (eds.), AI*IA99: Advances in\n  Artificial Intelligence, LNAI series, vol. 1792, pp. 72-83, Springer, 2000", "doi": null}
{"id": "http://arxiv.org/abs/2006.13843v1", "guidislink": true, "updated": "2020-06-24T16:13:10Z", "updated_parsed": [2020, 6, 24, 16, 13, 10, 2, 176, 0], "published": "2020-06-24T16:13:10Z", "published_parsed": [2020, 6, 24, 16, 13, 10, 2, 176, 0], "title": "Turbocharging Treewidth-Bounded Bayesian Network Structure Learning", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=30&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Turbocharging Treewidth-Bounded Bayesian Network Structure Learning"}, "summary": "We present a new approach for learning the structure of a treewidth-bounded\nBayesian Network (BN). The key to our approach is applying an exact method\n(based on MaxSAT) locally, to improve the score of a heuristically computed BN.\nThis approach allows us to scale the power of exact methods---so far only\napplicable to BNs with several dozens of nodes---to large BNs with several\nthousands of nodes. Our experiments show that our approach outperforms a\nstate-of-the-art heuristic method.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=30&max_results=10&sortBy=relevance&sortOrder=descending", "value": "We present a new approach for learning the structure of a treewidth-bounded\nBayesian Network (BN). The key to our approach is applying an exact method\n(based on MaxSAT) locally, to improve the score of a heuristically computed BN.\nThis approach allows us to scale the power of exact methods---so far only\napplicable to BNs with several dozens of nodes---to large BNs with several\nthousands of nodes. Our experiments show that our approach outperforms a\nstate-of-the-art heuristic method."}, "authors": ["Vaidyanathan P. R.", "Stefan Szeider"], "author_detail": {"name": "Stefan Szeider"}, "author": "Stefan Szeider", "arxiv_comment": "12 pages, 1 figure, 2 tables. Source code available at\n  https://www.ac.tuwien.ac.at/files/resources/software/bnslim.zip", "links": [{"href": "http://arxiv.org/abs/2006.13843v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2006.13843v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "I.2.6", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2006.13843v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2006.13843v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1803.08494v3", "guidislink": true, "updated": "2018-06-11T22:48:02Z", "updated_parsed": [2018, 6, 11, 22, 48, 2, 0, 162, 0], "published": "2018-03-22T17:57:16Z", "published_parsed": [2018, 3, 22, 17, 57, 16, 3, 81, 0], "title": "Group Normalization", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=40&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Group Normalization"}, "summary": "Batch Normalization (BN) is a milestone technique in the development of deep\nlearning, enabling various networks to train. However, normalizing along the\nbatch dimension introduces problems --- BN's error increases rapidly when the\nbatch size becomes smaller, caused by inaccurate batch statistics estimation.\nThis limits BN's usage for training larger models and transferring features to\ncomputer vision tasks including detection, segmentation, and video, which\nrequire small batches constrained by memory consumption. In this paper, we\npresent Group Normalization (GN) as a simple alternative to BN. GN divides the\nchannels into groups and computes within each group the mean and variance for\nnormalization. GN's computation is independent of batch sizes, and its accuracy\nis stable in a wide range of batch sizes. On ResNet-50 trained in ImageNet, GN\nhas 10.6% lower error than its BN counterpart when using a batch size of 2;\nwhen using typical batch sizes, GN is comparably good with BN and outperforms\nother normalization variants. Moreover, GN can be naturally transferred from\npre-training to fine-tuning. GN can outperform its BN-based counterparts for\nobject detection and segmentation in COCO, and for video classification in\nKinetics, showing that GN can effectively replace the powerful BN in a variety\nof tasks. GN can be easily implemented by a few lines of code in modern\nlibraries.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=40&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Batch Normalization (BN) is a milestone technique in the development of deep\nlearning, enabling various networks to train. However, normalizing along the\nbatch dimension introduces problems --- BN's error increases rapidly when the\nbatch size becomes smaller, caused by inaccurate batch statistics estimation.\nThis limits BN's usage for training larger models and transferring features to\ncomputer vision tasks including detection, segmentation, and video, which\nrequire small batches constrained by memory consumption. In this paper, we\npresent Group Normalization (GN) as a simple alternative to BN. GN divides the\nchannels into groups and computes within each group the mean and variance for\nnormalization. GN's computation is independent of batch sizes, and its accuracy\nis stable in a wide range of batch sizes. On ResNet-50 trained in ImageNet, GN\nhas 10.6% lower error than its BN counterpart when using a batch size of 2;\nwhen using typical batch sizes, GN is comparably good with BN and outperforms\nother normalization variants. Moreover, GN can be naturally transferred from\npre-training to fine-tuning. GN can outperform its BN-based counterparts for\nobject detection and segmentation in COCO, and for video classification in\nKinetics, showing that GN can effectively replace the powerful BN in a variety\nof tasks. GN can be easily implemented by a few lines of code in modern\nlibraries."}, "authors": ["Yuxin Wu", "Kaiming He"], "author_detail": {"name": "Kaiming He"}, "author": "Kaiming He", "arxiv_comment": "v3: Update trained-from-scratch results in COCO to 41.0AP. Code and\n  models at\n  https://github.com/facebookresearch/Detectron/blob/master/projects/GN", "links": [{"href": "http://arxiv.org/abs/1803.08494v3", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1803.08494v3", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1803.08494v3", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1803.08494v3", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1907.06916v2", "guidislink": true, "updated": "2019-07-22T13:04:27Z", "updated_parsed": [2019, 7, 22, 13, 4, 27, 0, 203, 0], "published": "2019-07-16T09:42:02Z", "published_parsed": [2019, 7, 16, 9, 42, 2, 1, 197, 0], "title": "Single-bit-per-weight deep convolutional neural networks without\n  batch-normalization layers for embedded systems", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=40&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Single-bit-per-weight deep convolutional neural networks without\n  batch-normalization layers for embedded systems"}, "summary": "Batch-normalization (BN) layers are thought to be an integrally important\nlayer type in today's state-of-the-art deep convolutional neural networks for\ncomputer vision tasks such as classification and detection. However, BN layers\nintroduce complexity and computational overheads that are highly undesirable\nfor training and/or inference on low-power custom hardware implementations of\nreal-time embedded vision systems such as UAVs, robots and Internet of Things\n(IoT) devices. They are also problematic when batch sizes need to be very small\nduring training, and innovations such as residual connections introduced more\nrecently than BN layers could potentially have lessened their impact. In this\npaper we aim to quantify the benefits BN layers offer in image classification\nnetworks, in comparison with alternative choices. In particular, we study\nnetworks that use shifted-ReLU layers instead of BN layers. We found, following\nexperiments with wide residual networks applied to the ImageNet, CIFAR 10 and\nCIFAR 100 image classification datasets, that BN layers do not consistently\noffer a significant advantage. We found that the accuracy margin offered by BN\nlayers depends on the data set, the network size, and the bit-depth of weights.\nWe conclude that in situations where BN layers are undesirable due to speed,\nmemory or complexity costs, that using shifted-ReLU layers instead should be\nconsidered; we found they can offer advantages in all these areas, and often do\nnot impose a significant accuracy cost.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=40&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Batch-normalization (BN) layers are thought to be an integrally important\nlayer type in today's state-of-the-art deep convolutional neural networks for\ncomputer vision tasks such as classification and detection. However, BN layers\nintroduce complexity and computational overheads that are highly undesirable\nfor training and/or inference on low-power custom hardware implementations of\nreal-time embedded vision systems such as UAVs, robots and Internet of Things\n(IoT) devices. They are also problematic when batch sizes need to be very small\nduring training, and innovations such as residual connections introduced more\nrecently than BN layers could potentially have lessened their impact. In this\npaper we aim to quantify the benefits BN layers offer in image classification\nnetworks, in comparison with alternative choices. In particular, we study\nnetworks that use shifted-ReLU layers instead of BN layers. We found, following\nexperiments with wide residual networks applied to the ImageNet, CIFAR 10 and\nCIFAR 100 image classification datasets, that BN layers do not consistently\noffer a significant advantage. We found that the accuracy margin offered by BN\nlayers depends on the data set, the network size, and the bit-depth of weights.\nWe conclude that in situations where BN layers are undesirable due to speed,\nmemory or complexity costs, that using shifted-ReLU layers instead should be\nconsidered; we found they can offer advantages in all these areas, and often do\nnot impose a significant accuracy cost."}, "authors": ["Mark D. McDonnell", "Hesham Mostafa", "Runchun Wang", "Andre van Schaik"], "author_detail": {"name": "Andre van Schaik"}, "author": "Andre van Schaik", "arxiv_comment": "8 pages, published IEEE conference paper", "links": [{"href": "http://arxiv.org/abs/1907.06916v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1907.06916v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.NE", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.ML", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1907.06916v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1907.06916v2", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/2001.02814v1", "guidislink": true, "updated": "2020-01-09T02:35:58Z", "updated_parsed": [2020, 1, 9, 2, 35, 58, 3, 9, 0], "published": "2020-01-09T02:35:58Z", "published_parsed": [2020, 1, 9, 2, 35, 58, 3, 9, 0], "title": "An Internal Covariate Shift Bounding Algorithm for Deep Neural Networks\n  by Unitizing Layers' Outputs", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=40&max_results=10&sortBy=relevance&sortOrder=descending", "value": "An Internal Covariate Shift Bounding Algorithm for Deep Neural Networks\n  by Unitizing Layers' Outputs"}, "summary": "Batch Normalization (BN) techniques have been proposed to reduce the\nso-called Internal Covariate Shift (ICS) by attempting to keep the\ndistributions of layer outputs unchanged. Experiments have shown their\neffectiveness on training deep neural networks. However, since only the first\ntwo moments are controlled in these BN techniques, it seems that a weak\nconstraint is imposed on layer distributions and furthermore whether such\nconstraint can reduce ICS is unknown. Thus this paper proposes a measure for\nICS by using the Earth Mover (EM) distance and then derives the upper and lower\nbounds for the measure to provide a theoretical analysis of BN. The upper bound\nhas shown that BN techniques can control ICS only for the outputs with low\ndimensions and small noise whereas their control is NOT effective in other\ncases. This paper also proves that such control is just a bounding of ICS\nrather than a reduction of ICS. Meanwhile, the analysis shows that the\nhigh-order moments and noise, which BN cannot control, have great impact on the\nlower bound. Based on such analysis, this paper furthermore proposes an\nalgorithm that unitizes the outputs with an adjustable parameter to further\nbound ICS in order to cope with the problems of BN. The upper bound for the\nproposed unitization is noise-free and only dominated by the parameter. Thus,\nthe parameter can be trained to tune the bound and further to control ICS.\nBesides, the unitization is embedded into the framework of BN to reduce the\ninformation loss. The experiments show that this proposed algorithm outperforms\nexisting BN techniques on CIFAR-10, CIFAR-100 and ImageNet datasets.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=40&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Batch Normalization (BN) techniques have been proposed to reduce the\nso-called Internal Covariate Shift (ICS) by attempting to keep the\ndistributions of layer outputs unchanged. Experiments have shown their\neffectiveness on training deep neural networks. However, since only the first\ntwo moments are controlled in these BN techniques, it seems that a weak\nconstraint is imposed on layer distributions and furthermore whether such\nconstraint can reduce ICS is unknown. Thus this paper proposes a measure for\nICS by using the Earth Mover (EM) distance and then derives the upper and lower\nbounds for the measure to provide a theoretical analysis of BN. The upper bound\nhas shown that BN techniques can control ICS only for the outputs with low\ndimensions and small noise whereas their control is NOT effective in other\ncases. This paper also proves that such control is just a bounding of ICS\nrather than a reduction of ICS. Meanwhile, the analysis shows that the\nhigh-order moments and noise, which BN cannot control, have great impact on the\nlower bound. Based on such analysis, this paper furthermore proposes an\nalgorithm that unitizes the outputs with an adjustable parameter to further\nbound ICS in order to cope with the problems of BN. The upper bound for the\nproposed unitization is noise-free and only dominated by the parameter. Thus,\nthe parameter can be trained to tune the bound and further to control ICS.\nBesides, the unitization is embedded into the framework of BN to reduce the\ninformation loss. The experiments show that this proposed algorithm outperforms\nexisting BN techniques on CIFAR-10, CIFAR-100 and ImageNet datasets."}, "authors": ["You Huang", "Yuanlong Yu"], "author_detail": {"name": "Yuanlong Yu"}, "author": "Yuanlong Yu", "arxiv_comment": "19 pages, 3 figures", "links": [{"href": "http://arxiv.org/abs/2001.02814v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2001.02814v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.ML", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2001.02814v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2001.02814v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/2001.11216v2", "guidislink": true, "updated": "2020-01-31T01:31:33Z", "updated_parsed": [2020, 1, 31, 1, 31, 33, 4, 31, 0], "published": "2020-01-30T09:00:08Z", "published_parsed": [2020, 1, 30, 9, 0, 8, 3, 30, 0], "title": "How Does BN Increase Collapsed Neural Network Filters?", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=40&max_results=10&sortBy=relevance&sortOrder=descending", "value": "How Does BN Increase Collapsed Neural Network Filters?"}, "summary": "Improving sparsity of deep neural networks (DNNs) is essential for network\ncompression and has drawn much attention. In this work, we disclose a harmful\nsparsifying process called filter collapse, which is common in DNNs with batch\nnormalization (BN) and rectified linear activation functions (e.g. ReLU, Leaky\nReLU). It occurs even without explicit sparsity-inducing regularizations such\nas $L_1$. This phenomenon is caused by the normalization effect of BN, which\ninduces a non-trainable region in the parameter space and reduces the network\ncapacity as a result. This phenomenon becomes more prominent when the network\nis trained with large learning rates (LR) or adaptive LR schedulers, and when\nthe network is finetuned. We analytically prove that the parameters of BN tend\nto become sparser during SGD updates with high gradient noise and that the\nsparsifying probability is proportional to the square of learning rate and\ninversely proportional to the square of the scale parameter of BN. To prevent\nthe undesirable collapsed filters, we propose a simple yet effective approach\nnamed post-shifted BN (psBN), which has the same representation ability as BN\nwhile being able to automatically make BN parameters trainable again as they\nsaturate during training. With psBN, we can recover collapsed filters and\nincrease the model performance in various tasks such as classification on\nCIFAR-10 and object detection on MS-COCO2017.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=40&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Improving sparsity of deep neural networks (DNNs) is essential for network\ncompression and has drawn much attention. In this work, we disclose a harmful\nsparsifying process called filter collapse, which is common in DNNs with batch\nnormalization (BN) and rectified linear activation functions (e.g. ReLU, Leaky\nReLU). It occurs even without explicit sparsity-inducing regularizations such\nas $L_1$. This phenomenon is caused by the normalization effect of BN, which\ninduces a non-trainable region in the parameter space and reduces the network\ncapacity as a result. This phenomenon becomes more prominent when the network\nis trained with large learning rates (LR) or adaptive LR schedulers, and when\nthe network is finetuned. We analytically prove that the parameters of BN tend\nto become sparser during SGD updates with high gradient noise and that the\nsparsifying probability is proportional to the square of learning rate and\ninversely proportional to the square of the scale parameter of BN. To prevent\nthe undesirable collapsed filters, we propose a simple yet effective approach\nnamed post-shifted BN (psBN), which has the same representation ability as BN\nwhile being able to automatically make BN parameters trainable again as they\nsaturate during training. With psBN, we can recover collapsed filters and\nincrease the model performance in various tasks such as classification on\nCIFAR-10 and object detection on MS-COCO2017."}, "authors": ["Sheng Zhou", "Xinjiang Wang", "Ping Luo", "Litong Feng", "Wenjie Li", "Wei Zhang"], "author_detail": {"name": "Wei Zhang"}, "author": "Wei Zhang", "links": [{"href": "http://arxiv.org/abs/2001.11216v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2001.11216v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.ML", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2001.11216v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2001.11216v2", "arxiv_comment": null, "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/2007.00364v2", "guidislink": true, "updated": "2020-07-02T08:03:45Z", "updated_parsed": [2020, 7, 2, 8, 3, 45, 3, 184, 0], "published": "2020-07-01T10:10:52Z", "published_parsed": [2020, 7, 1, 10, 10, 52, 2, 183, 0], "title": "Medical idioms for clinical Bayesian network development", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=40&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Medical idioms for clinical Bayesian network development"}, "summary": "Bayesian Networks (BNs) are graphical probabilistic models that have proven\npopular in medical applications. While numerous medical BNs have been\npublished, most are presented fait accompli without explanation of how the\nnetwork structure was developed or justification of why it represents the\ncorrect structure for the given medical application. This means that the\nprocess of building medical BNs from experts is typically ad hoc and offers\nlittle opportunity for methodological improvement. This paper proposes\ngenerally applicable and reusable medical reasoning patterns to aid those\ndeveloping medical BNs. The proposed method complements and extends the\nidiom-based approach introduced by Neil, Fenton, and Nielsen in 2000. We\npropose instances of their generic idioms that are specific to medical BNs. We\nrefer to the proposed medical reasoning patterns as medical idioms. In\naddition, we extend the use of idioms to represent interventional and\ncounterfactual reasoning. We believe that the proposed medical idioms are\nlogical reasoning patterns that can be combined, reused and applied generically\nto help develop medical BNs. All proposed medical idioms have been illustrated\nusing medical examples on coronary artery disease. The method has also been\napplied to other ongoing BNs being developed with medical experts. Finally, we\nshow that applying the proposed medical idioms to published BN models results\nin models with a clearer structure.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=40&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Bayesian Networks (BNs) are graphical probabilistic models that have proven\npopular in medical applications. While numerous medical BNs have been\npublished, most are presented fait accompli without explanation of how the\nnetwork structure was developed or justification of why it represents the\ncorrect structure for the given medical application. This means that the\nprocess of building medical BNs from experts is typically ad hoc and offers\nlittle opportunity for methodological improvement. This paper proposes\ngenerally applicable and reusable medical reasoning patterns to aid those\ndeveloping medical BNs. The proposed method complements and extends the\nidiom-based approach introduced by Neil, Fenton, and Nielsen in 2000. We\npropose instances of their generic idioms that are specific to medical BNs. We\nrefer to the proposed medical reasoning patterns as medical idioms. In\naddition, we extend the use of idioms to represent interventional and\ncounterfactual reasoning. We believe that the proposed medical idioms are\nlogical reasoning patterns that can be combined, reused and applied generically\nto help develop medical BNs. All proposed medical idioms have been illustrated\nusing medical examples on coronary artery disease. The method has also been\napplied to other ongoing BNs being developed with medical experts. Finally, we\nshow that applying the proposed medical idioms to published BN models results\nin models with a clearer structure."}, "authors": ["Evangelia Kyrimi", "Mariana Raniere Neves", "Scott McLachlan", "Martin Neil", "William Marsh", "Norman Fenton"], "author_detail": {"name": "Norman Fenton"}, "author": "Norman Fenton", "links": [{"title": "doi", "href": "http://dx.doi.org/10.1016/j.jbi.2020.103495", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/2007.00364v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2007.00364v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2007.00364v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2007.00364v2", "arxiv_comment": null, "journal_reference": null, "doi": "10.1016/j.jbi.2020.103495"}
{"id": "http://arxiv.org/abs/1501.01239v2", "guidislink": true, "updated": "2015-04-30T18:15:12Z", "updated_parsed": [2015, 4, 30, 18, 15, 12, 3, 120, 0], "published": "2015-01-06T17:14:11Z", "published_parsed": [2015, 1, 6, 17, 14, 11, 1, 6, 0], "title": "On the Relationship between Sum-Product Networks and Bayesian Networks", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=50&max_results=10&sortBy=relevance&sortOrder=descending", "value": "On the Relationship between Sum-Product Networks and Bayesian Networks"}, "summary": "In this paper, we establish some theoretical connections between Sum-Product\nNetworks (SPNs) and Bayesian Networks (BNs). We prove that every SPN can be\nconverted into a BN in linear time and space in terms of the network size. The\nkey insight is to use Algebraic Decision Diagrams (ADDs) to compactly represent\nthe local conditional probability distributions at each node in the resulting\nBN by exploiting context-specific independence (CSI). The generated BN has a\nsimple directed bipartite graphical structure. We show that by applying the\nVariable Elimination algorithm (VE) to the generated BN with ADD\nrepresentations, we can recover the original SPN where the SPN can be viewed as\na history record or caching of the VE inference process. To help state the\nproof clearly, we introduce the notion of {\\em normal} SPN and present a\ntheoretical analysis of the consistency and decomposability properties. We\nconclude the paper with some discussion of the implications of the proof and\nestablish a connection between the depth of an SPN and a lower bound of the\ntree-width of its corresponding BN.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=50&max_results=10&sortBy=relevance&sortOrder=descending", "value": "In this paper, we establish some theoretical connections between Sum-Product\nNetworks (SPNs) and Bayesian Networks (BNs). We prove that every SPN can be\nconverted into a BN in linear time and space in terms of the network size. The\nkey insight is to use Algebraic Decision Diagrams (ADDs) to compactly represent\nthe local conditional probability distributions at each node in the resulting\nBN by exploiting context-specific independence (CSI). The generated BN has a\nsimple directed bipartite graphical structure. We show that by applying the\nVariable Elimination algorithm (VE) to the generated BN with ADD\nrepresentations, we can recover the original SPN where the SPN can be viewed as\na history record or caching of the VE inference process. To help state the\nproof clearly, we introduce the notion of {\\em normal} SPN and present a\ntheoretical analysis of the consistency and decomposability properties. We\nconclude the paper with some discussion of the implications of the proof and\nestablish a connection between the depth of an SPN and a lower bound of the\ntree-width of its corresponding BN."}, "authors": ["Han Zhao", "Mazen Melibari", "Pascal Poupart"], "author_detail": {"name": "Pascal Poupart"}, "author": "Pascal Poupart", "arxiv_comment": "Full version of the same paper to appear at ICML-2015", "links": [{"href": "http://arxiv.org/abs/1501.01239v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1501.01239v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.ML", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1501.01239v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1501.01239v2", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1705.08011v2", "guidislink": true, "updated": "2019-02-19T17:19:55Z", "updated_parsed": [2019, 2, 19, 17, 19, 55, 1, 50, 0], "published": "2017-05-22T21:31:10Z", "published_parsed": [2017, 5, 22, 21, 31, 10, 0, 142, 0], "title": "Diminishing Batch Normalization", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=50&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Diminishing Batch Normalization"}, "summary": "In this paper, we propose a generalization of the Batch Normalization (BN)\nalgorithm, diminishing batch normalization (DBN), where we update the BN\nparameters in a diminishing moving average way. BN is very effective in\naccelerating the convergence of a neural network training phase that it has\nbecome a common practice. Our proposed DBN algorithm remains the overall\nstructure of the original BN algorithm while introduces a weighted averaging\nupdate to some trainable parameters. We provide an analysis of the convergence\nof the DBN algorithm that converges to a stationary point with respect to\ntrainable parameters. Our analysis can be easily generalized for original BN\nalgorithm by setting some parameters to constant. To the best knowledge of\nauthors, this analysis is the first of its kind for convergence with Batch\nNormalization introduced. We analyze a two-layer model with arbitrary\nactivation function. The primary challenge of the analysis is the fact that\nsome parameters are updated by gradient while others are not. The convergence\nanalysis applies to any activation function that satisfies our common\nassumptions. In the numerical experiments, we test the proposed algorithm on\ncomplex modern CNN models with stochastic gradients and ReLU activation. We\nobserve that DBN outperforms the original BN algorithm on MNIST, NI and\nCIFAR-10 datasets with reasonable complex FNN and CNN models.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=50&max_results=10&sortBy=relevance&sortOrder=descending", "value": "In this paper, we propose a generalization of the Batch Normalization (BN)\nalgorithm, diminishing batch normalization (DBN), where we update the BN\nparameters in a diminishing moving average way. BN is very effective in\naccelerating the convergence of a neural network training phase that it has\nbecome a common practice. Our proposed DBN algorithm remains the overall\nstructure of the original BN algorithm while introduces a weighted averaging\nupdate to some trainable parameters. We provide an analysis of the convergence\nof the DBN algorithm that converges to a stationary point with respect to\ntrainable parameters. Our analysis can be easily generalized for original BN\nalgorithm by setting some parameters to constant. To the best knowledge of\nauthors, this analysis is the first of its kind for convergence with Batch\nNormalization introduced. We analyze a two-layer model with arbitrary\nactivation function. The primary challenge of the analysis is the fact that\nsome parameters are updated by gradient while others are not. The convergence\nanalysis applies to any activation function that satisfies our common\nassumptions. In the numerical experiments, we test the proposed algorithm on\ncomplex modern CNN models with stochastic gradients and ReLU activation. We\nobserve that DBN outperforms the original BN algorithm on MNIST, NI and\nCIFAR-10 datasets with reasonable complex FNN and CNN models."}, "authors": ["Yintai Ma", "Diego Klabjan"], "author_detail": {"name": "Diego Klabjan"}, "author": "Diego Klabjan", "links": [{"href": "http://arxiv.org/abs/1705.08011v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1705.08011v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1705.08011v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1705.08011v2", "arxiv_comment": null, "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1802.10433v1", "guidislink": true, "updated": "2018-02-28T14:36:37Z", "updated_parsed": [2018, 2, 28, 14, 36, 37, 2, 59, 0], "published": "2018-02-28T14:36:37Z", "published_parsed": [2018, 2, 28, 14, 36, 37, 2, 59, 0], "title": "How long, O Bayesian network, will I sample thee? A program analysis\n  perspective on expected sampling times", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=50&max_results=10&sortBy=relevance&sortOrder=descending", "value": "How long, O Bayesian network, will I sample thee? A program analysis\n  perspective on expected sampling times"}, "summary": "Bayesian networks (BNs) are probabilistic graphical models for describing\ncomplex joint probability distributions. The main problem for BNs is inference:\nDetermine the probability of an event given observed evidence. Since exact\ninference is often infeasible for large BNs, popular approximate inference\nmethods rely on sampling.\n  We study the problem of determining the expected time to obtain a single\nvalid sample from a BN. To this end, we translate the BN together with\nobservations into a probabilistic program. We provide proof rules that yield\nthe exact expected runtime of this program in a fully automated fashion. We\nimplemented our approach and successfully analyzed various real-world BNs taken\nfrom the Bayesian network repository.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=50&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Bayesian networks (BNs) are probabilistic graphical models for describing\ncomplex joint probability distributions. The main problem for BNs is inference:\nDetermine the probability of an event given observed evidence. Since exact\ninference is often infeasible for large BNs, popular approximate inference\nmethods rely on sampling.\n  We study the problem of determining the expected time to obtain a single\nvalid sample from a BN. To this end, we translate the BN together with\nobservations into a probabilistic program. We provide proof rules that yield\nthe exact expected runtime of this program in a fully automated fashion. We\nimplemented our approach and successfully analyzed various real-world BNs taken\nfrom the Bayesian network repository."}, "authors": ["Kevin Batz", "Benjamin Lucien Kaminski", "Joost-Pieter Katoen", "Christoph Matheja"], "author_detail": {"name": "Christoph Matheja"}, "author": "Christoph Matheja", "links": [{"href": "http://arxiv.org/abs/1802.10433v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1802.10433v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.PL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.PL", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1802.10433v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1802.10433v1", "arxiv_comment": null, "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1908.02172v1", "guidislink": true, "updated": "2019-08-06T14:07:18Z", "updated_parsed": [2019, 8, 6, 14, 7, 18, 1, 218, 0], "published": "2019-08-06T14:07:18Z", "published_parsed": [2019, 8, 6, 14, 7, 18, 1, 218, 0], "title": "Bayesian Network Based Label Correlation Analysis For Multi-label\n  Classifier Chain", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=50&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Bayesian Network Based Label Correlation Analysis For Multi-label\n  Classifier Chain"}, "summary": "Classifier chain (CC) is a multi-label learning approach that constructs a\nsequence of binary classifiers according to a label order. Each classifier in\nthe sequence is responsible for predicting the relevance of one label. When\ntraining the classifier for a label, proceeding labels will be taken as\nextended features. If the extended features are highly correlated to the label,\nthe performance will be improved, otherwise, the performance will not be\ninfluenced or even degraded. How to discover label correlation and determine\nthe label order is critical for CC approach. This paper employs Bayesian\nnetwork (BN) to model the label correlations and proposes a new BN-based CC\nmethod (BNCC). First, conditional entropy is used to describe the dependency\nrelations among labels. Then, a BN is built up by taking nodes as labels and\nweights of edges as their dependency relations. A new scoring function is\nproposed to evaluate a BN structure, and a heuristic algorithm is introduced to\noptimize the BN. At last, by applying topological sorting on the nodes of the\noptimized BN, the label order for constructing CC model is derived.\nExperimental comparisons demonstrate the feasibility and effectiveness of the\nproposed method.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=50&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Classifier chain (CC) is a multi-label learning approach that constructs a\nsequence of binary classifiers according to a label order. Each classifier in\nthe sequence is responsible for predicting the relevance of one label. When\ntraining the classifier for a label, proceeding labels will be taken as\nextended features. If the extended features are highly correlated to the label,\nthe performance will be improved, otherwise, the performance will not be\ninfluenced or even degraded. How to discover label correlation and determine\nthe label order is critical for CC approach. This paper employs Bayesian\nnetwork (BN) to model the label correlations and proposes a new BN-based CC\nmethod (BNCC). First, conditional entropy is used to describe the dependency\nrelations among labels. Then, a BN is built up by taking nodes as labels and\nweights of edges as their dependency relations. A new scoring function is\nproposed to evaluate a BN structure, and a heuristic algorithm is introduced to\noptimize the BN. At last, by applying topological sorting on the nodes of the\noptimized BN, the label order for constructing CC model is derived.\nExperimental comparisons demonstrate the feasibility and effectiveness of the\nproposed method."}, "authors": ["Ran Wang", "Suhe Ye", "Ke Li", "Sam Kwong"], "author_detail": {"name": "Sam Kwong"}, "author": "Sam Kwong", "arxiv_comment": "27 pages", "links": [{"href": "http://arxiv.org/abs/1908.02172v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1908.02172v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.ML", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1908.02172v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1908.02172v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/2002.00224v2", "guidislink": true, "updated": "2020-02-04T07:18:20Z", "updated_parsed": [2020, 2, 4, 7, 18, 20, 1, 35, 0], "published": "2020-02-01T14:41:20Z", "published_parsed": [2020, 2, 1, 14, 41, 20, 5, 32, 0], "title": "Bayesian Networks in Healthcare: Distribution by Medical Condition", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=60&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Bayesian Networks in Healthcare: Distribution by Medical Condition"}, "summary": "Bayesian networks (BNs) have received increasing research attention that is\nnot matched by adoption in practice and yet have potential to significantly\nbenefit healthcare. Hitherto, research works have not investigated the types of\nmedical conditions being modelled with BNs, nor whether any differences exist\nin how and why they are applied to different conditions. This research seeks to\nidentify and quantify the range of medical conditions for which\nhealthcare-related BN models have been proposed, and the differences in\napproach between the most common medical conditions to which they have been\napplied. We found that almost two-thirds of all healthcare BNs are focused on\nfour conditions: cardiac, cancer, psychological and lung disorders. We believe\nthat a lack of understanding regarding how BNs work and what they are capable\nof exists, and that it is only with greater understanding and promotion that we\nmay ever realise the full potential of BNs to effect positive change in daily\nhealthcare practice.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=60&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Bayesian networks (BNs) have received increasing research attention that is\nnot matched by adoption in practice and yet have potential to significantly\nbenefit healthcare. Hitherto, research works have not investigated the types of\nmedical conditions being modelled with BNs, nor whether any differences exist\nin how and why they are applied to different conditions. This research seeks to\nidentify and quantify the range of medical conditions for which\nhealthcare-related BN models have been proposed, and the differences in\napproach between the most common medical conditions to which they have been\napplied. We found that almost two-thirds of all healthcare BNs are focused on\nfour conditions: cardiac, cancer, psychological and lung disorders. We believe\nthat a lack of understanding regarding how BNs work and what they are capable\nof exists, and that it is only with greater understanding and promotion that we\nmay ever realise the full potential of BNs to effect positive change in daily\nhealthcare practice."}, "authors": ["Scott McLachlan", "Kudakwashe Dube", "Graham A Hitman", "Norman E Fenton", "Evangelia Kyrimi"], "author_detail": {"name": "Evangelia Kyrimi"}, "author": "Evangelia Kyrimi", "links": [{"href": "http://arxiv.org/abs/2002.00224v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2002.00224v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CY", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CY", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2002.00224v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2002.00224v2", "arxiv_comment": null, "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1808.10240v2", "guidislink": true, "updated": "2020-04-08T08:12:21Z", "updated_parsed": [2020, 4, 8, 8, 12, 21, 2, 99, 0], "published": "2018-08-30T11:49:31Z", "published_parsed": [2018, 8, 30, 11, 49, 31, 3, 242, 0], "title": "Most Permissive Semantics of Boolean Networks", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=60&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Most Permissive Semantics of Boolean Networks"}, "summary": "As shown in (http://dx.doi.org/10.1101/2020.03.22.998377), the usual update\nmodes of Boolean networks (BNs), including synchronous and (generalized)\nasynchronous, fail to capture behaviors introduced by multivalued refinements.\nThus, update modes do not allow a correct abstract reasoning on dynamics of\nbiological systems, as they may lead to reject valid BN models.This technical\nreport lists the main definitions and properties of the most permissive\nsemantics of BNs introduced in http://dx.doi.org/10.1101/2020.03.22.998377.\nThis semantics meets with a correct abstraction of any multivalued refinements,\nwith any update mode. It subsumes all the usual updating modes, while enabling\nnew behaviors achievable by more concrete models. Moreover, it appears that\nclassical dynamical analyzes of reachability and attractors have a simpler\ncomputational complexity:- reachability can be assessed in a polynomial number\nof iterations. The computation of iterations is in NP in the very general case,\nand is linear when local functions are monotonic, or with some usual\nrepresentations of functions of BNs (binary decision diagrams, Petri nets,\nautomata networks, etc.). Thus, reachability is in P with locally-monotonic\nBNs, and P$^{\\text{NP}}$ otherwise (instead of being PSPACE-complete with\nupdate modes);- deciding wherever a configuration belongs to an attractor is in\ncoNP with locally-monotonic BNs, and coNP$^{\\text{coNP}}$ otherwise (instead of\nPSPACE-complete with update modes).Furthermore, we demonstrate that the\nsemantics completely captures any behavior achievable with any multilevel or\nODE refinement of the BN; and the semantics is minimal with respect to this\nmodel refinement criteria: to any most permissive trajectory, there exists a\nmultilevel refinement of the BN which can reproduce it.In brief, the most\npermissive semantics of BNs enables a correct abstract reasoning on dynamics of\nBNs, with a greater tractability than previously introduced update modes.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=60&max_results=10&sortBy=relevance&sortOrder=descending", "value": "As shown in (http://dx.doi.org/10.1101/2020.03.22.998377), the usual update\nmodes of Boolean networks (BNs), including synchronous and (generalized)\nasynchronous, fail to capture behaviors introduced by multivalued refinements.\nThus, update modes do not allow a correct abstract reasoning on dynamics of\nbiological systems, as they may lead to reject valid BN models.This technical\nreport lists the main definitions and properties of the most permissive\nsemantics of BNs introduced in http://dx.doi.org/10.1101/2020.03.22.998377.\nThis semantics meets with a correct abstraction of any multivalued refinements,\nwith any update mode. It subsumes all the usual updating modes, while enabling\nnew behaviors achievable by more concrete models. Moreover, it appears that\nclassical dynamical analyzes of reachability and attractors have a simpler\ncomputational complexity:- reachability can be assessed in a polynomial number\nof iterations. The computation of iterations is in NP in the very general case,\nand is linear when local functions are monotonic, or with some usual\nrepresentations of functions of BNs (binary decision diagrams, Petri nets,\nautomata networks, etc.). Thus, reachability is in P with locally-monotonic\nBNs, and P$^{\\text{NP}}$ otherwise (instead of being PSPACE-complete with\nupdate modes);- deciding wherever a configuration belongs to an attractor is in\ncoNP with locally-monotonic BNs, and coNP$^{\\text{coNP}}$ otherwise (instead of\nPSPACE-complete with update modes).Furthermore, we demonstrate that the\nsemantics completely captures any behavior achievable with any multilevel or\nODE refinement of the BN; and the semantics is minimal with respect to this\nmodel refinement criteria: to any most permissive trajectory, there exists a\nmultilevel refinement of the BN which can reproduce it.In brief, the most\npermissive semantics of BNs enables a correct abstract reasoning on dynamics of\nBNs, with a greater tractability than previously introduced update modes."}, "authors": ["Thomas Chatain", "Stefan Haar", "Juraj Kol{}k", "Loc Paulev"], "author_detail": {"name": "Loc Paulev"}, "author": "Loc Paulev", "links": [{"href": "http://arxiv.org/abs/1808.10240v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1808.10240v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.FL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.FL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LO", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "q-bio.QM", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1808.10240v2", "affiliation": "LaBRI, BioInfo - LRI", "arxiv_url": "http://arxiv.org/abs/1808.10240v2", "arxiv_comment": null, "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1307.3388v1", "guidislink": true, "updated": "2013-07-12T09:28:02Z", "updated_parsed": [2013, 7, 12, 9, 28, 2, 4, 193, 0], "published": "2013-07-12T09:28:02Z", "published_parsed": [2013, 7, 12, 9, 28, 2, 4, 193, 0], "title": "Dynamic networks reveal key players in aging", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=60&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Dynamic networks reveal key players in aging"}, "summary": "Motivation: Since susceptibility to diseases increases with age, studying\naging gains importance. Analyses of gene expression or sequence data, which\nhave been indispensable for investigating aging, have been limited to studying\ngenes and their protein products in isolation, ignoring their connectivities.\nHowever, proteins function by interacting with other proteins, and this is\nexactly what biological networks (BNs) model. Thus, analyzing the proteins' BN\ntopologies could contribute to understanding of aging. Current methods for\nanalyzing systems-level BNs deal with their static representations, even though\ncells are dynamic. For this reason, and because different data types can give\ncomplementary biological insights, we integrate current static BNs with\naging-related gene expression data to construct dynamic, age-specific BNs.\nThen, we apply sensitive measures of topology to the dynamic BNs to study\ncellular changes with age.\n  Results: While global BN topologies do not significantly change with age,\nlocal topologies of a number of genes do. We predict such genes as\naging-related. We demonstrate credibility of our predictions by: 1) observing\nsignificant overlap between our predicted aging-related genes and \"ground\ntruth\" aging-related genes; 2) showing that our aging-related predictions group\nby functions and diseases that are different than functions and diseases of\ngenes that are not predicted as aging-related; 3) observing significant overlap\nbetween functions and diseases that are enriched in our aging-related\npredictions and those that are enriched in \"ground truth\" aging-related data;\n4) providing evidence that diseases which are enriched in our aging-related\npredictions are linked to human aging; and 5) validating all of our\nhigh-scoring novel predictions via manual literature search.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=60&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Motivation: Since susceptibility to diseases increases with age, studying\naging gains importance. Analyses of gene expression or sequence data, which\nhave been indispensable for investigating aging, have been limited to studying\ngenes and their protein products in isolation, ignoring their connectivities.\nHowever, proteins function by interacting with other proteins, and this is\nexactly what biological networks (BNs) model. Thus, analyzing the proteins' BN\ntopologies could contribute to understanding of aging. Current methods for\nanalyzing systems-level BNs deal with their static representations, even though\ncells are dynamic. For this reason, and because different data types can give\ncomplementary biological insights, we integrate current static BNs with\naging-related gene expression data to construct dynamic, age-specific BNs.\nThen, we apply sensitive measures of topology to the dynamic BNs to study\ncellular changes with age.\n  Results: While global BN topologies do not significantly change with age,\nlocal topologies of a number of genes do. We predict such genes as\naging-related. We demonstrate credibility of our predictions by: 1) observing\nsignificant overlap between our predicted aging-related genes and \"ground\ntruth\" aging-related genes; 2) showing that our aging-related predictions group\nby functions and diseases that are different than functions and diseases of\ngenes that are not predicted as aging-related; 3) observing significant overlap\nbetween functions and diseases that are enriched in our aging-related\npredictions and those that are enriched in \"ground truth\" aging-related data;\n4) providing evidence that diseases which are enriched in our aging-related\npredictions are linked to human aging; and 5) validating all of our\nhigh-scoring novel predictions via manual literature search."}, "authors": ["Fazle Elahi Faisal", "Tijana Milenkovic"], "author_detail": {"name": "Tijana Milenkovic"}, "author": "Tijana Milenkovic", "links": [{"href": "http://arxiv.org/abs/1307.3388v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1307.3388v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CE", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CE", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "q-bio.MN", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1307.3388v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1307.3388v1", "arxiv_comment": null, "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/2003.07845v2", "guidislink": true, "updated": "2020-06-28T07:12:51Z", "updated_parsed": [2020, 6, 28, 7, 12, 51, 6, 180, 0], "published": "2020-03-17T17:50:26Z", "published_parsed": [2020, 3, 17, 17, 50, 26, 1, 77, 0], "title": "PowerNorm: Rethinking Batch Normalization in Transformers", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=70&max_results=10&sortBy=relevance&sortOrder=descending", "value": "PowerNorm: Rethinking Batch Normalization in Transformers"}, "summary": "The standard normalization method for neural network (NN) models used in\nNatural Language Processing (NLP) is layer normalization (LN). This is\ndifferent than batch normalization (BN), which is widely-adopted in Computer\nVision. The preferred use of LN in NLP is principally due to the empirical\nobservation that a (naive/vanilla) use of BN leads to significant performance\ndegradation for NLP tasks; however, a thorough understanding of the underlying\nreasons for this is not always evident. In this paper, we perform a systematic\nstudy of NLP transformer models to understand why BN has a poor performance, as\ncompared to LN. We find that the statistics of NLP data across the batch\ndimension exhibit large fluctuations throughout training. This results in\ninstability, if BN is naively implemented. To address this, we propose Power\nNormalization (PN), a novel normalization scheme that resolves this issue by\n(i) relaxing zero-mean normalization in BN, (ii) incorporating a running\nquadratic mean instead of per batch statistics to stabilize fluctuations, and\n(iii) using an approximate backpropagation for incorporating the running\nstatistics in the forward pass. We show theoretically, under mild assumptions,\nthat PN leads to a smaller Lipschitz constant for the loss, compared with BN.\nFurthermore, we prove that the approximate backpropagation scheme leads to\nbounded gradients. We extensively test PN for transformers on a range of NLP\ntasks, and we show that it significantly outperforms both LN and BN. In\nparticular, PN outperforms LN by 0.4/0.6 BLEU on IWSLT14/WMT14 and 5.6/3.0 PPL\non PTB/WikiText-103. We make our code publicly available at\n\\url{https://github.com/sIncerass/powernorm}.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=70&max_results=10&sortBy=relevance&sortOrder=descending", "value": "The standard normalization method for neural network (NN) models used in\nNatural Language Processing (NLP) is layer normalization (LN). This is\ndifferent than batch normalization (BN), which is widely-adopted in Computer\nVision. The preferred use of LN in NLP is principally due to the empirical\nobservation that a (naive/vanilla) use of BN leads to significant performance\ndegradation for NLP tasks; however, a thorough understanding of the underlying\nreasons for this is not always evident. In this paper, we perform a systematic\nstudy of NLP transformer models to understand why BN has a poor performance, as\ncompared to LN. We find that the statistics of NLP data across the batch\ndimension exhibit large fluctuations throughout training. This results in\ninstability, if BN is naively implemented. To address this, we propose Power\nNormalization (PN), a novel normalization scheme that resolves this issue by\n(i) relaxing zero-mean normalization in BN, (ii) incorporating a running\nquadratic mean instead of per batch statistics to stabilize fluctuations, and\n(iii) using an approximate backpropagation for incorporating the running\nstatistics in the forward pass. We show theoretically, under mild assumptions,\nthat PN leads to a smaller Lipschitz constant for the loss, compared with BN.\nFurthermore, we prove that the approximate backpropagation scheme leads to\nbounded gradients. We extensively test PN for transformers on a range of NLP\ntasks, and we show that it significantly outperforms both LN and BN. In\nparticular, PN outperforms LN by 0.4/0.6 BLEU on IWSLT14/WMT14 and 5.6/3.0 PPL\non PTB/WikiText-103. We make our code publicly available at\n\\url{https://github.com/sIncerass/powernorm}."}, "authors": ["Sheng Shen", "Zhewei Yao", "Amir Gholami", "Michael W. Mahoney", "Kurt Keutzer"], "author_detail": {"name": "Kurt Keutzer"}, "author": "Kurt Keutzer", "links": [{"href": "http://arxiv.org/abs/2003.07845v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2003.07845v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2003.07845v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2003.07845v2", "arxiv_comment": null, "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1202.4146v1", "guidislink": true, "updated": "2012-02-19T11:30:53Z", "updated_parsed": [2012, 2, 19, 11, 30, 53, 6, 50, 0], "published": "2012-02-19T11:30:53Z", "published_parsed": [2012, 2, 19, 11, 30, 53, 6, 50, 0], "title": "Bottleneck Non-Crossing Matching in the Plane", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=70&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Bottleneck Non-Crossing Matching in the Plane"}, "summary": "Let $P$ be a set of $2n$ points in the plane, and let $M_{\\rm C}$ (resp.,\n$M_{\\rm NC}$) denote a bottleneck matching (resp., a bottleneck non-crossing\nmatching) of $P$. We study the problem of computing $M_{\\rm NC}$. We first\nprove that the problem is NP-hard and does not admit a PTAS. Then, we present\nan $O(n^{1.5}\\log^{0.5} n)$-time algorithm that computes a non-crossing\nmatching $M$ of $P$, such that $bn(M) \\le 2\\sqrt{10} \\cdot bn(M_{\\rm NC})$,\nwhere $bn(M)$ is the length of a longest edge in $M$. An interesting\nimplication of our construction is that $bn(M_{\\rm NC})/bn(M_{\\rm C}) \\le\n2\\sqrt{10}$.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=70&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Let $P$ be a set of $2n$ points in the plane, and let $M_{\\rm C}$ (resp.,\n$M_{\\rm NC}$) denote a bottleneck matching (resp., a bottleneck non-crossing\nmatching) of $P$. We study the problem of computing $M_{\\rm NC}$. We first\nprove that the problem is NP-hard and does not admit a PTAS. Then, we present\nan $O(n^{1.5}\\log^{0.5} n)$-time algorithm that computes a non-crossing\nmatching $M$ of $P$, such that $bn(M) \\le 2\\sqrt{10} \\cdot bn(M_{\\rm NC})$,\nwhere $bn(M)$ is the length of a longest edge in $M$. An interesting\nimplication of our construction is that $bn(M_{\\rm NC})/bn(M_{\\rm C}) \\le\n2\\sqrt{10}$."}, "authors": ["A. Karim Abu-Affash", "Paz Carmi", "Matthew J. Katz", "Yohai Trabelsi"], "author_detail": {"name": "Yohai Trabelsi"}, "author": "Yohai Trabelsi", "arxiv_comment": "17 pages, 13 figures", "links": [{"href": "http://arxiv.org/abs/1202.4146v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1202.4146v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CG", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1202.4146v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1202.4146v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1301.0576v1", "guidislink": true, "updated": "2012-12-12T15:56:46Z", "updated_parsed": [2012, 12, 12, 15, 56, 46, 2, 347, 0], "published": "2012-12-12T15:56:46Z", "published_parsed": [2012, 12, 12, 15, 56, 46, 2, 347, 0], "title": "A Bayesian Network Scoring Metric That Is Based On Globally Uniform\n  Parameter Priors", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=80&max_results=10&sortBy=relevance&sortOrder=descending", "value": "A Bayesian Network Scoring Metric That Is Based On Globally Uniform\n  Parameter Priors"}, "summary": "We introduce a new Bayesian network (BN) scoring metric called the Global\nUniform (GU) metric. This metric is based on a particular type of default\nparameter prior. Such priors may be useful when a BN developer is not willing\nor able to specify domain-specific parameter priors. The GU parameter prior\nspecifies that every prior joint probability distribution P consistent with a\nBN structure S is considered to be equally likely. Distribution P is consistent\nwith S if P includes just the set of independence relations defined by S. We\nshow that the GU metric addresses some undesirable behavior of the BDeu and K2\nBayesian network scoring metrics, which also use particular forms of default\nparameter priors. A closed form formula for computing GU for special classes of\nBNs is derived. Efficiently computing GU for an arbitrary BN remains an open\nproblem.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=80&max_results=10&sortBy=relevance&sortOrder=descending", "value": "We introduce a new Bayesian network (BN) scoring metric called the Global\nUniform (GU) metric. This metric is based on a particular type of default\nparameter prior. Such priors may be useful when a BN developer is not willing\nor able to specify domain-specific parameter priors. The GU parameter prior\nspecifies that every prior joint probability distribution P consistent with a\nBN structure S is considered to be equally likely. Distribution P is consistent\nwith S if P includes just the set of independence relations defined by S. We\nshow that the GU metric addresses some undesirable behavior of the BDeu and K2\nBayesian network scoring metrics, which also use particular forms of default\nparameter priors. A closed form formula for computing GU for special classes of\nBNs is derived. Efficiently computing GU for an arbitrary BN remains an open\nproblem."}, "authors": ["Mehmet Kayaalp", "Gregory F. Cooper"], "author_detail": {"name": "Gregory F. Cooper"}, "author": "Gregory F. Cooper", "arxiv_comment": "Appears in Proceedings of the Eighteenth Conference on Uncertainty in\n  Artificial Intelligence (UAI2002)", "links": [{"href": "http://arxiv.org/abs/1301.0576v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1301.0576v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1301.0576v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1301.0576v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1301.2260v1", "guidislink": true, "updated": "2013-01-10T16:22:53Z", "updated_parsed": [2013, 1, 10, 16, 22, 53, 3, 10, 0], "published": "2013-01-10T16:22:53Z", "published_parsed": [2013, 1, 10, 16, 22, 53, 3, 10, 0], "title": "Confidence Inference in Bayesian Networks", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=80&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Confidence Inference in Bayesian Networks"}, "summary": "We present two sampling algorithms for probabilistic confidence inference in\nBayesian networks. These two algorithms (we call them AIS-BN-mu and\nAIS-BN-sigma algorithms) guarantee that estimates of posterior probabilities\nare with a given probability within a desired precision bound. Our algorithms\nare based on recent advances in sampling algorithms for (1) estimating the mean\nof bounded random variables and (2) adaptive importance sampling in Bayesian\nnetworks. In addition to a simple stopping rule for sampling that they provide,\nthe AIS-BN-mu and AIS-BN-sigma algorithms are capable of guiding the learning\nprocess in the AIS-BN algorithm. An empirical evaluation of the proposed\nalgorithms shows excellent performance, even for very unlikely evidence.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=80&max_results=10&sortBy=relevance&sortOrder=descending", "value": "We present two sampling algorithms for probabilistic confidence inference in\nBayesian networks. These two algorithms (we call them AIS-BN-mu and\nAIS-BN-sigma algorithms) guarantee that estimates of posterior probabilities\nare with a given probability within a desired precision bound. Our algorithms\nare based on recent advances in sampling algorithms for (1) estimating the mean\nof bounded random variables and (2) adaptive importance sampling in Bayesian\nnetworks. In addition to a simple stopping rule for sampling that they provide,\nthe AIS-BN-mu and AIS-BN-sigma algorithms are capable of guiding the learning\nprocess in the AIS-BN algorithm. An empirical evaluation of the proposed\nalgorithms shows excellent performance, even for very unlikely evidence."}, "authors": ["Jian Cheng", "Marek J. Druzdzel"], "author_detail": {"name": "Marek J. Druzdzel"}, "author": "Marek J. Druzdzel", "arxiv_comment": "Appears in Proceedings of the Seventeenth Conference on Uncertainty\n  in Artificial Intelligence (UAI2001)", "links": [{"href": "http://arxiv.org/abs/1301.2260v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1301.2260v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1301.2260v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1301.2260v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1301.6684v1", "guidislink": true, "updated": "2013-01-23T15:57:14Z", "updated_parsed": [2013, 1, 23, 15, 57, 14, 2, 23, 0], "published": "2013-01-23T15:57:14Z", "published_parsed": [2013, 1, 23, 15, 57, 14, 2, 23, 0], "title": "Comparing Bayesian Network Classifiers", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=80&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Comparing Bayesian Network Classifiers"}, "summary": "In this paper, we empirically evaluate algorithms for learning four types of\nBayesian network (BN) classifiers - Naive-Bayes, tree augmented Naive-Bayes, BN\naugmented Naive-Bayes and general BNs, where the latter two are learned using\ntwo variants of a conditional-independence (CI) based BN-learning algorithm.\nExperimental results show the obtained classifiers, learned using the CI based\nalgorithms, are competitive with (or superior to) the best known classifiers,\nbased on both Bayesian networks and other formalisms; and that the\ncomputational time for learning and using these classifiers is relatively\nsmall. Moreover, these results also suggest a way to learn yet more effective\nclassifiers; we demonstrate empirically that this new algorithm does work as\nexpected. Collectively, these results argue that BN classifiers deserve more\nattention in machine learning and data mining communities.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=80&max_results=10&sortBy=relevance&sortOrder=descending", "value": "In this paper, we empirically evaluate algorithms for learning four types of\nBayesian network (BN) classifiers - Naive-Bayes, tree augmented Naive-Bayes, BN\naugmented Naive-Bayes and general BNs, where the latter two are learned using\ntwo variants of a conditional-independence (CI) based BN-learning algorithm.\nExperimental results show the obtained classifiers, learned using the CI based\nalgorithms, are competitive with (or superior to) the best known classifiers,\nbased on both Bayesian networks and other formalisms; and that the\ncomputational time for learning and using these classifiers is relatively\nsmall. Moreover, these results also suggest a way to learn yet more effective\nclassifiers; we demonstrate empirically that this new algorithm does work as\nexpected. Collectively, these results argue that BN classifiers deserve more\nattention in machine learning and data mining communities."}, "authors": ["Jie Cheng", "Russell Greiner"], "author_detail": {"name": "Russell Greiner"}, "author": "Russell Greiner", "arxiv_comment": "Appears in Proceedings of the Fifteenth Conference on Uncertainty in\n  Artificial Intelligence (UAI1999)", "links": [{"href": "http://arxiv.org/abs/1301.6684v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1301.6684v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.ML", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1301.6684v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1301.6684v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1303.1473v1", "guidislink": true, "updated": "2013-03-06T14:20:12Z", "updated_parsed": [2013, 3, 6, 14, 20, 12, 2, 65, 0], "published": "2013-03-06T14:20:12Z", "published_parsed": [2013, 3, 6, 14, 20, 12, 2, 65, 0], "title": "Deriving a Minimal I-map of a Belief Network Relative to a Target\n  Ordering of its Nodes", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=80&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Deriving a Minimal I-map of a Belief Network Relative to a Target\n  Ordering of its Nodes"}, "summary": "This paper identifies and solves a new optimization problem: Given a belief\nnetwork (BN) and a target ordering on its variables, how can we efficiently\nderive its minimal I-map whose arcs are consistent with the target ordering? We\npresent three solutions to this problem, all of which lead to directed acyclic\ngraphs based on the original BN's recursive basis relative to the specified\nordering (such a DAG is sometimes termed the boundary DAG drawn from the given\nBN relative to the said ordering [5]). Along the way, we also uncover an\nimportant general principal about arc reversals: when reordering a BN according\nto some target ordering, (while attempting to minimize the number of arcs\ngenerated), the sequence of arc reversals should follow the topological\nordering induced by the original belief network's arcs to as great an extent as\npossible. These results promise to have a significant impact on the derivation\nof consensus models, as well as on other algorithms that require the\nreconfiguration and/or combination of BN's.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=80&max_results=10&sortBy=relevance&sortOrder=descending", "value": "This paper identifies and solves a new optimization problem: Given a belief\nnetwork (BN) and a target ordering on its variables, how can we efficiently\nderive its minimal I-map whose arcs are consistent with the target ordering? We\npresent three solutions to this problem, all of which lead to directed acyclic\ngraphs based on the original BN's recursive basis relative to the specified\nordering (such a DAG is sometimes termed the boundary DAG drawn from the given\nBN relative to the said ordering [5]). Along the way, we also uncover an\nimportant general principal about arc reversals: when reordering a BN according\nto some target ordering, (while attempting to minimize the number of arcs\ngenerated), the sequence of arc reversals should follow the topological\nordering induced by the original belief network's arcs to as great an extent as\npossible. These results promise to have a significant impact on the derivation\nof consensus models, as well as on other algorithms that require the\nreconfiguration and/or combination of BN's."}, "authors": ["Izhar Matzkevich", "Bruce Abramson"], "author_detail": {"name": "Bruce Abramson"}, "author": "Bruce Abramson", "arxiv_comment": "Appears in Proceedings of the Ninth Conference on Uncertainty in\n  Artificial Intelligence (UAI1993)", "links": [{"href": "http://arxiv.org/abs/1303.1473v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1303.1473v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1303.1473v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1303.1473v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1312.7542v1", "guidislink": true, "updated": "2013-12-29T14:38:24Z", "updated_parsed": [2013, 12, 29, 14, 38, 24, 6, 363, 0], "published": "2013-12-29T14:38:24Z", "published_parsed": [2013, 12, 29, 14, 38, 24, 6, 363, 0], "title": "Towards Connected Enterprises: The Business Network System", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=80&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Towards Connected Enterprises: The Business Network System"}, "summary": "The discovery, representation and reconstruction of Business Networks (BN)\nfrom Network Mining (NM) raw data is a difficult problem for enterprises. This\nis due to huge amounts of complex business processes within and across\nenterprise boundaries, heterogeneous technology stacks, and fragmented data. To\nremain competitive, visibility into the enterprise and partner networks on\ndifferent, interrelated abstraction levels is desirable. We present a novel\ndata discovery, mining and network inference system, called Business Network\nSystem (BNS), that reconstructs the BN--integration and business process\nnetworks--from raw data, hidden in the enterprises' landscapes. BNS provides a\nnew, declarative foundation for gathering information, defining a network\nmodel, inferring the network and check its conformance to the real-world\n\"as-is\" network. The paper covers both the foundation and the key features of\nBNS, including its underlying technologies, its overall system architecture,\nand its most interesting capabilities.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=80&max_results=10&sortBy=relevance&sortOrder=descending", "value": "The discovery, representation and reconstruction of Business Networks (BN)\nfrom Network Mining (NM) raw data is a difficult problem for enterprises. This\nis due to huge amounts of complex business processes within and across\nenterprise boundaries, heterogeneous technology stacks, and fragmented data. To\nremain competitive, visibility into the enterprise and partner networks on\ndifferent, interrelated abstraction levels is desirable. We present a novel\ndata discovery, mining and network inference system, called Business Network\nSystem (BNS), that reconstructs the BN--integration and business process\nnetworks--from raw data, hidden in the enterprises' landscapes. BNS provides a\nnew, declarative foundation for gathering information, defining a network\nmodel, inferring the network and check its conformance to the real-world\n\"as-is\" network. The paper covers both the foundation and the key features of\nBNS, including its underlying technologies, its overall system architecture,\nand its most interesting capabilities."}, "authors": ["Daniel Ritter"], "author_detail": {"name": "Daniel Ritter"}, "author": "Daniel Ritter", "arxiv_comment": "10 pages, 15. GI-Fachtagung Datenbanksysteme f\\\"ur Business,\n  Technologie und Web (BTW): Data Management in the Cloud (DMC), Magdeburg,\n  2013. arXiv admin note: text overlap with arXiv:1312.7436", "links": [{"href": "http://arxiv.org/abs/1312.7542v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1312.7542v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.DB", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.DB", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1312.7542v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1312.7542v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1812.03271v1", "guidislink": true, "updated": "2018-12-08T06:53:48Z", "updated_parsed": [2018, 12, 8, 6, 53, 48, 5, 342, 0], "published": "2018-12-08T06:53:48Z", "published_parsed": [2018, 12, 8, 6, 53, 48, 5, 342, 0], "title": "Generalized Batch Normalization: Towards Accelerating Deep Neural\n  Networks", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=90&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Generalized Batch Normalization: Towards Accelerating Deep Neural\n  Networks"}, "summary": "Utilizing recently introduced concepts from statistics and quantitative risk\nmanagement, we present a general variant of Batch Normalization (BN) that\noffers accelerated convergence of Neural Network training compared to\nconventional BN. In general, we show that mean and standard deviation are not\nalways the most appropriate choice for the centering and scaling procedure\nwithin the BN transformation, particularly if ReLU follows the normalization\nstep. We present a Generalized Batch Normalization (GBN) transformation, which\ncan utilize a variety of alternative deviation measures for scaling and\nstatistics for centering, choices which naturally arise from the theory of\ngeneralized deviation measures and risk theory in general. When used in\nconjunction with the ReLU non-linearity, the underlying risk theory suggests\nnatural, arguably optimal choices for the deviation measure and statistic.\nUtilizing the suggested deviation measure and statistic, we show experimentally\nthat training is accelerated more so than with conventional BN, often with\nimproved error rate as well. Overall, we propose a more flexible BN\ntransformation supported by a complimentary theoretical framework that can\npotentially guide design choices.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=90&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Utilizing recently introduced concepts from statistics and quantitative risk\nmanagement, we present a general variant of Batch Normalization (BN) that\noffers accelerated convergence of Neural Network training compared to\nconventional BN. In general, we show that mean and standard deviation are not\nalways the most appropriate choice for the centering and scaling procedure\nwithin the BN transformation, particularly if ReLU follows the normalization\nstep. We present a Generalized Batch Normalization (GBN) transformation, which\ncan utilize a variety of alternative deviation measures for scaling and\nstatistics for centering, choices which naturally arise from the theory of\ngeneralized deviation measures and risk theory in general. When used in\nconjunction with the ReLU non-linearity, the underlying risk theory suggests\nnatural, arguably optimal choices for the deviation measure and statistic.\nUtilizing the suggested deviation measure and statistic, we show experimentally\nthat training is accelerated more so than with conventional BN, often with\nimproved error rate as well. Overall, we propose a more flexible BN\ntransformation supported by a complimentary theoretical framework that can\npotentially guide design choices."}, "authors": ["Xiaoyong Yuan", "Zheng Feng", "Matthew Norton", "Xiaolin Li"], "author_detail": {"name": "Xiaolin Li"}, "author": "Xiaolin Li", "arxiv_comment": "accepted at AAAI-19", "links": [{"href": "http://arxiv.org/abs/1812.03271v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1812.03271v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.ML", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1812.03271v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1812.03271v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1904.06031v2", "guidislink": true, "updated": "2019-08-13T22:17:03Z", "updated_parsed": [2019, 8, 13, 22, 17, 3, 1, 225, 0], "published": "2019-04-12T04:54:56Z", "published_parsed": [2019, 4, 12, 4, 54, 56, 4, 102, 0], "title": "EvalNorm: Estimating Batch Normalization Statistics for Evaluation", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=90&max_results=10&sortBy=relevance&sortOrder=descending", "value": "EvalNorm: Estimating Batch Normalization Statistics for Evaluation"}, "summary": "Batch normalization (BN) has been very effective for deep learning and is\nwidely used. However, when training with small minibatches, models using BN\nexhibit a significant degradation in performance. In this paper we study this\npeculiar behavior of BN to gain a better understanding of the problem, and\nidentify a cause. We propose 'EvalNorm' to address the issue by estimating\ncorrected normalization statistics to use for BN during evaluation. EvalNorm\nsupports online estimation of the corrected statistics while the model is being\ntrained, and does not affect the training scheme of the model. As a result,\nEvalNorm can also be used with existing pre-trained models allowing them to\nbenefit from our method. EvalNorm yields large gains for models trained with\nsmaller batches. Our experiments show that EvalNorm performs 6.18% (absolute)\nbetter than vanilla BN for a batchsize of 2 on ImageNet validation set and from\n1.5 to 7.0 points (absolute) gain on the COCO object detection benchmark across\na variety of setups.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=90&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Batch normalization (BN) has been very effective for deep learning and is\nwidely used. However, when training with small minibatches, models using BN\nexhibit a significant degradation in performance. In this paper we study this\npeculiar behavior of BN to gain a better understanding of the problem, and\nidentify a cause. We propose 'EvalNorm' to address the issue by estimating\ncorrected normalization statistics to use for BN during evaluation. EvalNorm\nsupports online estimation of the corrected statistics while the model is being\ntrained, and does not affect the training scheme of the model. As a result,\nEvalNorm can also be used with existing pre-trained models allowing them to\nbenefit from our method. EvalNorm yields large gains for models trained with\nsmaller batches. Our experiments show that EvalNorm performs 6.18% (absolute)\nbetter than vanilla BN for a batchsize of 2 on ImageNet validation set and from\n1.5 to 7.0 points (absolute) gain on the COCO object detection benchmark across\na variety of setups."}, "authors": ["Saurabh Singh", "Abhinav Shrivastava"], "author_detail": {"name": "Abhinav Shrivastava"}, "author": "Abhinav Shrivastava", "arxiv_comment": "Accepted at ICCV 2019", "links": [{"href": "http://arxiv.org/abs/1904.06031v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1904.06031v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1904.06031v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1904.06031v2", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1905.13467v1", "guidislink": true, "updated": "2019-05-31T09:01:08Z", "updated_parsed": [2019, 5, 31, 9, 1, 8, 4, 151, 0], "published": "2019-05-31T09:01:08Z", "published_parsed": [2019, 5, 31, 9, 1, 8, 4, 151, 0], "title": "Concurrency in Boolean networks", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=90&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Concurrency in Boolean networks"}, "summary": "Boolean networks (BNs) are widely used to model the qualitative dynamics of\nbiological systems. Besides the logical rules determining the evolution of each\ncomponent with respect to the state of its regulators, the scheduling of\ncomponent updates can have a dramatic impact on the predicted behaviours. In\nthis paper, we explore the use of Read (contextual) Petri Nets (RPNs) to study\ndynamics of BNs from a concurrency theory perspective. After showing\nbi-directional translations between RPNs and BNs and analogies between results\non synchronism sensitivity, we illustrate that usual updating modes for BNs can\nmiss plausible behaviours, i.e., incorrectly conclude on the\nabsence/impossibility of reaching specific configurations. We propose an\nencoding of BNs capitalizing on the RPN semantics enabling more behaviour than\nthe generalized asynchronous updating mode. The proposed encoding ensures a\ncorrect abstraction of any multivalued refinement, as one may expect to achieve\nwhen modelling biological systems with no assumption on its time features.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=90&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Boolean networks (BNs) are widely used to model the qualitative dynamics of\nbiological systems. Besides the logical rules determining the evolution of each\ncomponent with respect to the state of its regulators, the scheduling of\ncomponent updates can have a dramatic impact on the predicted behaviours. In\nthis paper, we explore the use of Read (contextual) Petri Nets (RPNs) to study\ndynamics of BNs from a concurrency theory perspective. After showing\nbi-directional translations between RPNs and BNs and analogies between results\non synchronism sensitivity, we illustrate that usual updating modes for BNs can\nmiss plausible behaviours, i.e., incorrectly conclude on the\nabsence/impossibility of reaching specific configurations. We propose an\nencoding of BNs capitalizing on the RPN semantics enabling more behaviour than\nthe generalized asynchronous updating mode. The proposed encoding ensures a\ncorrect abstraction of any multivalued refinement, as one may expect to achieve\nwhen modelling biological systems with no assumption on its time features."}, "authors": ["Thomas Chatain", "Stefan Haar", "Juraj Kolk", "Loc Paulev", "Aalok Thakkar"], "author_detail": {"name": "Aalok Thakkar"}, "author": "Aalok Thakkar", "links": [{"title": "doi", "href": "http://dx.doi.org/10.1007/s11047-019-09748-4", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/1905.13467v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1905.13467v1", "rel": "related", "type": "application/pdf"}], "arxiv_comment": "Accepted in Natural Computing, 2019", "arxiv_primary_category": {"term": "cs.LO", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LO", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.DM", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.FL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "q-bio.MN", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1905.13467v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1905.13467v1", "journal_reference": null, "doi": "10.1007/s11047-019-09748-4"}
{"id": "http://arxiv.org/abs/2004.07507v1", "guidislink": true, "updated": "2020-04-16T07:58:47Z", "updated_parsed": [2020, 4, 16, 7, 58, 47, 3, 107, 0], "published": "2020-04-16T07:58:47Z", "published_parsed": [2020, 4, 16, 7, 58, 47, 3, 107, 0], "title": "Continual Learning with Extended Kronecker-factored Approximate\n  Curvature", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=90&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Continual Learning with Extended Kronecker-factored Approximate\n  Curvature"}, "summary": "We propose a quadratic penalty method for continual learning of neural\nnetworks that contain batch normalization (BN) layers. The Hessian of a loss\nfunction represents the curvature of the quadratic penalty function, and a\nKronecker-factored approximate curvature (K-FAC) is used widely to practically\ncompute the Hessian of a neural network. However, the approximation is not\nvalid if there is dependence between examples, typically caused by BN layers in\ndeep network architectures. We extend the K-FAC method so that the\ninter-example relations are taken into account and the Hessian of deep neural\nnetworks can be properly approximated under practical assumptions. We also\npropose a method of weight merging and reparameterization to properly handle\nstatistical parameters of BN, which plays a critical role for continual\nlearning with BN, and a method that selects hyperparameters without source task\ndata. Our method shows better performance than baselines in the permuted MNIST\ntask with BN layers and in sequential learning from the ImageNet classification\ntask to fine-grained classification tasks with ResNet-50, without any explicit\nor implicit use of source task data for hyperparameter selection.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=90&max_results=10&sortBy=relevance&sortOrder=descending", "value": "We propose a quadratic penalty method for continual learning of neural\nnetworks that contain batch normalization (BN) layers. The Hessian of a loss\nfunction represents the curvature of the quadratic penalty function, and a\nKronecker-factored approximate curvature (K-FAC) is used widely to practically\ncompute the Hessian of a neural network. However, the approximation is not\nvalid if there is dependence between examples, typically caused by BN layers in\ndeep network architectures. We extend the K-FAC method so that the\ninter-example relations are taken into account and the Hessian of deep neural\nnetworks can be properly approximated under practical assumptions. We also\npropose a method of weight merging and reparameterization to properly handle\nstatistical parameters of BN, which plays a critical role for continual\nlearning with BN, and a method that selects hyperparameters without source task\ndata. Our method shows better performance than baselines in the permuted MNIST\ntask with BN layers and in sequential learning from the ImageNet classification\ntask to fine-grained classification tasks with ResNet-50, without any explicit\nor implicit use of source task data for hyperparameter selection."}, "authors": ["Janghyeon Lee", "Hyeong Gwon Hong", "Donggyu Joo", "Junmo Kim"], "author_detail": {"name": "Junmo Kim"}, "author": "Junmo Kim", "arxiv_comment": "CVPR 2020", "links": [{"href": "http://arxiv.org/abs/2004.07507v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2004.07507v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.ML", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2004.07507v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2004.07507v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/2006.06926v2", "guidislink": true, "updated": "2020-06-17T10:03:31Z", "updated_parsed": [2020, 6, 17, 10, 3, 31, 2, 169, 0], "published": "2020-06-12T03:19:48Z", "published_parsed": [2020, 6, 12, 3, 19, 48, 4, 164, 0], "title": "Efficient Conversion of Bayesian Network Learning into Quadratic\n  Unconstrained Binary Optimization", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=90&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Efficient Conversion of Bayesian Network Learning into Quadratic\n  Unconstrained Binary Optimization"}, "summary": "Ising machines (IMs) are a potential breakthrough in the NP-hard problem of\nscore-based Bayesian network (BN) learning. To utilize the power of IMs,\nencoding of BN learning into quadratic unconstrained binary optimization (QUBO)\nhas been proposed using up to $\\mathcal{O}(N^2)$ bits, for $N$ variables in BN\nand $M = 2$ parents each. However, this approach is usually infeasible owing to\nthe upper bound of IM bits when $M \\geq 3$. In this paper, we propose an\nefficient conversion method for BN learning into QUBO with a maximum of $\\sum_n\n(\\Lambda_n - 1) + \\binom N2$ bits, for $\\Lambda_n$ parent set candidates each.\nThe advance selection of parent set candidates plays an essential role in\nreducing the number of required bits. We also develop a pre-processing\nalgorithm based on the capabilities of a classification and regression tree\n(CART), which allows us to search for parent set candidates consistent with\nscore minimization in a realistic timeframe.Our conversion method enables us to\nmore significantly reduce the upper bound of the required bits in comparison to\nan existing method, and is therefore expected to make a significant\ncontribution to the advancement of scalable score-based BN learning.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=90&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Ising machines (IMs) are a potential breakthrough in the NP-hard problem of\nscore-based Bayesian network (BN) learning. To utilize the power of IMs,\nencoding of BN learning into quadratic unconstrained binary optimization (QUBO)\nhas been proposed using up to $\\mathcal{O}(N^2)$ bits, for $N$ variables in BN\nand $M = 2$ parents each. However, this approach is usually infeasible owing to\nthe upper bound of IM bits when $M \\geq 3$. In this paper, we propose an\nefficient conversion method for BN learning into QUBO with a maximum of $\\sum_n\n(\\Lambda_n - 1) + \\binom N2$ bits, for $\\Lambda_n$ parent set candidates each.\nThe advance selection of parent set candidates plays an essential role in\nreducing the number of required bits. We also develop a pre-processing\nalgorithm based on the capabilities of a classification and regression tree\n(CART), which allows us to search for parent set candidates consistent with\nscore minimization in a realistic timeframe.Our conversion method enables us to\nmore significantly reduce the upper bound of the required bits in comparison to\nan existing method, and is therefore expected to make a significant\ncontribution to the advancement of scalable score-based BN learning."}, "authors": ["Yuta Shikuri"], "author_detail": {"name": "Yuta Shikuri"}, "author": "Yuta Shikuri", "arxiv_comment": "11 pages, 3 figures, NeurIPS 2020 (under review)", "links": [{"href": "http://arxiv.org/abs/2006.06926v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2006.06926v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.ML", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2006.06926v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2006.06926v2", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/2007.09450v2", "guidislink": true, "updated": "2020-07-26T08:03:08Z", "updated_parsed": [2020, 7, 26, 8, 3, 8, 6, 208, 0], "published": "2020-07-18T15:16:13Z", "published_parsed": [2020, 7, 18, 15, 16, 13, 5, 200, 0], "title": "Analysis of Bayesian Networks via Prob-Solvable Loops", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=90&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Analysis of Bayesian Networks via Prob-Solvable Loops"}, "summary": "Prob-solvable loops are probabilistic programs with polynomial assignments\nover random variables and parametrised distributions, for which the full\nautomation of moment-based invariant generation is decidable. In this paper we\nextend Prob-solvable loops with new features essential for encoding Bayesian\nnetworks (BNs). We show that various BNs, such as discrete, Gaussian,\nconditional linear Gaussian and dynamic BNs, can be naturally encoded as\nProb-solvable loops. Thanks to these encodings, we can automatically solve\nseveral BN related problems, including exact inference, sensitivity analysis,\nfiltering and computing the expected number of rejecting samples in\nsampling-based procedures. We evaluate our work on a number of BN benchmarks,\nusing automated invariant generation within Prob-solvable loop analysis.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=90&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Prob-solvable loops are probabilistic programs with polynomial assignments\nover random variables and parametrised distributions, for which the full\nautomation of moment-based invariant generation is decidable. In this paper we\nextend Prob-solvable loops with new features essential for encoding Bayesian\nnetworks (BNs). We show that various BNs, such as discrete, Gaussian,\nconditional linear Gaussian and dynamic BNs, can be naturally encoded as\nProb-solvable loops. Thanks to these encodings, we can automatically solve\nseveral BN related problems, including exact inference, sensitivity analysis,\nfiltering and computing the expected number of rejecting samples in\nsampling-based procedures. We evaluate our work on a number of BN benchmarks,\nusing automated invariant generation within Prob-solvable loop analysis."}, "authors": ["Ezio Bartocci", "Laura Kovcs", "Miroslav Stankovi"], "author_detail": {"name": "Miroslav Stankovi"}, "author": "Miroslav Stankovi", "links": [{"href": "http://arxiv.org/abs/2007.09450v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2007.09450v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.FL", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2007.09450v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2007.09450v2", "arxiv_comment": null, "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/2101.00072v1", "guidislink": true, "updated": "2020-12-31T21:07:56Z", "updated_parsed": [2020, 12, 31, 21, 7, 56, 3, 366, 0], "published": "2020-12-31T21:07:56Z", "published_parsed": [2020, 12, 31, 21, 7, 56, 3, 366, 0], "title": "Explicit regularization and implicit bias in deep network classifiers\n  trained with the square loss", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=90&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Explicit regularization and implicit bias in deep network classifiers\n  trained with the square loss"}, "summary": "Deep ReLU networks trained with the square loss have been observed to perform\nwell in classification tasks. We provide here a theoretical justification based\non analysis of the associated gradient flow. We show that convergence to a\nsolution with the absolute minimum norm is expected when normalization\ntechniques such as Batch Normalization (BN) or Weight Normalization (WN) are\nused together with Weight Decay (WD). The main property of the minimizers that\nbounds their expected error is the norm: we prove that among all the\nclose-to-interpolating solutions, the ones associated with smaller Frobenius\nnorms of the unnormalized weight matrices have better margin and better bounds\non the expected classification error. With BN but in the absence of WD, the\ndynamical system is singular. Implicit dynamical regularization -- that is\nzero-initial conditions biasing the dynamics towards high margin solutions --\nis also possible in the no-BN and no-WD case. The theory yields several\npredictions, including the role of BN and weight decay, aspects of Papyan, Han\nand Donoho's Neural Collapse and the constraints induced by BN on the network\nweights.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=90&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Deep ReLU networks trained with the square loss have been observed to perform\nwell in classification tasks. We provide here a theoretical justification based\non analysis of the associated gradient flow. We show that convergence to a\nsolution with the absolute minimum norm is expected when normalization\ntechniques such as Batch Normalization (BN) or Weight Normalization (WN) are\nused together with Weight Decay (WD). The main property of the minimizers that\nbounds their expected error is the norm: we prove that among all the\nclose-to-interpolating solutions, the ones associated with smaller Frobenius\nnorms of the unnormalized weight matrices have better margin and better bounds\non the expected classification error. With BN but in the absence of WD, the\ndynamical system is singular. Implicit dynamical regularization -- that is\nzero-initial conditions biasing the dynamics towards high margin solutions --\nis also possible in the no-BN and no-WD case. The theory yields several\npredictions, including the role of BN and weight decay, aspects of Papyan, Han\nand Donoho's Neural Collapse and the constraints induced by BN on the network\nweights."}, "authors": ["Tomaso Poggio", "Qianli Liao"], "author_detail": {"name": "Qianli Liao"}, "author": "Qianli Liao", "links": [{"href": "http://arxiv.org/abs/2101.00072v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2101.00072v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.ML", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2101.00072v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2101.00072v1", "arxiv_comment": null, "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/2101.02944v1", "guidislink": true, "updated": "2021-01-08T10:23:24Z", "updated_parsed": [2021, 1, 8, 10, 23, 24, 4, 8, 0], "published": "2021-01-08T10:23:24Z", "published_parsed": [2021, 1, 8, 10, 23, 24, 4, 8, 0], "title": "BN-invariant sharpness regularizes the training model to better\n  generalization", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=90&max_results=10&sortBy=relevance&sortOrder=descending", "value": "BN-invariant sharpness regularizes the training model to better\n  generalization"}, "summary": "It is arguably believed that flatter minima can generalize better. However,\nit has been pointed out that the usual definitions of sharpness, which consider\neither the maxima or the integral of loss over a $\\delta$ ball of parameters\naround minima, cannot give consistent measurement for scale invariant neural\nnetworks, e.g., networks with batch normalization layer. In this paper, we\nfirst propose a measure of sharpness, BN-Sharpness, which gives consistent\nvalue for equivalent networks under BN. It achieves the property of scale\ninvariance by connecting the integral diameter with the scale of parameter.\nThen we present a computation-efficient way to calculate the BN-sharpness\napproximately i.e., one dimensional integral along the \"sharpest\" direction.\nFurthermore, we use the BN-sharpness to regularize the training and design an\nalgorithm to minimize the new regularized objective. Our algorithm achieves\nconsiderably better performance than vanilla SGD over various experiment\nsettings.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=90&max_results=10&sortBy=relevance&sortOrder=descending", "value": "It is arguably believed that flatter minima can generalize better. However,\nit has been pointed out that the usual definitions of sharpness, which consider\neither the maxima or the integral of loss over a $\\delta$ ball of parameters\naround minima, cannot give consistent measurement for scale invariant neural\nnetworks, e.g., networks with batch normalization layer. In this paper, we\nfirst propose a measure of sharpness, BN-Sharpness, which gives consistent\nvalue for equivalent networks under BN. It achieves the property of scale\ninvariance by connecting the integral diameter with the scale of parameter.\nThen we present a computation-efficient way to calculate the BN-sharpness\napproximately i.e., one dimensional integral along the \"sharpest\" direction.\nFurthermore, we use the BN-sharpness to regularize the training and design an\nalgorithm to minimize the new regularized objective. Our algorithm achieves\nconsiderably better performance than vanilla SGD over various experiment\nsettings."}, "authors": ["Mingyang Yi", "Huishuai Zhang", "Wei Chen", "Zhi-Ming Ma", "Tie-Yan Liu"], "author_detail": {"name": "Tie-Yan Liu"}, "author": "Tie-Yan Liu", "links": [{"href": "http://arxiv.org/abs/2101.02944v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2101.02944v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2101.02944v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2101.02944v1", "arxiv_comment": null, "journal_reference": "Published in IJCAI2019", "doi": null}
{"id": "http://arxiv.org/abs/1206.6854v1", "guidislink": true, "updated": "2012-06-27T16:25:42Z", "updated_parsed": [2012, 6, 27, 16, 25, 42, 2, 179, 0], "published": "2012-06-27T16:25:42Z", "published_parsed": [2012, 6, 27, 16, 25, 42, 2, 179, 0], "title": "Belief Update in CLG Bayesian Networks With Lazy Propagation", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=110&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Belief Update in CLG Bayesian Networks With Lazy Propagation"}, "summary": "In recent years Bayesian networks (BNs) with a mixture of continuous and\ndiscrete variables have received an increasing level of attention. We present\nan architecture for exact belief update in Conditional Linear Gaussian BNs (CLG\nBNs). The architecture is an extension of lazy propagation using operations of\nLauritzen & Jensen [6] and Cowell [2]. By decomposing clique and separator\npotentials into sets of factors, the proposed architecture takes advantage of\nindependence and irrelevance properties induced by the structure of the graph\nand the evidence. The resulting benefits are illustrated by examples. Results\nof a preliminary empirical performance evaluation indicate a significant\npotential of the proposed architecture.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=110&max_results=10&sortBy=relevance&sortOrder=descending", "value": "In recent years Bayesian networks (BNs) with a mixture of continuous and\ndiscrete variables have received an increasing level of attention. We present\nan architecture for exact belief update in Conditional Linear Gaussian BNs (CLG\nBNs). The architecture is an extension of lazy propagation using operations of\nLauritzen & Jensen [6] and Cowell [2]. By decomposing clique and separator\npotentials into sets of factors, the proposed architecture takes advantage of\nindependence and irrelevance properties induced by the structure of the graph\nand the evidence. The resulting benefits are illustrated by examples. Results\nof a preliminary empirical performance evaluation indicate a significant\npotential of the proposed architecture."}, "authors": ["Anders L. Madsen"], "author_detail": {"name": "Anders L. Madsen"}, "author": "Anders L. Madsen", "arxiv_comment": "Appears in Proceedings of the Twenty-Second Conference on Uncertainty\n  in Artificial Intelligence (UAI2006)", "links": [{"href": "http://arxiv.org/abs/1206.6854v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1206.6854v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1206.6854v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1206.6854v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1301.7416v1", "guidislink": true, "updated": "2013-01-30T15:07:07Z", "updated_parsed": [2013, 1, 30, 15, 7, 7, 2, 30, 0], "published": "2013-01-30T15:07:07Z", "published_parsed": [2013, 1, 30, 15, 7, 7, 2, 30, 0], "title": "Probabilistic Inference in Influence Diagrams", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=110&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Probabilistic Inference in Influence Diagrams"}, "summary": "This paper is about reducing influence diagram (ID) evaluation into Bayesian\nnetwork (BN) inference problems. Such reduction is interesting because it\nenables one to readily use one's favorite BN inference algorithm to efficiently\nevaluate IDs. Two such reduction methods have been proposed previously (Cooper\n1988, Shachter and Peot 1992). This paper proposes a new method. The BN\ninference problems induced by the mew method are much easier to solve than\nthose induced by the two previous methods.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=110&max_results=10&sortBy=relevance&sortOrder=descending", "value": "This paper is about reducing influence diagram (ID) evaluation into Bayesian\nnetwork (BN) inference problems. Such reduction is interesting because it\nenables one to readily use one's favorite BN inference algorithm to efficiently\nevaluate IDs. Two such reduction methods have been proposed previously (Cooper\n1988, Shachter and Peot 1992). This paper proposes a new method. The BN\ninference problems induced by the mew method are much easier to solve than\nthose induced by the two previous methods."}, "authors": ["Nevin Lianwen Zhang"], "author_detail": {"name": "Nevin Lianwen Zhang"}, "author": "Nevin Lianwen Zhang", "arxiv_comment": "Appears in Proceedings of the Fourteenth Conference on Uncertainty in\n  Artificial Intelligence (UAI1998)", "links": [{"href": "http://arxiv.org/abs/1301.7416v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1301.7416v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1301.7416v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1301.7416v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1309.6825v2", "guidislink": true, "updated": "2015-03-23T13:56:08Z", "updated_parsed": [2015, 3, 23, 13, 56, 8, 0, 82, 0], "published": "2013-09-26T12:37:01Z", "published_parsed": [2013, 9, 26, 12, 37, 1, 3, 269, 0], "title": "Advances in Bayesian Network Learning using Integer Programming", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=110&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Advances in Bayesian Network Learning using Integer Programming"}, "summary": "We consider the problem of learning Bayesian networks (BNs) from complete\ndiscrete data. This problem of discrete optimisation is formulated as an\ninteger program (IP). We describe the various steps we have taken to allow\nefficient solving of this IP. These are (i) efficient search for cutting\nplanes, (ii) a fast greedy algorithm to find high-scoring (perhaps not optimal)\nBNs and (iii) tightening the linear relaxation of the IP. After relating this\nBN learning problem to set covering and the multidimensional 0-1 knapsack\nproblem, we present our empirical results. These show improvements, sometimes\ndramatic, over earlier results.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=110&max_results=10&sortBy=relevance&sortOrder=descending", "value": "We consider the problem of learning Bayesian networks (BNs) from complete\ndiscrete data. This problem of discrete optimisation is formulated as an\ninteger program (IP). We describe the various steps we have taken to allow\nefficient solving of this IP. These are (i) efficient search for cutting\nplanes, (ii) a fast greedy algorithm to find high-scoring (perhaps not optimal)\nBNs and (iii) tightening the linear relaxation of the IP. After relating this\nBN learning problem to set covering and the multidimensional 0-1 knapsack\nproblem, we present our empirical results. These show improvements, sometimes\ndramatic, over earlier results."}, "authors": ["Mark Bartlett", "James Cussens"], "author_detail": {"name": "James Cussens"}, "author": "James Cussens", "arxiv_comment": "Appears in Proceedings of the Twenty-Ninth Conference on Uncertainty\n  in Artificial Intelligence (UAI2013)", "links": [{"href": "http://arxiv.org/abs/1309.6825v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1309.6825v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1309.6825v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1309.6825v2", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1401.3429v1", "guidislink": true, "updated": "2014-01-15T04:46:37Z", "updated_parsed": [2014, 1, 15, 4, 46, 37, 2, 15, 0], "published": "2014-01-15T04:46:37Z", "published_parsed": [2014, 1, 15, 4, 46, 37, 2, 15, 0], "title": "Latent Tree Models and Approximate Inference in Bayesian Networks", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=110&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Latent Tree Models and Approximate Inference in Bayesian Networks"}, "summary": "We propose a novel method for approximate inference in Bayesian networks\n(BNs). The idea is to sample data from a BN, learn a latent tree model (LTM)\nfrom the data offline, and when online, make inference with the LTM instead of\nthe original BN. Because LTMs are tree-structured, inference takes linear time.\nIn the meantime, they can represent complex relationship among leaf nodes and\nhence the approximation accuracy is often good. Empirical evidence shows that\nour method can achieve good approximation accuracy at low online computational\ncost.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=110&max_results=10&sortBy=relevance&sortOrder=descending", "value": "We propose a novel method for approximate inference in Bayesian networks\n(BNs). The idea is to sample data from a BN, learn a latent tree model (LTM)\nfrom the data offline, and when online, make inference with the LTM instead of\nthe original BN. Because LTMs are tree-structured, inference takes linear time.\nIn the meantime, they can represent complex relationship among leaf nodes and\nhence the approximation accuracy is often good. Empirical evidence shows that\nour method can achieve good approximation accuracy at low online computational\ncost."}, "authors": ["Yi Wang", "Nevin L. Zhang", "Tao Chen"], "author_detail": {"name": "Tao Chen"}, "author": "Tao Chen", "links": [{"title": "doi", "href": "http://dx.doi.org/10.1613/jair.2530", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/1401.3429v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1401.3429v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1401.3429v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1401.3429v1", "arxiv_comment": null, "journal_reference": "Journal Of Artificial Intelligence Research, Volume 32, pages\n  879-900, 2008", "doi": "10.1613/jair.2530"}
{"id": "http://arxiv.org/abs/1711.07277v2", "guidislink": true, "updated": "2018-04-17T10:56:09Z", "updated_parsed": [2018, 4, 17, 10, 56, 9, 1, 107, 0], "published": "2017-11-20T12:12:45Z", "published_parsed": [2017, 11, 20, 12, 12, 45, 0, 324, 0], "title": "Backscatter Communications for the Internet of Things: A Stochastic\n  Geometry Approach", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=130&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Backscatter Communications for the Internet of Things: A Stochastic\n  Geometry Approach"}, "summary": "Motivated by the recent advances in the Internet of Things (IoT) and in\nWireless Power Transfer (WPT), we study a network architecture that consists of\npower beacons (PBs) and passive backscatter nodes (BNs). The PBs transmit a\nsinusoidal continuous wave (CW) and the BNs reflect back a portion of this\nsignal while harvesting the remaining part. A BN harvests energy from multiple\nnearby PBs and modulates its information bits on the composite CW through\nbackscatter modulation. The analysis poses real challenges due to the double\nfading channel, and its dependence on the PPPs of both the BNs and PBs.\nHowever, with the help of stochastic geometry, we derive the coverage\nprobability and the capacity of the network in tractable and easily computable\nexpressions, which depend on different system parameters. We observe that the\ncoverage probability decreases with an increase in the density of the BNs,\nwhile the capacity of the network improves. We further compare the performance\nof this network with a regular powered network in which the BNs have a reliable\npower source and show that for a very high density of the PBs, the coverage\nprobability of the former network approaches that of the regular powered\nnetwork.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=130&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Motivated by the recent advances in the Internet of Things (IoT) and in\nWireless Power Transfer (WPT), we study a network architecture that consists of\npower beacons (PBs) and passive backscatter nodes (BNs). The PBs transmit a\nsinusoidal continuous wave (CW) and the BNs reflect back a portion of this\nsignal while harvesting the remaining part. A BN harvests energy from multiple\nnearby PBs and modulates its information bits on the composite CW through\nbackscatter modulation. The analysis poses real challenges due to the double\nfading channel, and its dependence on the PPPs of both the BNs and PBs.\nHowever, with the help of stochastic geometry, we derive the coverage\nprobability and the capacity of the network in tractable and easily computable\nexpressions, which depend on different system parameters. We observe that the\ncoverage probability decreases with an increase in the density of the BNs,\nwhile the capacity of the network improves. We further compare the performance\nof this network with a regular powered network in which the BNs have a reliable\npower source and show that for a very high density of the PBs, the coverage\nprobability of the former network approaches that of the regular powered\nnetwork."}, "authors": ["Mudasar Bacha", "Bruno Clerckx"], "author_detail": {"name": "Bruno Clerckx"}, "author": "Bruno Clerckx", "arxiv_comment": "This work has been submitted for a possible journal publication", "links": [{"href": "http://arxiv.org/abs/1711.07277v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1711.07277v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.IT", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.IT", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "math.IT", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1711.07277v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1711.07277v2", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1704.02373v3", "guidislink": true, "updated": "2019-05-11T16:19:20Z", "updated_parsed": [2019, 5, 11, 16, 19, 20, 5, 131, 0], "published": "2017-04-06T09:37:41Z", "published_parsed": [2017, 4, 6, 9, 37, 41, 3, 96, 0], "title": "Time-Contrastive Learning Based DNN Bottleneck Features for\n  Text-Dependent Speaker Verification", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=130&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Time-Contrastive Learning Based DNN Bottleneck Features for\n  Text-Dependent Speaker Verification"}, "summary": "In this paper, we present a time-contrastive learning (TCL) based bottleneck\n(BN)feature extraction method for speech signals with an application to\ntext-dependent (TD) speaker verification (SV). It is well-known that speech\nsignals exhibit quasi-stationary behavior in and only in a short interval, and\nthe TCL method aims to exploit this temporal structure. More specifically, it\ntrains deep neural networks (DNNs) to discriminate temporal events obtained by\nuniformly segmenting speech signals, in contrast to existing DNN based BN\nfeature extraction methods that train DNNs using labeled data to discriminate\nspeakers or pass-phrases or phones or a combination of them. In the context of\nspeaker verification, speech data of fixed pass-phrases are used for TCL-BN\ntraining, while the pass-phrases used for TCL-BN training are excluded from\nbeing used for SV, so that the learned features can be considered generic. The\nmethod is evaluated on the RedDots Challenge 2016 database. Experimental\nresults show that TCL-BN is superior to the existing speaker and pass-phrase\ndiscriminant BN features and the Mel-frequency cepstral coefficient feature for\ntext-dependent speaker verification.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=130&max_results=10&sortBy=relevance&sortOrder=descending", "value": "In this paper, we present a time-contrastive learning (TCL) based bottleneck\n(BN)feature extraction method for speech signals with an application to\ntext-dependent (TD) speaker verification (SV). It is well-known that speech\nsignals exhibit quasi-stationary behavior in and only in a short interval, and\nthe TCL method aims to exploit this temporal structure. More specifically, it\ntrains deep neural networks (DNNs) to discriminate temporal events obtained by\nuniformly segmenting speech signals, in contrast to existing DNN based BN\nfeature extraction methods that train DNNs using labeled data to discriminate\nspeakers or pass-phrases or phones or a combination of them. In the context of\nspeaker verification, speech data of fixed pass-phrases are used for TCL-BN\ntraining, while the pass-phrases used for TCL-BN training are excluded from\nbeing used for SV, so that the learned features can be considered generic. The\nmethod is evaluated on the RedDots Challenge 2016 database. Experimental\nresults show that TCL-BN is superior to the existing speaker and pass-phrase\ndiscriminant BN features and the Mel-frequency cepstral coefficient feature for\ntext-dependent speaker verification."}, "authors": ["Achintya Kr. Sarkar", "Zheng-Hua Tan"], "author_detail": {"name": "Zheng-Hua Tan"}, "author": "Zheng-Hua Tan", "links": [{"href": "http://arxiv.org/abs/1704.02373v3", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1704.02373v3", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.SD", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.SD", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1704.02373v3", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1704.02373v3", "arxiv_comment": null, "journal_reference": "NIPS Time Series Workshop 2017, Long Beach, CA, USA", "doi": null}
{"id": "http://arxiv.org/abs/1707.03098v1", "guidislink": true, "updated": "2017-07-11T01:48:47Z", "updated_parsed": [2017, 7, 11, 1, 48, 47, 1, 192, 0], "published": "2017-07-11T01:48:47Z", "published_parsed": [2017, 7, 11, 1, 48, 47, 1, 192, 0], "title": "An Optimal Bayesian Network Based Solution Scheme for the Constrained\n  Stochastic On-line Equi-Partitioning Problem", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=130&max_results=10&sortBy=relevance&sortOrder=descending", "value": "An Optimal Bayesian Network Based Solution Scheme for the Constrained\n  Stochastic On-line Equi-Partitioning Problem"}, "summary": "A number of intriguing decision scenarios revolve around partitioning a\ncollection of objects to optimize some application specific objective function.\nThis problem is generally referred to as the Object Partitioning Problem (OPP)\nand is known to be NP-hard. We here consider a particularly challenging version\nof OPP, namely, the Stochastic On-line Equi-Partitioning Problem (SO-EPP). In\nSO-EPP, the target partitioning is unknown and has to be inferred purely from\nobserving an on-line sequence of object pairs. The paired objects belong to the\nsame partition with probability $p$ and to different partitions with\nprobability $1-p$, with $p$ also being unknown. As an additional complication,\nthe partitions are required to be of equal cardinality. Previously, only\nsub-optimal solution strategies have been proposed for SO- EPP. In this paper,\nwe propose the first optimal solution strategy. In brief, the scheme that we\npropose, BN-EPP, is founded on a Bayesian network representation of SO-EPP\nproblems. Based on probabilistic reasoning, we are not only able to infer the\nunderlying object partitioning with optimal accuracy. We are also able to\nsimultaneously infer $p$, allowing us to accelerate learning as object pairs\narrive. Furthermore, our scheme is the first to support arbitrary constraints\non the partitioning (Constrained SO-EPP). Being optimal, BN-EPP provides\nsuperior performance compared to existing solution schemes. We additionally\nintroduce Walk-BN-EPP, a novel WalkSAT inspired algorithm for solving large\nscale BN-EPP problems. Finally, we provide a BN-EPP based solution to the\nproblem of order picking, a representative real-life application of BN-EPP.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=130&max_results=10&sortBy=relevance&sortOrder=descending", "value": "A number of intriguing decision scenarios revolve around partitioning a\ncollection of objects to optimize some application specific objective function.\nThis problem is generally referred to as the Object Partitioning Problem (OPP)\nand is known to be NP-hard. We here consider a particularly challenging version\nof OPP, namely, the Stochastic On-line Equi-Partitioning Problem (SO-EPP). In\nSO-EPP, the target partitioning is unknown and has to be inferred purely from\nobserving an on-line sequence of object pairs. The paired objects belong to the\nsame partition with probability $p$ and to different partitions with\nprobability $1-p$, with $p$ also being unknown. As an additional complication,\nthe partitions are required to be of equal cardinality. Previously, only\nsub-optimal solution strategies have been proposed for SO- EPP. In this paper,\nwe propose the first optimal solution strategy. In brief, the scheme that we\npropose, BN-EPP, is founded on a Bayesian network representation of SO-EPP\nproblems. Based on probabilistic reasoning, we are not only able to infer the\nunderlying object partitioning with optimal accuracy. We are also able to\nsimultaneously infer $p$, allowing us to accelerate learning as object pairs\narrive. Furthermore, our scheme is the first to support arbitrary constraints\non the partitioning (Constrained SO-EPP). Being optimal, BN-EPP provides\nsuperior performance compared to existing solution schemes. We additionally\nintroduce Walk-BN-EPP, a novel WalkSAT inspired algorithm for solving large\nscale BN-EPP problems. Finally, we provide a BN-EPP based solution to the\nproblem of order picking, a representative real-life application of BN-EPP."}, "authors": ["Sondre Glimsdal", "Ole-Christoffer Granmo"], "author_detail": {"name": "Ole-Christoffer Granmo"}, "author": "Ole-Christoffer Granmo", "arxiv_comment": "15 pages, 7 figures", "links": [{"href": "http://arxiv.org/abs/1707.03098v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1707.03098v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1707.03098v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1707.03098v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1806.02375v4", "guidislink": true, "updated": "2018-11-30T05:56:20Z", "updated_parsed": [2018, 11, 30, 5, 56, 20, 4, 334, 0], "published": "2018-06-01T03:57:56Z", "published_parsed": [2018, 6, 1, 3, 57, 56, 4, 152, 0], "title": "Understanding Batch Normalization", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=130&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Understanding Batch Normalization"}, "summary": "Batch normalization (BN) is a technique to normalize activations in\nintermediate layers of deep neural networks. Its tendency to improve accuracy\nand speed up training have established BN as a favorite technique in deep\nlearning. Yet, despite its enormous success, there remains little consensus on\nthe exact reason and mechanism behind these improvements. In this paper we take\na step towards a better understanding of BN, following an empirical approach.\nWe conduct several experiments, and show that BN primarily enables training\nwith larger learning rates, which is the cause for faster convergence and\nbetter generalization. For networks without BN we demonstrate how large\ngradient updates can result in diverging loss and activations growing\nuncontrollably with network depth, which limits possible learning rates. BN\navoids this problem by constantly correcting activations to be zero-mean and of\nunit standard deviation, which enables larger gradient steps, yields faster\nconvergence and may help bypass sharp local minima. We further show various\nways in which gradients and activations of deep unnormalized networks are\nill-behaved. We contrast our results against recent findings in random matrix\ntheory, shedding new light on classical initialization schemes and their\nconsequences.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=130&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Batch normalization (BN) is a technique to normalize activations in\nintermediate layers of deep neural networks. Its tendency to improve accuracy\nand speed up training have established BN as a favorite technique in deep\nlearning. Yet, despite its enormous success, there remains little consensus on\nthe exact reason and mechanism behind these improvements. In this paper we take\na step towards a better understanding of BN, following an empirical approach.\nWe conduct several experiments, and show that BN primarily enables training\nwith larger learning rates, which is the cause for faster convergence and\nbetter generalization. For networks without BN we demonstrate how large\ngradient updates can result in diverging loss and activations growing\nuncontrollably with network depth, which limits possible learning rates. BN\navoids this problem by constantly correcting activations to be zero-mean and of\nunit standard deviation, which enables larger gradient steps, yields faster\nconvergence and may help bypass sharp local minima. We further show various\nways in which gradients and activations of deep unnormalized networks are\nill-behaved. We contrast our results against recent findings in random matrix\ntheory, shedding new light on classical initialization schemes and their\nconsequences."}, "authors": ["Johan Bjorck", "Carla Gomes", "Bart Selman", "Kilian Q. Weinberger"], "author_detail": {"name": "Kilian Q. Weinberger"}, "author": "Kilian Q. Weinberger", "links": [{"href": "http://arxiv.org/abs/1806.02375v4", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1806.02375v4", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.ML", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1806.02375v4", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1806.02375v4", "arxiv_comment": null, "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1806.02892v2", "guidislink": true, "updated": "2018-11-14T16:09:03Z", "updated_parsed": [2018, 11, 14, 16, 9, 3, 2, 318, 0], "published": "2018-06-07T20:41:09Z", "published_parsed": [2018, 6, 7, 20, 41, 9, 3, 158, 0], "title": "Training Faster by Separating Modes of Variation in Batch-normalized\n  Models", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=130&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Training Faster by Separating Modes of Variation in Batch-normalized\n  Models"}, "summary": "Batch Normalization (BN) is essential to effectively train state-of-the-art\ndeep Convolutional Neural Networks (CNN). It normalizes inputs to the layers\nduring training using the statistics of each mini-batch. In this work, we study\nBN from the viewpoint of Fisher kernels. We show that assuming samples within a\nmini-batch are from the same probability density function, then BN is identical\nto the Fisher vector of a Gaussian distribution. That means BN can be explained\nin terms of kernels that naturally emerge from the probability density function\nof the underlying data distribution. However, given the rectifying\nnon-linearities employed in CNN architectures, distribution of inputs to the\nlayers show heavy tail and asymmetric characteristics. Therefore, we propose\napproximating underlying data distribution not with one, but a mixture of\nGaussian densities. Deriving Fisher vector for a Gaussian Mixture Model (GMM),\nreveals that BN can be improved by independently normalizing with respect to\nthe statistics of disentangled sub-populations. We refer to our proposed soft\npiecewise version of BN as Mixture Normalization (MN). Through extensive set of\nexperiments on CIFAR-10 and CIFAR-100, we show that MN not only effectively\naccelerates training image classification and Generative Adversarial networks,\nbut also reaches higher quality models.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=130&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Batch Normalization (BN) is essential to effectively train state-of-the-art\ndeep Convolutional Neural Networks (CNN). It normalizes inputs to the layers\nduring training using the statistics of each mini-batch. In this work, we study\nBN from the viewpoint of Fisher kernels. We show that assuming samples within a\nmini-batch are from the same probability density function, then BN is identical\nto the Fisher vector of a Gaussian distribution. That means BN can be explained\nin terms of kernels that naturally emerge from the probability density function\nof the underlying data distribution. However, given the rectifying\nnon-linearities employed in CNN architectures, distribution of inputs to the\nlayers show heavy tail and asymmetric characteristics. Therefore, we propose\napproximating underlying data distribution not with one, but a mixture of\nGaussian densities. Deriving Fisher vector for a Gaussian Mixture Model (GMM),\nreveals that BN can be improved by independently normalizing with respect to\nthe statistics of disentangled sub-populations. We refer to our proposed soft\npiecewise version of BN as Mixture Normalization (MN). Through extensive set of\nexperiments on CIFAR-10 and CIFAR-100, we show that MN not only effectively\naccelerates training image classification and Generative Adversarial networks,\nbut also reaches higher quality models."}, "authors": ["Mahdi M. Kalayeh", "Mubarak Shah"], "author_detail": {"name": "Mubarak Shah"}, "author": "Mubarak Shah", "links": [{"href": "http://arxiv.org/abs/1806.02892v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1806.02892v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.ML", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1806.02892v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1806.02892v2", "arxiv_comment": null, "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/2010.04947v1", "guidislink": true, "updated": "2020-10-10T08:48:41Z", "updated_parsed": [2020, 10, 10, 8, 48, 41, 5, 284, 0], "published": "2020-10-10T08:48:41Z", "published_parsed": [2020, 10, 10, 8, 48, 41, 5, 284, 0], "title": "Double Forward Propagation for Memorized Batch Normalization", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=130&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Double Forward Propagation for Memorized Batch Normalization"}, "summary": "Batch Normalization (BN) has been a standard component in designing deep\nneural networks (DNNs). Although the standard BN can significantly accelerate\nthe training of DNNs and improve the generalization performance, it has several\nunderlying limitations which may hamper the performance in both training and\ninference. In the training stage, BN relies on estimating the mean and variance\nof data using a single minibatch. Consequently, BN can be unstable when the\nbatch size is very small or the data is poorly sampled. In the inference stage,\nBN often uses the so called moving mean and moving variance instead of batch\nstatistics, i.e., the training and inference rules in BN are not consistent.\nRegarding these issues, we propose a memorized batch normalization (MBN), which\nconsiders multiple recent batches to obtain more accurate and robust\nstatistics. Note that after the SGD update for each batch, the model parameters\nwill change, and the features will change accordingly, leading to the\nDistribution Shift before and after the update for the considered batch. To\nalleviate this issue, we present a simple Double-Forward scheme in MBN which\ncan further improve the performance. Compared to related methods, the proposed\nMBN exhibits consistent behaviors in both training and inference. Empirical\nresults show that the MBN based models trained with the Double-Forward scheme\ngreatly reduce the sensitivity of data and significantly improve the\ngeneralization performance.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=130&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Batch Normalization (BN) has been a standard component in designing deep\nneural networks (DNNs). Although the standard BN can significantly accelerate\nthe training of DNNs and improve the generalization performance, it has several\nunderlying limitations which may hamper the performance in both training and\ninference. In the training stage, BN relies on estimating the mean and variance\nof data using a single minibatch. Consequently, BN can be unstable when the\nbatch size is very small or the data is poorly sampled. In the inference stage,\nBN often uses the so called moving mean and moving variance instead of batch\nstatistics, i.e., the training and inference rules in BN are not consistent.\nRegarding these issues, we propose a memorized batch normalization (MBN), which\nconsiders multiple recent batches to obtain more accurate and robust\nstatistics. Note that after the SGD update for each batch, the model parameters\nwill change, and the features will change accordingly, leading to the\nDistribution Shift before and after the update for the considered batch. To\nalleviate this issue, we present a simple Double-Forward scheme in MBN which\ncan further improve the performance. Compared to related methods, the proposed\nMBN exhibits consistent behaviors in both training and inference. Empirical\nresults show that the MBN based models trained with the Double-Forward scheme\ngreatly reduce the sensitivity of data and significantly improve the\ngeneralization performance."}, "authors": ["Yong Guo", "Qingyao Wu", "Chaorui Deng", "Jian Chen", "Mingkui Tan"], "author_detail": {"name": "Mingkui Tan"}, "author": "Mingkui Tan", "arxiv_comment": "AAAI2018, 8 pages, 3 figures", "links": [{"href": "http://arxiv.org/abs/2010.04947v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2010.04947v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2010.04947v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2010.04947v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1202.3777v1", "guidislink": true, "updated": "2012-02-14T16:41:17Z", "updated_parsed": [2012, 2, 14, 16, 41, 17, 1, 45, 0], "published": "2012-02-14T16:41:17Z", "published_parsed": [2012, 2, 14, 16, 41, 17, 1, 45, 0], "title": "Belief Propagation by Message Passing in Junction Trees: Computing Each\n  Message Faster Using GPU Parallelization", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=150&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Belief Propagation by Message Passing in Junction Trees: Computing Each\n  Message Faster Using GPU Parallelization"}, "summary": "Compiling Bayesian networks (BNs) to junction trees and performing belief\npropagation over them is among the most prominent approaches to computing\nposteriors in BNs. However, belief propagation over junction tree is known to\nbe computationally intensive in the general case. Its complexity may increase\ndramatically with the connectivity and state space cardinality of Bayesian\nnetwork nodes. In this paper, we address this computational challenge using GPU\nparallelization. We develop data structures and algorithms that extend existing\njunction tree techniques, and specifically develop a novel approach to\ncomputing each belief propagation message in parallel. We implement our\napproach on an NVIDIA GPU and test it using BNs from several applications.\nExperimentally, we study how junction tree parameters affect parallelization\nopportunities and hence the performance of our algorithm. We achieve speedups\nranging from 0.68 to 9.18 for the BNs studied.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=150&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Compiling Bayesian networks (BNs) to junction trees and performing belief\npropagation over them is among the most prominent approaches to computing\nposteriors in BNs. However, belief propagation over junction tree is known to\nbe computationally intensive in the general case. Its complexity may increase\ndramatically with the connectivity and state space cardinality of Bayesian\nnetwork nodes. In this paper, we address this computational challenge using GPU\nparallelization. We develop data structures and algorithms that extend existing\njunction tree techniques, and specifically develop a novel approach to\ncomputing each belief propagation message in parallel. We implement our\napproach on an NVIDIA GPU and test it using BNs from several applications.\nExperimentally, we study how junction tree parameters affect parallelization\nopportunities and hence the performance of our algorithm. We achieve speedups\nranging from 0.68 to 9.18 for the BNs studied."}, "authors": ["Lu Zheng", "Ole Mengshoel", "Jike Chong"], "author_detail": {"name": "Jike Chong"}, "author": "Jike Chong", "links": [{"href": "http://arxiv.org/abs/1202.3777v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1202.3777v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.DC", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1202.3777v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1202.3777v1", "arxiv_comment": null, "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1206.3244v1", "guidislink": true, "updated": "2012-06-13T15:06:22Z", "updated_parsed": [2012, 6, 13, 15, 6, 22, 2, 165, 0], "published": "2012-06-13T15:06:22Z", "published_parsed": [2012, 6, 13, 15, 6, 22, 2, 165, 0], "title": "Bayesian network learning by compiling to weighted MAX-SAT", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=150&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Bayesian network learning by compiling to weighted MAX-SAT"}, "summary": "The problem of learning discrete Bayesian networks from data is encoded as a\nweighted MAX-SAT problem and the MaxWalkSat local search algorithm is used to\naddress it. For each dataset, the per-variable summands of the (BDeu) marginal\nlikelihood for different choices of parents ('family scores') are computed\nprior to applying MaxWalkSat. Each permissible choice of parents for each\nvariable is encoded as a distinct propositional atom and the associated family\nscore encoded as a 'soft' weighted single-literal clause. Two approaches to\nenforcing acyclicity are considered: either by encoding the ancestor relation\nor by attaching a total order to each graph and encoding that. The latter\napproach gives better results. Learning experiments have been conducted on 21\nsynthetic datasets sampled from 7 BNs. The largest dataset has 10,000\ndatapoints and 60 variables producing (for the 'ancestor' encoding) a weighted\nCNF input file with 19,932 atoms and 269,367 clauses. For most datasets,\nMaxWalkSat quickly finds BNs with higher BDeu score than the 'true' BN. The\neffect of adding prior information is assessed. It is further shown that\nBayesian model averaging can be effected by collecting BNs generated during the\nsearch.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=150&max_results=10&sortBy=relevance&sortOrder=descending", "value": "The problem of learning discrete Bayesian networks from data is encoded as a\nweighted MAX-SAT problem and the MaxWalkSat local search algorithm is used to\naddress it. For each dataset, the per-variable summands of the (BDeu) marginal\nlikelihood for different choices of parents ('family scores') are computed\nprior to applying MaxWalkSat. Each permissible choice of parents for each\nvariable is encoded as a distinct propositional atom and the associated family\nscore encoded as a 'soft' weighted single-literal clause. Two approaches to\nenforcing acyclicity are considered: either by encoding the ancestor relation\nor by attaching a total order to each graph and encoding that. The latter\napproach gives better results. Learning experiments have been conducted on 21\nsynthetic datasets sampled from 7 BNs. The largest dataset has 10,000\ndatapoints and 60 variables producing (for the 'ancestor' encoding) a weighted\nCNF input file with 19,932 atoms and 269,367 clauses. For most datasets,\nMaxWalkSat quickly finds BNs with higher BDeu score than the 'true' BN. The\neffect of adding prior information is assessed. It is further shown that\nBayesian model averaging can be effected by collecting BNs generated during the\nsearch."}, "authors": ["James Cussens"], "author_detail": {"name": "James Cussens"}, "author": "James Cussens", "arxiv_comment": "Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty\n  in Artificial Intelligence (UAI2008)", "links": [{"href": "http://arxiv.org/abs/1206.3244v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1206.3244v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1206.3244v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1206.3244v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1206.3293v1", "guidislink": true, "updated": "2012-06-13T15:52:10Z", "updated_parsed": [2012, 6, 13, 15, 52, 10, 2, 165, 0], "published": "2012-06-13T15:52:10Z", "published_parsed": [2012, 6, 13, 15, 52, 10, 2, 165, 0], "title": "Propagation using Chain Event Graphs", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=150&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Propagation using Chain Event Graphs"}, "summary": "A Chain Event Graph (CEG) is a graphial model which designed to embody\nconditional independencies in problems whose state spaces are highly asymmetric\nand do not admit a natural product structure. In this paer we present a\nprobability propagation algorithm which uses the topology of the CEG to build a\ntransporter CEG. Intriungly,the transporter CEG is directly analogous to the\ntriangulated Bayesian Network (BN) in the more conventional junction tree\npropagation algorithms used with BNs. The propagation method uses factorization\nformulae also analogous to (but different from) the ones using potentials on\ncliques and separators of the BN. It appears that the methods will be typically\nmore efficient than the BN algorithms when applied to contexts where there is\nsignificant asymmetry present.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=150&max_results=10&sortBy=relevance&sortOrder=descending", "value": "A Chain Event Graph (CEG) is a graphial model which designed to embody\nconditional independencies in problems whose state spaces are highly asymmetric\nand do not admit a natural product structure. In this paer we present a\nprobability propagation algorithm which uses the topology of the CEG to build a\ntransporter CEG. Intriungly,the transporter CEG is directly analogous to the\ntriangulated Bayesian Network (BN) in the more conventional junction tree\npropagation algorithms used with BNs. The propagation method uses factorization\nformulae also analogous to (but different from) the ones using potentials on\ncliques and separators of the BN. It appears that the methods will be typically\nmore efficient than the BN algorithms when applied to contexts where there is\nsignificant asymmetry present."}, "authors": ["Peter Thwaites", "Jim Q. Smith", "Robert G. Cowell"], "author_detail": {"name": "Robert G. Cowell"}, "author": "Robert G. Cowell", "arxiv_comment": "Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty\n  in Artificial Intelligence (UAI2008)", "links": [{"href": "http://arxiv.org/abs/1206.3293v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1206.3293v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1206.3293v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1206.3293v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1212.2507v1", "guidislink": true, "updated": "2012-10-19T15:08:55Z", "updated_parsed": [2012, 10, 19, 15, 8, 55, 4, 293, 0], "published": "2012-10-19T15:08:55Z", "published_parsed": [2012, 10, 19, 15, 8, 55, 4, 293, 0], "title": "An Importance Sampling Algorithm Based on Evidence Pre-propagation", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=150&max_results=10&sortBy=relevance&sortOrder=descending", "value": "An Importance Sampling Algorithm Based on Evidence Pre-propagation"}, "summary": "Precision achieved by stochastic sampling algorithms for Bayesian networks\ntypically deteriorates in face of extremely unlikely evidence. To address this\nproblem, we propose the Evidence Pre-propagation Importance Sampling algorithm\n(EPIS-BN), an importance sampling algorithm that computes an approximate\nimportance function by the heuristic methods: loopy belief Propagation and\ne-cutoff. We tested the performance of e-cutoff on three large real Bayesian\nnetworks: ANDES, CPCS, and PATHFINDER. We observed that on each of these\nnetworks the EPIS-BN algorithm gives us a considerable improvement over the\ncurrent state of the art algorithm, the AIS-BN algorithm. In addition, it\navoids the costly learning stage of the AIS-BN algorithm.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=150&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Precision achieved by stochastic sampling algorithms for Bayesian networks\ntypically deteriorates in face of extremely unlikely evidence. To address this\nproblem, we propose the Evidence Pre-propagation Importance Sampling algorithm\n(EPIS-BN), an importance sampling algorithm that computes an approximate\nimportance function by the heuristic methods: loopy belief Propagation and\ne-cutoff. We tested the performance of e-cutoff on three large real Bayesian\nnetworks: ANDES, CPCS, and PATHFINDER. We observed that on each of these\nnetworks the EPIS-BN algorithm gives us a considerable improvement over the\ncurrent state of the art algorithm, the AIS-BN algorithm. In addition, it\navoids the costly learning stage of the AIS-BN algorithm."}, "authors": ["Changhe Yuan", "Marek J. Druzdzel"], "author_detail": {"name": "Marek J. Druzdzel"}, "author": "Marek J. Druzdzel", "arxiv_comment": "Appears in Proceedings of the Nineteenth Conference on Uncertainty in\n  Artificial Intelligence (UAI2003)", "links": [{"href": "http://arxiv.org/abs/1212.2507v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1212.2507v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1212.2507v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1212.2507v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1301.6734v1", "guidislink": true, "updated": "2013-01-23T16:00:36Z", "updated_parsed": [2013, 1, 23, 16, 0, 36, 2, 23, 0], "published": "2013-01-23T16:00:36Z", "published_parsed": [2013, 1, 23, 16, 0, 36, 2, 23, 0], "title": "Bayesian Networks for Dependability Analysis: an Application to Digital\n  Control Reliability", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=150&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Bayesian Networks for Dependability Analysis: an Application to Digital\n  Control Reliability"}, "summary": "Bayesian Networks (BN) provide robust probabilistic methods of reasoning\nunder uncertainty, but despite their formal grounds are strictly based on the\nnotion of conditional dependence, not much attention has been paid so far to\ntheir use in dependability analysis. The aim of this paper is to propose BN as\na suitable tool for dependability analysis, by challenging the formalism with\nbasic issues arising in dependability tasks. We will discuss how both modeling\nand analysis issues can be naturally dealt with by BN. Moreover, we will show\nhow some limitations intrinsic to combinatorial dependability methods such as\nFault Trees can be overcome using BN. This will be pursued through the study of\na real-world example concerning the reliability analysis of a redundant digital\nProgrammable Logic Controller (PLC) with majority voting 2:3", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=150&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Bayesian Networks (BN) provide robust probabilistic methods of reasoning\nunder uncertainty, but despite their formal grounds are strictly based on the\nnotion of conditional dependence, not much attention has been paid so far to\ntheir use in dependability analysis. The aim of this paper is to propose BN as\na suitable tool for dependability analysis, by challenging the formalism with\nbasic issues arising in dependability tasks. We will discuss how both modeling\nand analysis issues can be naturally dealt with by BN. Moreover, we will show\nhow some limitations intrinsic to combinatorial dependability methods such as\nFault Trees can be overcome using BN. This will be pursued through the study of\na real-world example concerning the reliability analysis of a redundant digital\nProgrammable Logic Controller (PLC) with majority voting 2:3"}, "authors": ["Luigi Portinale", "Andrea Bobbio"], "author_detail": {"name": "Andrea Bobbio"}, "author": "Andrea Bobbio", "arxiv_comment": "Appears in Proceedings of the Fifteenth Conference on Uncertainty in\n  Artificial Intelligence (UAI1999)", "links": [{"href": "http://arxiv.org/abs/1301.6734v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1301.6734v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1301.6734v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1301.6734v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1301.7403v1", "guidislink": true, "updated": "2013-01-30T15:06:05Z", "updated_parsed": [2013, 1, 30, 15, 6, 5, 2, 30, 0], "published": "2013-01-30T15:06:05Z", "published_parsed": [2013, 1, 30, 15, 6, 5, 2, 30, 0], "title": "A Multivariate Discretization Method for Learning Bayesian Networks from\n  Mixed Data", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=150&max_results=10&sortBy=relevance&sortOrder=descending", "value": "A Multivariate Discretization Method for Learning Bayesian Networks from\n  Mixed Data"}, "summary": "In this paper we address the problem of discretization in the context of\nlearning Bayesian networks (BNs) from data containing both continuous and\ndiscrete variables. We describe a new technique for <EM>multivariate</EM>\ndiscretization, whereby each continuous variable is discretized while taking\ninto account its interaction with the other variables. The technique is based\non the use of a Bayesian scoring metric that scores the discretization policy\nfor a continuous variable given a BN structure and the observed data. Since the\nmetric is relative to the BN structure currently being evaluated, the\ndiscretization of a variable needs to be dynamically adjusted as the BN\nstructure changes.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=150&max_results=10&sortBy=relevance&sortOrder=descending", "value": "In this paper we address the problem of discretization in the context of\nlearning Bayesian networks (BNs) from data containing both continuous and\ndiscrete variables. We describe a new technique for <EM>multivariate</EM>\ndiscretization, whereby each continuous variable is discretized while taking\ninto account its interaction with the other variables. The technique is based\non the use of a Bayesian scoring metric that scores the discretization policy\nfor a continuous variable given a BN structure and the observed data. Since the\nmetric is relative to the BN structure currently being evaluated, the\ndiscretization of a variable needs to be dynamically adjusted as the BN\nstructure changes."}, "authors": ["Stefano Monti", "Gregory F. Cooper"], "author_detail": {"name": "Gregory F. Cooper"}, "author": "Gregory F. Cooper", "arxiv_comment": "Appears in Proceedings of the Fourteenth Conference on Uncertainty in\n  Artificial Intelligence (UAI1998)", "links": [{"href": "http://arxiv.org/abs/1301.7403v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1301.7403v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1301.7403v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1301.7403v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1302.1542v1", "guidislink": true, "updated": "2013-02-06T15:55:43Z", "updated_parsed": [2013, 2, 6, 15, 55, 43, 2, 37, 0], "published": "2013-02-06T15:55:43Z", "published_parsed": [2013, 2, 6, 15, 55, 43, 2, 37, 0], "title": "Learning Bayesian Nets that Perform Well", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=160&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Learning Bayesian Nets that Perform Well"}, "summary": "A Bayesian net (BN) is more than a succinct way to encode a probabilistic\ndistribution; it also corresponds to a function used to answer queries. A BN\ncan therefore be evaluated by the accuracy of the answers it returns. Many\nalgorithms for learning BNs, however, attempt to optimize another criterion\n(usually likelihood, possibly augmented with a regularizing term), which is\nindependent of the distribution of queries that are posed. This paper takes the\n\"performance criteria\" seriously, and considers the challenge of computing the\nBN whose performance - read \"accuracy over the distribution of queries\" - is\noptimal. We show that many aspects of this learning task are more difficult\nthan the corresponding subtasks in the standard model.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=160&max_results=10&sortBy=relevance&sortOrder=descending", "value": "A Bayesian net (BN) is more than a succinct way to encode a probabilistic\ndistribution; it also corresponds to a function used to answer queries. A BN\ncan therefore be evaluated by the accuracy of the answers it returns. Many\nalgorithms for learning BNs, however, attempt to optimize another criterion\n(usually likelihood, possibly augmented with a regularizing term), which is\nindependent of the distribution of queries that are posed. This paper takes the\n\"performance criteria\" seriously, and considers the challenge of computing the\nBN whose performance - read \"accuracy over the distribution of queries\" - is\noptimal. We show that many aspects of this learning task are more difficult\nthan the corresponding subtasks in the standard model."}, "authors": ["Russell Greiner", "Adam J. Grove", "Dale Schuurmans"], "author_detail": {"name": "Dale Schuurmans"}, "author": "Dale Schuurmans", "arxiv_comment": "Appears in Proceedings of the Thirteenth Conference on Uncertainty in\n  Artificial Intelligence (UAI1997)", "links": [{"href": "http://arxiv.org/abs/1302.1542v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1302.1542v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1302.1542v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1302.1542v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1302.6779v1", "guidislink": true, "updated": "2013-02-27T14:13:16Z", "updated_parsed": [2013, 2, 27, 14, 13, 16, 2, 58, 0], "published": "2013-02-27T14:13:16Z", "published_parsed": [2013, 2, 27, 14, 13, 16, 2, 58, 0], "title": "An Evaluation of an Algorithm for Inductive Learning of Bayesian Belief\n  Networks Usin", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=160&max_results=10&sortBy=relevance&sortOrder=descending", "value": "An Evaluation of an Algorithm for Inductive Learning of Bayesian Belief\n  Networks Usin"}, "summary": "Bayesian learning of belief networks (BLN) is a method for automatically\nconstructing belief networks (BNs) from data using search and Bayesian scoring\ntechniques. K2 is a particular instantiation of the method that implements a\ngreedy search strategy. To evaluate the accuracy of K2, we randomly generated a\nnumber of BNs and for each of those we simulated data sets. K2 was then used to\ninduce the generating BNs from the simulated data. We examine the performance\nof the program, and the factors that influence it. We also present a simple BN\nmodel, developed from our results, which predicts the accuracy of K2, when\ngiven various characteristics of the data set.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=160&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Bayesian learning of belief networks (BLN) is a method for automatically\nconstructing belief networks (BNs) from data using search and Bayesian scoring\ntechniques. K2 is a particular instantiation of the method that implements a\ngreedy search strategy. To evaluate the accuracy of K2, we randomly generated a\nnumber of BNs and for each of those we simulated data sets. K2 was then used to\ninduce the generating BNs from the simulated data. We examine the performance\nof the program, and the factors that influence it. We also present a simple BN\nmodel, developed from our results, which predicts the accuracy of K2, when\ngiven various characteristics of the data set."}, "authors": ["Constantin F. Aliferis", "Gregory F. Cooper"], "author_detail": {"name": "Gregory F. Cooper"}, "author": "Gregory F. Cooper", "arxiv_comment": "Appears in Proceedings of the Tenth Conference on Uncertainty in\n  Artificial Intelligence (UAI1994)", "links": [{"href": "http://arxiv.org/abs/1302.6779v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1302.6779v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1302.6779v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1302.6779v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1312.7436v1", "guidislink": true, "updated": "2013-12-28T14:16:58Z", "updated_parsed": [2013, 12, 28, 14, 16, 58, 5, 362, 0], "published": "2013-12-28T14:16:58Z", "published_parsed": [2013, 12, 28, 14, 16, 58, 5, 362, 0], "title": "Advanced Data Processing in the Business Network System", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=160&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Advanced Data Processing in the Business Network System"}, "summary": "The discovery, representation and reconstruction of Business Networks (BN)\nfrom Network Mining (NM) raw data is a difficult problem for enterprises. This\nis due to huge amounts of e.g. complex business processes within and across\nenterprise boundaries, heterogeneous technology stacks, and fragmented data. To\nremain competitive, visibility into the enterprise and partner networks on\ndifferent, interrelated abstraction levels is desirable.\n  We show the query and data processing capabilities of a novel data discovery,\nmining and network inference system, called Business Network System (BNS) that\nreconstructs the BN--integration and business process networks - from raw data,\nhidden in the enterprises' landscapes. The paper covers both the foundation and\nthe key data processing characteristics features of BNS, including its\nunderlying technologies, its overall system architecture, and data provenance\napproach.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=160&max_results=10&sortBy=relevance&sortOrder=descending", "value": "The discovery, representation and reconstruction of Business Networks (BN)\nfrom Network Mining (NM) raw data is a difficult problem for enterprises. This\nis due to huge amounts of e.g. complex business processes within and across\nenterprise boundaries, heterogeneous technology stacks, and fragmented data. To\nremain competitive, visibility into the enterprise and partner networks on\ndifferent, interrelated abstraction levels is desirable.\n  We show the query and data processing capabilities of a novel data discovery,\nmining and network inference system, called Business Network System (BNS) that\nreconstructs the BN--integration and business process networks - from raw data,\nhidden in the enterprises' landscapes. The paper covers both the foundation and\nthe key data processing characteristics features of BNS, including its\nunderlying technologies, its overall system architecture, and data provenance\napproach."}, "authors": ["Daniel Ritter"], "author_detail": {"name": "Daniel Ritter"}, "author": "Daniel Ritter", "links": [{"title": "doi", "href": "http://dx.doi.org/10.7763/IJMLC", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/1312.7436v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1312.7436v1", "rel": "related", "type": "application/pdf"}], "arxiv_comment": "5 pages, 2nd International Conference on Knowledge Discovery (ICKD),\n  Copenhagen, 2013. Proceedings in the International Journal of Machine\n  Learning and Computing (IJMLC)", "arxiv_primary_category": {"term": "cs.OH", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.OH", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1312.7436v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1312.7436v1", "journal_reference": "International Journal of Machine Learning and Computing, Volume 3,\n  Number 2, 2013", "doi": "10.7763/IJMLC"}
{"id": "http://arxiv.org/abs/1507.03168v1", "guidislink": true, "updated": "2015-07-11T23:10:17Z", "updated_parsed": [2015, 7, 11, 23, 10, 17, 5, 192, 0], "published": "2015-07-11T23:10:17Z", "published_parsed": [2015, 7, 11, 23, 10, 17, 5, 192, 0], "title": "Using Bayesian Network Representations for Effective Sampling from\n  Generative Network Models", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=160&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Using Bayesian Network Representations for Effective Sampling from\n  Generative Network Models"}, "summary": "Bayesian networks (BNs) are used for inference and sampling by exploiting\nconditional independence among random variables. Context specific independence\n(CSI) is a property of graphical models where additional independence relations\narise in the context of particular values of random variables (RVs).\nIdentifying and exploiting CSI properties can simplify inference. Some\ngenerative network models (models that generate social/information network\nsamples from a network distribution P(G)), with complex interactions among a\nset of RVs, can be represented with probabilistic graphical models, in\nparticular with BNs. In the present work we show one such a case. We discuss\nhow a mixed Kronecker Product Graph Model can be represented as a BN, and study\nits BN properties that can be used for efficient sampling. Specifically, we\nshow that instead of exhibiting CSI properties, the model has deterministic\ncontext-specific dependence (DCSD). Exploiting this property focuses the\nsampling method on a subset of the sampling space that improves efficiency.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=160&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Bayesian networks (BNs) are used for inference and sampling by exploiting\nconditional independence among random variables. Context specific independence\n(CSI) is a property of graphical models where additional independence relations\narise in the context of particular values of random variables (RVs).\nIdentifying and exploiting CSI properties can simplify inference. Some\ngenerative network models (models that generate social/information network\nsamples from a network distribution P(G)), with complex interactions among a\nset of RVs, can be represented with probabilistic graphical models, in\nparticular with BNs. In the present work we show one such a case. We discuss\nhow a mixed Kronecker Product Graph Model can be represented as a BN, and study\nits BN properties that can be used for efficient sampling. Specifically, we\nshow that instead of exhibiting CSI properties, the model has deterministic\ncontext-specific dependence (DCSD). Exploiting this property focuses the\nsampling method on a subset of the sampling space that improves efficiency."}, "authors": ["Pablo Robles-Granda", "Sebastian Moreno", "Jennifer Neville"], "author_detail": {"name": "Jennifer Neville"}, "author": "Jennifer Neville", "links": [{"href": "http://arxiv.org/abs/1507.03168v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1507.03168v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1507.03168v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1507.03168v1", "arxiv_comment": null, "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1804.08450v1", "guidislink": true, "updated": "2018-04-23T14:06:50Z", "updated_parsed": [2018, 4, 23, 14, 6, 50, 0, 113, 0], "published": "2018-04-23T14:06:50Z", "published_parsed": [2018, 4, 23, 14, 6, 50, 0, 113, 0], "title": "Decorrelated Batch Normalization", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=170&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Decorrelated Batch Normalization"}, "summary": "Batch Normalization (BN) is capable of accelerating the training of deep\nmodels by centering and scaling activations within mini-batches. In this work,\nwe propose Decorrelated Batch Normalization (DBN), which not just centers and\nscales activations but whitens them. We explore multiple whitening techniques,\nand find that PCA whitening causes a problem we call stochastic axis swapping,\nwhich is detrimental to learning. We show that ZCA whitening does not suffer\nfrom this problem, permitting successful learning. DBN retains the desirable\nqualities of BN and further improves BN's optimization efficiency and\ngeneralization ability. We design comprehensive experiments to show that DBN\ncan improve the performance of BN on multilayer perceptrons and convolutional\nneural networks. Furthermore, we consistently improve the accuracy of residual\nnetworks on CIFAR-10, CIFAR-100, and ImageNet.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=170&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Batch Normalization (BN) is capable of accelerating the training of deep\nmodels by centering and scaling activations within mini-batches. In this work,\nwe propose Decorrelated Batch Normalization (DBN), which not just centers and\nscales activations but whitens them. We explore multiple whitening techniques,\nand find that PCA whitening causes a problem we call stochastic axis swapping,\nwhich is detrimental to learning. We show that ZCA whitening does not suffer\nfrom this problem, permitting successful learning. DBN retains the desirable\nqualities of BN and further improves BN's optimization efficiency and\ngeneralization ability. We design comprehensive experiments to show that DBN\ncan improve the performance of BN on multilayer perceptrons and convolutional\nneural networks. Furthermore, we consistently improve the accuracy of residual\nnetworks on CIFAR-10, CIFAR-100, and ImageNet."}, "authors": ["Lei Huang", "Dawei Yang", "Bo Lang", "Jia Deng"], "author_detail": {"name": "Jia Deng"}, "author": "Jia Deng", "arxiv_comment": "Accepted to CVPR 2018. Code available at\n  https://github.com/umich-vl/DecorrelatedBN", "links": [{"href": "http://arxiv.org/abs/1804.08450v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1804.08450v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.ML", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1804.08450v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1804.08450v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1807.02566v1", "guidislink": true, "updated": "2018-06-29T15:00:02Z", "updated_parsed": [2018, 6, 29, 15, 0, 2, 4, 180, 0], "published": "2018-06-29T15:00:02Z", "published_parsed": [2018, 6, 29, 15, 0, 2, 4, 180, 0], "title": "Updating Probabilistic Knowledge on Condition/Event Nets using Bayesian\n  Networks", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=170&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Updating Probabilistic Knowledge on Condition/Event Nets using Bayesian\n  Networks"}, "summary": "The paper extends Bayesian networks (BNs) by a mechanism for dynamic changes\nto the probability distributions represented by BNs. One application scenario\nis the process of knowledge acquisition of an observer interacting with a\nsystem. In particular, the paper considers condition/event nets where the\nobserver's knowledge about the current marking is a probability distribution\nover markings. The observer can interact with the net to deduce information\nabout the marking by requesting certain transitions to fire and observing their\nsuccess or failure.\n  Aiming for an efficient implementation of dynamic changes to probability\ndistributions of BNs, we consider a modular form of networks that form the\narrows of a free PROP with a commutative comonoid structure, also known as term\ngraphs. The algebraic structure of such PROPs supplies us with a compositional\nsemantics that functorially maps BNs to their underlying probability\ndistribution and, in particular, it provides a convenient means to describe\nstructural updates of networks.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=170&max_results=10&sortBy=relevance&sortOrder=descending", "value": "The paper extends Bayesian networks (BNs) by a mechanism for dynamic changes\nto the probability distributions represented by BNs. One application scenario\nis the process of knowledge acquisition of an observer interacting with a\nsystem. In particular, the paper considers condition/event nets where the\nobserver's knowledge about the current marking is a probability distribution\nover markings. The observer can interact with the net to deduce information\nabout the marking by requesting certain transitions to fire and observing their\nsuccess or failure.\n  Aiming for an efficient implementation of dynamic changes to probability\ndistributions of BNs, we consider a modular form of networks that form the\narrows of a free PROP with a commutative comonoid structure, also known as term\ngraphs. The algebraic structure of such PROPs supplies us with a compositional\nsemantics that functorially maps BNs to their underlying probability\ndistribution and, in particular, it provides a convenient means to describe\nstructural updates of networks."}, "authors": ["Benjamin Cabrera", "Tobias Heindel", "Reiko Heckel", "Barbara Knig"], "author_detail": {"name": "Barbara Knig"}, "author": "Barbara Knig", "arxiv_comment": "Accepted at CONCUR '18", "links": [{"href": "http://arxiv.org/abs/1807.02566v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1807.02566v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LO", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LO", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.SI", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1807.02566v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1807.02566v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1806.10927v1", "guidislink": true, "updated": "2018-06-27T13:11:26Z", "updated_parsed": [2018, 6, 27, 13, 11, 26, 2, 178, 0], "published": "2018-06-27T13:11:26Z", "published_parsed": [2018, 6, 27, 13, 11, 26, 2, 178, 0], "title": "Towards the Existential Control of Boolean Networks: A Preliminary\n  Report (Extended Abstract)", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=170&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Towards the Existential Control of Boolean Networks: A Preliminary\n  Report (Extended Abstract)"}, "summary": "Given a Boolean network BN and a subset A of attractors of BN, we study the\nproblem of identifying a minimal subset C of vertices of BN, such that the\ndynamics of BN can reach from a state s in any attractor As in A to any\nattractor At in A by controlling or toggling a subset of vertices in C in a\nsingle time step. We describe a method based on the decomposition of the\nnetwork structure into strongly connected components called blocks. The control\nsubset can be locally computed for each such block and the results then merged\nto derive the global control subset C. This potentially improves the efficiency\nfor many real-life networks that are large but modular and well-structured. We\nare currently in the process of implementing our method in software.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=170&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Given a Boolean network BN and a subset A of attractors of BN, we study the\nproblem of identifying a minimal subset C of vertices of BN, such that the\ndynamics of BN can reach from a state s in any attractor As in A to any\nattractor At in A by controlling or toggling a subset of vertices in C in a\nsingle time step. We describe a method based on the decomposition of the\nnetwork structure into strongly connected components called blocks. The control\nsubset can be locally computed for each such block and the results then merged\nto derive the global control subset C. This potentially improves the efficiency\nfor many real-life networks that are large but modular and well-structured. We\nare currently in the process of implementing our method in software."}, "authors": ["Soumya Paul", "Jun Pang", "Cui Su"], "author_detail": {"name": "Cui Su"}, "author": "Cui Su", "arxiv_comment": "arXiv admin note: text overlap with arXiv:1804.07221", "links": [{"href": "http://arxiv.org/abs/1806.10927v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1806.10927v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.SY", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.SY", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1806.10927v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1806.10927v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1908.04008v2", "guidislink": true, "updated": "2019-09-18T02:52:32Z", "updated_parsed": [2019, 9, 18, 2, 52, 32, 2, 261, 0], "published": "2019-08-12T05:42:09Z", "published_parsed": [2019, 8, 12, 5, 42, 9, 0, 224, 0], "title": "Instance Enhancement Batch Normalization: an Adaptive Regulator of Batch\n  Noise", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=170&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Instance Enhancement Batch Normalization: an Adaptive Regulator of Batch\n  Noise"}, "summary": "Batch Normalization (BN)(Ioffe and Szegedy 2015) normalizes the features of\nan input image via statistics of a batch of images and hence BN will bring the\nnoise to the gradient of the training loss. Previous works indicate that the\nnoise is important for the optimization and generalization of deep neural\nnetworks, but too much noise will harm the performance of networks. In our\npaper, we offer a new point of view that self-attention mechanism can help to\nregulate the noise by enhancing instance-specific information to obtain a\nbetter regularization effect. Therefore, we propose an attention-based BN\ncalled Instance Enhancement Batch Normalization (IEBN) that recalibrates the\ninformation of each channel by a simple linear transformation. IEBN has a good\ncapacity of regulating noise and stabilizing network training to improve\ngeneralization even in the presence of two kinds of noise attacks during\ntraining. Finally, IEBN outperforms BN with only a light parameter increment in\nimage classification tasks for different network structures and benchmark\ndatasets.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=170&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Batch Normalization (BN)(Ioffe and Szegedy 2015) normalizes the features of\nan input image via statistics of a batch of images and hence BN will bring the\nnoise to the gradient of the training loss. Previous works indicate that the\nnoise is important for the optimization and generalization of deep neural\nnetworks, but too much noise will harm the performance of networks. In our\npaper, we offer a new point of view that self-attention mechanism can help to\nregulate the noise by enhancing instance-specific information to obtain a\nbetter regularization effect. Therefore, we propose an attention-based BN\ncalled Instance Enhancement Batch Normalization (IEBN) that recalibrates the\ninformation of each channel by a simple linear transformation. IEBN has a good\ncapacity of regulating noise and stabilizing network training to improve\ngeneralization even in the presence of two kinds of noise attacks during\ntraining. Finally, IEBN outperforms BN with only a light parameter increment in\nimage classification tasks for different network structures and benchmark\ndatasets."}, "authors": ["Senwei Liang", "Zhongzhan Huang", "Mingfu Liang", "Haizhao Yang"], "author_detail": {"name": "Haizhao Yang"}, "author": "Haizhao Yang", "links": [{"href": "http://arxiv.org/abs/1908.04008v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1908.04008v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.ML", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1908.04008v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1908.04008v2", "arxiv_comment": null, "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/2006.13382v2", "guidislink": true, "updated": "2020-10-21T13:49:26Z", "updated_parsed": [2020, 10, 21, 13, 49, 26, 2, 295, 0], "published": "2020-06-23T23:29:51Z", "published_parsed": [2020, 6, 23, 23, 29, 51, 1, 175, 0], "title": "A spherical analysis of Adam with Batch Normalization", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=180&max_results=10&sortBy=relevance&sortOrder=descending", "value": "A spherical analysis of Adam with Batch Normalization"}, "summary": "Batch Normalization (BN) is a prominent deep learning technique. In spite of\nits apparent simplicity, its implications over optimization are yet to be fully\nunderstood. While previous studies mostly focus on the interaction between BN\nand stochastic gradient descent (SGD), we develop a geometric perspective which\nallows us to precisely characterize the relation between BN and Adam. More\nprecisely, we leverage the radial invariance of groups of parameters, such as\nfilters for convolutional neural networks, to translate the optimization steps\non the $L_2$ unit hypersphere. This formulation and the associated geometric\ninterpretation shed new light on the training dynamics. Firstly, we use it to\nderive the first effective learning rate expression of Adam. Then we show that,\nin the presence of BN layers, performing SGD alone is actually equivalent to a\nvariant of Adam constrained to the unit hypersphere. Finally, our analysis\noutlines phenomena that previous variants of Adam act on and we experimentally\nvalidate their importance in the optimization process.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=180&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Batch Normalization (BN) is a prominent deep learning technique. In spite of\nits apparent simplicity, its implications over optimization are yet to be fully\nunderstood. While previous studies mostly focus on the interaction between BN\nand stochastic gradient descent (SGD), we develop a geometric perspective which\nallows us to precisely characterize the relation between BN and Adam. More\nprecisely, we leverage the radial invariance of groups of parameters, such as\nfilters for convolutional neural networks, to translate the optimization steps\non the $L_2$ unit hypersphere. This formulation and the associated geometric\ninterpretation shed new light on the training dynamics. Firstly, we use it to\nderive the first effective learning rate expression of Adam. Then we show that,\nin the presence of BN layers, performing SGD alone is actually equivalent to a\nvariant of Adam constrained to the unit hypersphere. Finally, our analysis\noutlines phenomena that previous variants of Adam act on and we experimentally\nvalidate their importance in the optimization process."}, "authors": ["Simon Roburin", "Yann de Mont-Marin", "Andrei Bursuc", "Renaud Marlet", "Patrick Prez", "Mathieu Aubry"], "author_detail": {"name": "Mathieu Aubry"}, "author": "Mathieu Aubry", "links": [{"href": "http://arxiv.org/abs/2006.13382v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2006.13382v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.ML", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2006.13382v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2006.13382v2", "arxiv_comment": null, "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/2007.09181v1", "guidislink": true, "updated": "2020-07-17T18:29:49Z", "updated_parsed": [2020, 7, 17, 18, 29, 49, 4, 199, 0], "published": "2020-07-17T18:29:49Z", "published_parsed": [2020, 7, 17, 18, 29, 49, 4, 199, 0], "title": "Network Learning Approaches to study World Happiness", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=180&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Network Learning Approaches to study World Happiness"}, "summary": "The United Nations in its 2011 resolution declared the pursuit of happiness a\nfundamental human goal and proposed public and economic policies centered\naround happiness. In this paper we used 2 types of computational strategies\nviz. Predictive Modelling and Bayesian Networks (BNs) to model the processed\nhistorical happiness index data of 156 nations published by UN since 2012. We\nattacked the problem of prediction using General Regression Neural Networks\n(GRNNs) and show that it out performs other state of the art predictive models.\nTo understand causal links amongst key features that have been proven to have a\nsignificant impact on world happiness, we first used a manual discretization\nscheme to discretize continuous variables into 3 levels viz. Low, Medium and\nHigh. A consensus World Happiness BN structure was then fixed after\namalgamating information by learning 10000 different BNs using bootstrapping.\nLastly, exact inference through conditional probability queries was used on\nthis BN to unravel interesting relationships among the important features\naffecting happiness which would be useful in policy making.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=180&max_results=10&sortBy=relevance&sortOrder=descending", "value": "The United Nations in its 2011 resolution declared the pursuit of happiness a\nfundamental human goal and proposed public and economic policies centered\naround happiness. In this paper we used 2 types of computational strategies\nviz. Predictive Modelling and Bayesian Networks (BNs) to model the processed\nhistorical happiness index data of 156 nations published by UN since 2012. We\nattacked the problem of prediction using General Regression Neural Networks\n(GRNNs) and show that it out performs other state of the art predictive models.\nTo understand causal links amongst key features that have been proven to have a\nsignificant impact on world happiness, we first used a manual discretization\nscheme to discretize continuous variables into 3 levels viz. Low, Medium and\nHigh. A consensus World Happiness BN structure was then fixed after\namalgamating information by learning 10000 different BNs using bootstrapping.\nLastly, exact inference through conditional probability queries was used on\nthis BN to unravel interesting relationships among the important features\naffecting happiness which would be useful in policy making."}, "authors": ["Siddharth Dixit", "Meghna Chaudhary", "Niteesh Sahni"], "author_detail": {"name": "Niteesh Sahni"}, "author": "Niteesh Sahni", "arxiv_comment": "13 Pages, 8 figures", "links": [{"href": "http://arxiv.org/abs/2007.09181v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2007.09181v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CY", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CY", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.SI", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.AP", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2007.09181v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2007.09181v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/2008.13128v1", "guidislink": true, "updated": "2020-08-30T09:33:29Z", "updated_parsed": [2020, 8, 30, 9, 33, 29, 6, 243, 0], "published": "2020-08-30T09:33:29Z", "published_parsed": [2020, 8, 30, 9, 33, 29, 6, 243, 0], "title": "Optimal Quantization for Batch Normalization in Neural Network\n  Deployments and Beyond", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=180&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Optimal Quantization for Batch Normalization in Neural Network\n  Deployments and Beyond"}, "summary": "Quantized Neural Networks (QNNs) use low bit-width fixed-point numbers for\nrepresenting weight parameters and activations, and are often used in\nreal-world applications due to their saving of computation resources and\nreproducibility of results.\n  Batch Normalization (BN) poses a challenge for QNNs for requiring floating\npoints in reciprocal operations, and previous QNNs either require computing BN\nat high precision or revise BN to some variants in heuristic ways.\n  In this work, we propose a novel method to quantize BN by converting an\naffine transformation of two floating points to a fixed-point operation with\nshared quantized scale, which is friendly for hardware acceleration and model\ndeployment.\n  We confirm that our method maintains same outputs through rigorous\ntheoretical analysis and numerical analysis. Accuracy and efficiency of our\nquantization method are verified by experiments at layer level on CIFAR and\nImageNet datasets.\n  We also believe that our method is potentially useful in other problems\ninvolving quantization.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=180&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Quantized Neural Networks (QNNs) use low bit-width fixed-point numbers for\nrepresenting weight parameters and activations, and are often used in\nreal-world applications due to their saving of computation resources and\nreproducibility of results.\n  Batch Normalization (BN) poses a challenge for QNNs for requiring floating\npoints in reciprocal operations, and previous QNNs either require computing BN\nat high precision or revise BN to some variants in heuristic ways.\n  In this work, we propose a novel method to quantize BN by converting an\naffine transformation of two floating points to a fixed-point operation with\nshared quantized scale, which is friendly for hardware acceleration and model\ndeployment.\n  We confirm that our method maintains same outputs through rigorous\ntheoretical analysis and numerical analysis. Accuracy and efficiency of our\nquantization method are verified by experiments at layer level on CIFAR and\nImageNet datasets.\n  We also believe that our method is potentially useful in other problems\ninvolving quantization."}, "authors": ["Dachao Lin", "Peiqin Sun", "Guangzeng Xie", "Shuchang Zhou", "Zhihua Zhang"], "author_detail": {"name": "Zhihua Zhang"}, "author": "Zhihua Zhang", "links": [{"href": "http://arxiv.org/abs/2008.13128v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2008.13128v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.ML", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2008.13128v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2008.13128v1", "arxiv_comment": null, "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/2011.12536v1", "guidislink": true, "updated": "2020-11-25T06:11:06Z", "updated_parsed": [2020, 11, 25, 6, 11, 6, 2, 330, 0], "published": "2020-11-25T06:11:06Z", "published_parsed": [2020, 11, 25, 6, 11, 6, 2, 330, 0], "title": "Vocal Tract Length Perturbation for Text-Dependent Speaker Verification\n  with Autoregressive Prediction Coding", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=180&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Vocal Tract Length Perturbation for Text-Dependent Speaker Verification\n  with Autoregressive Prediction Coding"}, "summary": "In this letter, we propose a vocal tract length (VTL) perturbation method for\ntext-dependent speaker verification (TD-SV), in which a set of TD-SV systems\nare trained, one for each VTL factor, and score-level fusion is applied to make\na final decision. Next, we explore the bottleneck (BN) feature extracted by\ntraining deep neural networks with a self-supervised objective, autoregressive\npredictive coding (APC), for TD-SV and compare it with the well-studied\nspeaker-discriminant BN feature. The proposed VTL method is then applied to APC\nand speaker-discriminant BN features. In the end, we combine the VTL\nperturbation systems trained on MFCC and the two BN features in the score\ndomain. Experiments are performed on the RedDots challenge 2016 database of\nTD-SV using short utterances with Gaussian mixture model-universal background\nmodel and i-vector techniques. Results show the proposed methods significantly\noutperform the baselines.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=180&max_results=10&sortBy=relevance&sortOrder=descending", "value": "In this letter, we propose a vocal tract length (VTL) perturbation method for\ntext-dependent speaker verification (TD-SV), in which a set of TD-SV systems\nare trained, one for each VTL factor, and score-level fusion is applied to make\na final decision. Next, we explore the bottleneck (BN) feature extracted by\ntraining deep neural networks with a self-supervised objective, autoregressive\npredictive coding (APC), for TD-SV and compare it with the well-studied\nspeaker-discriminant BN feature. The proposed VTL method is then applied to APC\nand speaker-discriminant BN features. In the end, we combine the VTL\nperturbation systems trained on MFCC and the two BN features in the score\ndomain. Experiments are performed on the RedDots challenge 2016 database of\nTD-SV using short utterances with Gaussian mixture model-universal background\nmodel and i-vector techniques. Results show the proposed methods significantly\noutperform the baselines."}, "authors": ["Achintya kr. Sarkar", "Zheng-Hua Tan"], "author_detail": {"name": "Zheng-Hua Tan"}, "author": "Zheng-Hua Tan", "links": [{"href": "http://arxiv.org/abs/2011.12536v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2011.12536v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.SD", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.SD", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "eess.AS", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2011.12536v1", "affiliation": "Senior Member, IEEE", "arxiv_url": "http://arxiv.org/abs/2011.12536v1", "arxiv_comment": null, "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/2012.10800v1", "guidislink": true, "updated": "2020-12-19T22:29:49Z", "updated_parsed": [2020, 12, 19, 22, 29, 49, 5, 354, 0], "published": "2020-12-19T22:29:49Z", "published_parsed": [2020, 12, 19, 22, 29, 49, 5, 354, 0], "title": "Probabilistic Dependency Graphs", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=180&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Probabilistic Dependency Graphs"}, "summary": "We introduce Probabilistic Dependency Graphs (PDGs), a new class of directed\ngraphical models. PDGs can capture inconsistent beliefs in a natural way and\nare more modular than Bayesian Networks (BNs), in that they make it easier to\nincorporate new information and restructure the representation. We show by\nexample how PDGs are an especially natural modeling tool. We provide three\nsemantics for PDGs, each of which can be derived from a scoring function (on\njoint distributions over the variables in the network) that can be viewed as\nrepresenting a distribution's incompatibility with the PDG. For the PDG\ncorresponding to a BN, this function is uniquely minimized by the distribution\nthe BN represents, showing that PDG semantics extend BN semantics. We show\nfurther that factor graphs and their exponential families can also be\nfaithfully represented as PDGs, while there are significant barriers to\nmodeling a PDG with a factor graph.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=180&max_results=10&sortBy=relevance&sortOrder=descending", "value": "We introduce Probabilistic Dependency Graphs (PDGs), a new class of directed\ngraphical models. PDGs can capture inconsistent beliefs in a natural way and\nare more modular than Bayesian Networks (BNs), in that they make it easier to\nincorporate new information and restructure the representation. We show by\nexample how PDGs are an especially natural modeling tool. We provide three\nsemantics for PDGs, each of which can be derived from a scoring function (on\njoint distributions over the variables in the network) that can be viewed as\nrepresenting a distribution's incompatibility with the PDG. For the PDG\ncorresponding to a BN, this function is uniquely minimized by the distribution\nthe BN represents, showing that PDG semantics extend BN semantics. We show\nfurther that factor graphs and their exponential families can also be\nfaithfully represented as PDGs, while there are significant barriers to\nmodeling a PDG with a factor graph."}, "authors": ["Oliver Richardson", "Joseph Y Halpern"], "author_detail": {"name": "Joseph Y Halpern"}, "author": "Joseph Y Halpern", "arxiv_comment": "5 figures, 7 pages", "links": [{"href": "http://arxiv.org/abs/2012.10800v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2012.10800v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.IT", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "math.IT", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2012.10800v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2012.10800v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/2101.06073v1", "guidislink": true, "updated": "2021-01-15T11:41:41Z", "updated_parsed": [2021, 1, 15, 11, 41, 41, 4, 15, 0], "published": "2021-01-15T11:41:41Z", "published_parsed": [2021, 1, 15, 11, 41, 41, 4, 15, 0], "title": "Dynamic Normalization", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=180&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Dynamic Normalization"}, "summary": "Batch Normalization has become one of the essential components in CNN. It\nallows the network to use a higher learning rate and speed up training. And the\nnetwork doesn't need to be initialized carefully. However, in our work, we find\nthat a simple extension of BN can increase the performance of the network.\nFirst, we extend BN to adaptively generate scale and shift parameters for each\nmini-batch data, called DN-C (Batch-shared and Channel-wise). We use the\nstatistical characteristics of mini-batch data ($E[X],\nStd[X]\\in\\mathbb{R}^{c}$) as the input of SC module. Then we extend BN to\nadaptively generate scale and shift parameters for each channel of each sample,\ncalled DN-B (Batch and Channel-wise). Our experiments show that DN-C model\ncan't train normally, but DN-B model has very good robustness. In\nclassification task, DN-B can improve the accuracy of the MobileNetV2 on\nImageNet-100 more than 2% with only 0.6% additional Mult-Adds. In detection\ntask, DN-B can improve the accuracy of the SSDLite on MS-COCO nearly 4% mAP\nwith the same settings. Compared with BN, DN-B has stable performance when\nusing higher learning rate or smaller batch size.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=180&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Batch Normalization has become one of the essential components in CNN. It\nallows the network to use a higher learning rate and speed up training. And the\nnetwork doesn't need to be initialized carefully. However, in our work, we find\nthat a simple extension of BN can increase the performance of the network.\nFirst, we extend BN to adaptively generate scale and shift parameters for each\nmini-batch data, called DN-C (Batch-shared and Channel-wise). We use the\nstatistical characteristics of mini-batch data ($E[X],\nStd[X]\\in\\mathbb{R}^{c}$) as the input of SC module. Then we extend BN to\nadaptively generate scale and shift parameters for each channel of each sample,\ncalled DN-B (Batch and Channel-wise). Our experiments show that DN-C model\ncan't train normally, but DN-B model has very good robustness. In\nclassification task, DN-B can improve the accuracy of the MobileNetV2 on\nImageNet-100 more than 2% with only 0.6% additional Mult-Adds. In detection\ntask, DN-B can improve the accuracy of the SSDLite on MS-COCO nearly 4% mAP\nwith the same settings. Compared with BN, DN-B has stable performance when\nusing higher learning rate or smaller batch size."}, "authors": ["Chuan Liu", "Yi Gao", "Jiancheng Lv"], "author_detail": {"name": "Jiancheng Lv"}, "author": "Jiancheng Lv", "arxiv_comment": "9 pages, 5 figures", "links": [{"href": "http://arxiv.org/abs/2101.06073v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2101.06073v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2101.06073v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2101.06073v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/2101.07525v1", "guidislink": true, "updated": "2021-01-19T09:27:03Z", "updated_parsed": [2021, 1, 19, 9, 27, 3, 1, 19, 0], "published": "2021-01-19T09:27:03Z", "published_parsed": [2021, 1, 19, 9, 27, 3, 1, 19, 0], "title": "Momentum^2 Teacher: Momentum Teacher with Momentum Statistics for\n  Self-Supervised Learning", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=180&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Momentum^2 Teacher: Momentum Teacher with Momentum Statistics for\n  Self-Supervised Learning"}, "summary": "In this paper, we present a novel approach, Momentum$^2$ Teacher, for\nstudent-teacher based self-supervised learning. The approach performs momentum\nupdate on both network weights and batch normalization (BN) statistics. The\nteacher's weight is a momentum update of the student, and the teacher's BN\nstatistics is a momentum update of those in history. The Momentum$^2$ Teacher\nis simple and efficient. It can achieve the state of the art results (74.5\\%)\nunder ImageNet linear evaluation protocol using small-batch size(\\eg, 128),\nwithout requiring large-batch training on special hardware like TPU or\ninefficient across GPU operation (\\eg, shuffling BN, synced BN). Our\nimplementation and pre-trained models will be given on\nGitHub\\footnote{https://github.com/zengarden/momentum2-teacher}.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=180&max_results=10&sortBy=relevance&sortOrder=descending", "value": "In this paper, we present a novel approach, Momentum$^2$ Teacher, for\nstudent-teacher based self-supervised learning. The approach performs momentum\nupdate on both network weights and batch normalization (BN) statistics. The\nteacher's weight is a momentum update of the student, and the teacher's BN\nstatistics is a momentum update of those in history. The Momentum$^2$ Teacher\nis simple and efficient. It can achieve the state of the art results (74.5\\%)\nunder ImageNet linear evaluation protocol using small-batch size(\\eg, 128),\nwithout requiring large-batch training on special hardware like TPU or\ninefficient across GPU operation (\\eg, shuffling BN, synced BN). Our\nimplementation and pre-trained models will be given on\nGitHub\\footnote{https://github.com/zengarden/momentum2-teacher}."}, "authors": ["Zeming Li", "Songtao Liu", "Jian Sun"], "author_detail": {"name": "Jian Sun"}, "author": "Jian Sun", "arxiv_comment": "11 pages, Tech report", "links": [{"href": "http://arxiv.org/abs/2101.07525v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2101.07525v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2101.07525v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2101.07525v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/2101.08482v1", "guidislink": true, "updated": "2021-01-21T07:45:37Z", "updated_parsed": [2021, 1, 21, 7, 45, 37, 3, 21, 0], "published": "2021-01-21T07:45:37Z", "published_parsed": [2021, 1, 21, 7, 45, 37, 3, 21, 0], "title": "Exponential Moving Average Normalization for Self-supervised and\n  Semi-supervised Learning", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=190&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Exponential Moving Average Normalization for Self-supervised and\n  Semi-supervised Learning"}, "summary": "We present a plug-in replacement for batch normalization (BN) called\nexponential moving average normalization (EMAN), which improves the performance\nof existing student-teacher based self- and semi-supervised learning\ntechniques. Unlike the standard BN, where the statistics are computed within\neach batch, EMAN, used in the teacher, updates its statistics by exponential\nmoving average from the BN statistics of the student. This design reduces the\nintrinsic cross-sample dependency of BN and enhance the generalization of the\nteacher. EMAN improves strong baselines for self-supervised learning by 4-6/1-2\npoints and semi-supervised learning by about 7/2 points, when 1%/10% supervised\nlabels are available on ImageNet. These improvements are consistent across\nmethods, network architectures, training duration, and datasets, demonstrating\nthe general effectiveness of this technique.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=190&max_results=10&sortBy=relevance&sortOrder=descending", "value": "We present a plug-in replacement for batch normalization (BN) called\nexponential moving average normalization (EMAN), which improves the performance\nof existing student-teacher based self- and semi-supervised learning\ntechniques. Unlike the standard BN, where the statistics are computed within\neach batch, EMAN, used in the teacher, updates its statistics by exponential\nmoving average from the BN statistics of the student. This design reduces the\nintrinsic cross-sample dependency of BN and enhance the generalization of the\nteacher. EMAN improves strong baselines for self-supervised learning by 4-6/1-2\npoints and semi-supervised learning by about 7/2 points, when 1%/10% supervised\nlabels are available on ImageNet. These improvements are consistent across\nmethods, network architectures, training duration, and datasets, demonstrating\nthe general effectiveness of this technique."}, "authors": ["Zhaowei Cai", "Avinash Ravichandran", "Subhransu Maji", "Charless Fowlkes", "Zhuowen Tu", "Stefano Soatto"], "author_detail": {"name": "Stefano Soatto"}, "author": "Stefano Soatto", "links": [{"href": "http://arxiv.org/abs/2101.08482v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2101.08482v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2101.08482v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2101.08482v1", "arxiv_comment": null, "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1905.04554v1", "guidislink": true, "updated": "2019-05-11T17:20:19Z", "updated_parsed": [2019, 5, 11, 17, 20, 19, 5, 131, 0], "published": "2019-05-11T17:20:19Z", "published_parsed": [2019, 5, 11, 17, 20, 19, 5, 131, 0], "title": "Time-Contrastive Learning Based Deep Bottleneck Features for\n  Text-Dependent Speaker Verification", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=190&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Time-Contrastive Learning Based Deep Bottleneck Features for\n  Text-Dependent Speaker Verification"}, "summary": "There are a number of studies about extraction of bottleneck (BN) features\nfrom deep neural networks (DNNs)trained to discriminate speakers, pass-phrases\nand triphone states for improving the performance of text-dependent speaker\nverification (TD-SV). However, a moderate success has been achieved. A recent\nstudy [1] presented a time contrastive learning (TCL) concept to explore the\nnon-stationarity of brain signals for classification of brain states. Speech\nsignals have similar non-stationarity property, and TCL further has the\nadvantage of having no need for labeled data. We therefore present a TCL based\nBN feature extraction method. The method uniformly partitions each speech\nutterance in a training dataset into a predefined number of multi-frame\nsegments. Each segment in an utterance corresponds to one class, and class\nlabels are shared across utterances. DNNs are then trained to discriminate all\nspeech frames among the classes to exploit the temporal structure of speech. In\naddition, we propose a segment-based unsupervised clustering algorithm to\nre-assign class labels to the segments. TD-SV experiments were conducted on the\nRedDots challenge database. The TCL-DNNs were trained using speech data of\nfixed pass-phrases that were excluded from the TD-SV evaluation set, so the\nlearned features can be considered phrase-independent. We compare the\nperformance of the proposed TCL bottleneck (BN) feature with those of\nshort-time cepstral features and BN features extracted from DNNs discriminating\nspeakers, pass-phrases, speaker+pass-phrase, as well as monophones whose labels\nand boundaries are generated by three different automatic speech recognition\n(ASR) systems. Experimental results show that the proposed TCL-BN outperforms\ncepstral features and speaker+pass-phrase discriminant BN features, and its\nperformance is on par with those of ASR derived BN features. Moreover,....", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=190&max_results=10&sortBy=relevance&sortOrder=descending", "value": "There are a number of studies about extraction of bottleneck (BN) features\nfrom deep neural networks (DNNs)trained to discriminate speakers, pass-phrases\nand triphone states for improving the performance of text-dependent speaker\nverification (TD-SV). However, a moderate success has been achieved. A recent\nstudy [1] presented a time contrastive learning (TCL) concept to explore the\nnon-stationarity of brain signals for classification of brain states. Speech\nsignals have similar non-stationarity property, and TCL further has the\nadvantage of having no need for labeled data. We therefore present a TCL based\nBN feature extraction method. The method uniformly partitions each speech\nutterance in a training dataset into a predefined number of multi-frame\nsegments. Each segment in an utterance corresponds to one class, and class\nlabels are shared across utterances. DNNs are then trained to discriminate all\nspeech frames among the classes to exploit the temporal structure of speech. In\naddition, we propose a segment-based unsupervised clustering algorithm to\nre-assign class labels to the segments. TD-SV experiments were conducted on the\nRedDots challenge database. The TCL-DNNs were trained using speech data of\nfixed pass-phrases that were excluded from the TD-SV evaluation set, so the\nlearned features can be considered phrase-independent. We compare the\nperformance of the proposed TCL bottleneck (BN) feature with those of\nshort-time cepstral features and BN features extracted from DNNs discriminating\nspeakers, pass-phrases, speaker+pass-phrase, as well as monophones whose labels\nand boundaries are generated by three different automatic speech recognition\n(ASR) systems. Experimental results show that the proposed TCL-BN outperforms\ncepstral features and speaker+pass-phrase discriminant BN features, and its\nperformance is on par with those of ASR derived BN features. Moreover,...."}, "authors": ["Achintya kr. Sarkar", "Zheng-Hua Tan", "Hao Tang", "Suwon Shon", "James Glass"], "author_detail": {"name": "James Glass"}, "author": "James Glass", "links": [{"title": "doi", "href": "http://dx.doi.org/10.1109/TASLP.2019.2915322", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/1905.04554v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1905.04554v1", "rel": "related", "type": "application/pdf"}], "arxiv_comment": "Copyright (c) 2019 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "arxiv_primary_category": {"term": "cs.SD", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.SD", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "eess.AS", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1905.04554v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1905.04554v1", "journal_reference": "IEEE/ACM Transactions on Audio, Speech, and Language Processing,\n  2019", "doi": "10.1109/TASLP.2019.2915322"}
{"id": "http://arxiv.org/abs/2003.01207v1", "guidislink": true, "updated": "2020-03-02T21:55:35Z", "updated_parsed": [2020, 3, 2, 21, 55, 35, 0, 62, 0], "published": "2020-03-02T21:55:35Z", "published_parsed": [2020, 3, 2, 21, 55, 35, 0, 62, 0], "title": "BARD: A structured technique for group elicitation of Bayesian networks\n  to support analytic reasoning", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=190&max_results=10&sortBy=relevance&sortOrder=descending", "value": "BARD: A structured technique for group elicitation of Bayesian networks\n  to support analytic reasoning"}, "summary": "In many complex, real-world situations, problem solving and decision making\nrequire effective reasoning about causation and uncertainty. However, human\nreasoning in these cases is prone to confusion and error. Bayesian networks\n(BNs) are an artificial intelligence technology that models uncertain\nsituations, supporting probabilistic and causal reasoning and decision making.\nHowever, to date, BN methodologies and software require significant upfront\ntraining, do not provide much guidance on the model building process, and do\nnot support collaboratively building BNs. BARD (Bayesian ARgumentation via\nDelphi) is both a methodology and an expert system that utilises (1) BNs as the\nunderlying structured representations for better argument analysis, (2) a\nmulti-user web-based software platform and Delphi-style social processes to\nassist with collaboration, and (3) short, high-quality e-courses on demand, a\nhighly structured process to guide BN construction, and a variety of helpful\ntools to assist in building and reasoning with BNs, including an automated\nexplanation tool to assist effective report writing. The result is an\nend-to-end online platform, with associated online training, for groups without\nprior BN expertise to understand and analyse a problem, build a model of its\nunderlying probabilistic causal structure, validate and reason with the causal\nmodel, and use it to produce a written analytic report. Initial experimental\nresults demonstrate that BARD aids in problem solving, reasoning and\ncollaboration.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=190&max_results=10&sortBy=relevance&sortOrder=descending", "value": "In many complex, real-world situations, problem solving and decision making\nrequire effective reasoning about causation and uncertainty. However, human\nreasoning in these cases is prone to confusion and error. Bayesian networks\n(BNs) are an artificial intelligence technology that models uncertain\nsituations, supporting probabilistic and causal reasoning and decision making.\nHowever, to date, BN methodologies and software require significant upfront\ntraining, do not provide much guidance on the model building process, and do\nnot support collaboratively building BNs. BARD (Bayesian ARgumentation via\nDelphi) is both a methodology and an expert system that utilises (1) BNs as the\nunderlying structured representations for better argument analysis, (2) a\nmulti-user web-based software platform and Delphi-style social processes to\nassist with collaboration, and (3) short, high-quality e-courses on demand, a\nhighly structured process to guide BN construction, and a variety of helpful\ntools to assist in building and reasoning with BNs, including an automated\nexplanation tool to assist effective report writing. The result is an\nend-to-end online platform, with associated online training, for groups without\nprior BN expertise to understand and analyse a problem, build a model of its\nunderlying probabilistic causal structure, validate and reason with the causal\nmodel, and use it to produce a written analytic report. Initial experimental\nresults demonstrate that BARD aids in problem solving, reasoning and\ncollaboration."}, "authors": ["Ann E. Nicholson", "Kevin B. Korb", "Erik P. Nyberg", "Michael Wybrow", "Ingrid Zukerman", "Steven Mascaro", "Shreshth Thakur", "Abraham Oshni Alvandi", "Jeff Riley", "Ross Pearson", "Shane Morris", "Matthieu Herrmann", "A. K. M. Azad", "Fergus Bolger", "Ulrike Hahn", "David Lagnado"], "author_detail": {"name": "David Lagnado"}, "author": "David Lagnado", "links": [{"href": "http://arxiv.org/abs/2003.01207v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2003.01207v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2003.01207v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2003.01207v1", "arxiv_comment": null, "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1711.11193v1", "guidislink": true, "updated": "2017-11-30T02:25:32Z", "updated_parsed": [2017, 11, 30, 2, 25, 32, 3, 334, 0], "published": "2017-11-30T02:25:32Z", "published_parsed": [2017, 11, 30, 2, 25, 32, 3, 334, 0], "title": "Design of Non-orthogonal Multiple Access Enhanced Backscatter\n  Communication", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=200&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Design of Non-orthogonal Multiple Access Enhanced Backscatter\n  Communication"}, "summary": "Backscatter communication (BackCom), which allows a backscatter node (BN) to\ncommunicate with the reader by modulating and reflecting the incident\ncontinuous wave from the reader, is considered as a promising solution to power\nthe future Internet-of-Things. In this paper, we consider a single BackCom\nsystem, where multiple BNs are served by a reader. We propose to use the\npower-domain non-orthogonal multiple access (NOMA), i.e., multiplexing the BNs\nin different regions or with different backscattered power levels, to enhance\nthe spectrum efficiency of the BackCom system. To better exploit power-domain\nNOMA, we propose to set the reflection coefficients for multiplexed BNs to be\ndifferent. Based on this considered model, we develop the reflection\ncoefficient selection criteria. To illustrate the enhanced system with the\nproposed criteria, we analyze the performance of BackCom system in terms of the\naverage number of bits that can be successfully decoded by the reader for\ntwo-node pairing case and the average number of successful BNs for the general\nmultiplexing case. Our results shows that NOMA achieves much better performance\ngain in the BackCom system as compared to its performance gain in the\nconventional system, which highlights the importance of applying NOMA to the\nBackCom system.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=200&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Backscatter communication (BackCom), which allows a backscatter node (BN) to\ncommunicate with the reader by modulating and reflecting the incident\ncontinuous wave from the reader, is considered as a promising solution to power\nthe future Internet-of-Things. In this paper, we consider a single BackCom\nsystem, where multiple BNs are served by a reader. We propose to use the\npower-domain non-orthogonal multiple access (NOMA), i.e., multiplexing the BNs\nin different regions or with different backscattered power levels, to enhance\nthe spectrum efficiency of the BackCom system. To better exploit power-domain\nNOMA, we propose to set the reflection coefficients for multiplexed BNs to be\ndifferent. Based on this considered model, we develop the reflection\ncoefficient selection criteria. To illustrate the enhanced system with the\nproposed criteria, we analyze the performance of BackCom system in terms of the\naverage number of bits that can be successfully decoded by the reader for\ntwo-node pairing case and the average number of successful BNs for the general\nmultiplexing case. Our results shows that NOMA achieves much better performance\ngain in the BackCom system as compared to its performance gain in the\nconventional system, which highlights the importance of applying NOMA to the\nBackCom system."}, "authors": ["Jing Guo", "Xiangyun Zhou", "Salman Durrani", "Halim Yanikomeroglu"], "author_detail": {"name": "Halim Yanikomeroglu"}, "author": "Halim Yanikomeroglu", "arxiv_comment": "submitted to the IEEE for possible publication", "links": [{"href": "http://arxiv.org/abs/1711.11193v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1711.11193v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.IT", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.IT", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "math.IT", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1711.11193v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1711.11193v1", "journal_reference": "IEEE Transactions on Wireless Communications, vol. 10, no. 8, pp.\n  6837-6852, Oct. 2018", "doi": null}
{"id": "http://arxiv.org/abs/1601.07460v4", "guidislink": true, "updated": "2017-03-03T05:57:15Z", "updated_parsed": [2017, 3, 3, 5, 57, 15, 4, 62, 0], "published": "2016-01-27T17:41:05Z", "published_parsed": [2016, 1, 27, 17, 41, 5, 2, 27, 0], "title": "Information-theoretic limits of Bayesian network structure learning", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=200&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Information-theoretic limits of Bayesian network structure learning"}, "summary": "In this paper, we study the information-theoretic limits of learning the\nstructure of Bayesian networks (BNs), on discrete as well as continuous random\nvariables, from a finite number of samples. We show that the minimum number of\nsamples required by any procedure to recover the correct structure grows as\n$\\Omega(m)$ and $\\Omega(k \\log m + (k^2/m))$ for non-sparse and sparse BNs\nrespectively, where $m$ is the number of variables and $k$ is the maximum\nnumber of parents per node. We provide a simple recipe, based on an extension\nof the Fano's inequality, to obtain information-theoretic limits of structure\nrecovery for any exponential family BN. We instantiate our result for specific\nconditional distributions in the exponential family to characterize the\nfundamental limits of learning various commonly used BNs, such as conditional\nprobability table based networks, gaussian BNs, noisy-OR networks, and logistic\nregression networks. En route to obtaining our main results, we obtain tight\nbounds on the number of sparse and non-sparse essential-DAGs. Finally, as a\nbyproduct, we recover the information-theoretic limits of sparse variable\nselection for logistic regression.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=200&max_results=10&sortBy=relevance&sortOrder=descending", "value": "In this paper, we study the information-theoretic limits of learning the\nstructure of Bayesian networks (BNs), on discrete as well as continuous random\nvariables, from a finite number of samples. We show that the minimum number of\nsamples required by any procedure to recover the correct structure grows as\n$\\Omega(m)$ and $\\Omega(k \\log m + (k^2/m))$ for non-sparse and sparse BNs\nrespectively, where $m$ is the number of variables and $k$ is the maximum\nnumber of parents per node. We provide a simple recipe, based on an extension\nof the Fano's inequality, to obtain information-theoretic limits of structure\nrecovery for any exponential family BN. We instantiate our result for specific\nconditional distributions in the exponential family to characterize the\nfundamental limits of learning various commonly used BNs, such as conditional\nprobability table based networks, gaussian BNs, noisy-OR networks, and logistic\nregression networks. En route to obtaining our main results, we obtain tight\nbounds on the number of sparse and non-sparse essential-DAGs. Finally, as a\nbyproduct, we recover the information-theoretic limits of sparse variable\nselection for logistic regression."}, "authors": ["Asish Ghoshal", "Jean Honorio"], "author_detail": {"name": "Jean Honorio"}, "author": "Jean Honorio", "arxiv_comment": "Accepted to AISTATS 2017, Florida", "links": [{"href": "http://arxiv.org/abs/1601.07460v4", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1601.07460v4", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.IT", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "math.IT", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.ML", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1601.07460v4", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1601.07460v4", "journal_reference": "International Conference on Artificial Intelligence and Statistics\n  (AISTATS), 2017", "doi": null}
{"id": "http://arxiv.org/abs/1802.09769v1", "guidislink": true, "updated": "2018-02-27T08:29:16Z", "updated_parsed": [2018, 2, 27, 8, 29, 16, 1, 58, 0], "published": "2018-02-27T08:29:16Z", "published_parsed": [2018, 2, 27, 8, 29, 16, 1, 58, 0], "title": "L1-Norm Batch Normalization for Efficient Training of Deep Neural\n  Networks", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=200&max_results=10&sortBy=relevance&sortOrder=descending", "value": "L1-Norm Batch Normalization for Efficient Training of Deep Neural\n  Networks"}, "summary": "Batch Normalization (BN) has been proven to be quite effective at\naccelerating and improving the training of deep neural networks (DNNs).\nHowever, BN brings additional computation, consumes more memory and generally\nslows down the training process by a large margin, which aggravates the\ntraining effort. Furthermore, the nonlinear square and root operations in BN\nalso impede the low bit-width quantization techniques, which draws much\nattention in deep learning hardware community. In this work, we propose an\nL1-norm BN (L1BN) with only linear operations in both the forward and the\nbackward propagations during training. L1BN is shown to be approximately\nequivalent to the original L2-norm BN (L2BN) by multiplying a scaling factor.\nExperiments on various convolutional neural networks (CNNs) and generative\nadversarial networks (GANs) reveal that L1BN maintains almost the same\naccuracies and convergence rates compared to L2BN but with higher computational\nefficiency. On FPGA platform, the proposed signum and absolute operations in\nL1BN can achieve 1.5$\\times$ speedup and save 50\\% power consumption, compared\nwith the original costly square and root operations, respectively. This\nhardware-friendly normalization method not only surpasses L2BN in speed, but\nalso simplify the hardware design of ASIC accelerators with higher energy\nefficiency. Last but not the least, L1BN promises a fully quantized training of\nDNNs, which is crucial to future adaptive terminal devices.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=200&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Batch Normalization (BN) has been proven to be quite effective at\naccelerating and improving the training of deep neural networks (DNNs).\nHowever, BN brings additional computation, consumes more memory and generally\nslows down the training process by a large margin, which aggravates the\ntraining effort. Furthermore, the nonlinear square and root operations in BN\nalso impede the low bit-width quantization techniques, which draws much\nattention in deep learning hardware community. In this work, we propose an\nL1-norm BN (L1BN) with only linear operations in both the forward and the\nbackward propagations during training. L1BN is shown to be approximately\nequivalent to the original L2-norm BN (L2BN) by multiplying a scaling factor.\nExperiments on various convolutional neural networks (CNNs) and generative\nadversarial networks (GANs) reveal that L1BN maintains almost the same\naccuracies and convergence rates compared to L2BN but with higher computational\nefficiency. On FPGA platform, the proposed signum and absolute operations in\nL1BN can achieve 1.5$\\times$ speedup and save 50\\% power consumption, compared\nwith the original costly square and root operations, respectively. This\nhardware-friendly normalization method not only surpasses L2BN in speed, but\nalso simplify the hardware design of ASIC accelerators with higher energy\nefficiency. Last but not the least, L1BN promises a fully quantized training of\nDNNs, which is crucial to future adaptive terminal devices."}, "authors": ["Shuang Wu", "Guoqi Li", "Lei Deng", "Liu Liu", "Yuan Xie", "Luping Shi"], "author_detail": {"name": "Luping Shi"}, "author": "Luping Shi", "links": [{"title": "doi", "href": "http://dx.doi.org/10.1109/TNNLS.2018.2876179", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/1802.09769v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1802.09769v1", "rel": "related", "type": "application/pdf"}], "arxiv_comment": "8 pages, 4 figures", "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1802.09769v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1802.09769v1", "journal_reference": null, "doi": "10.1109/TNNLS.2018.2876179"}
{"id": "http://arxiv.org/abs/1906.03787v2", "guidislink": true, "updated": "2019-12-21T20:48:24Z", "updated_parsed": [2019, 12, 21, 20, 48, 24, 5, 355, 0], "published": "2019-06-10T03:41:52Z", "published_parsed": [2019, 6, 10, 3, 41, 52, 0, 161, 0], "title": "Intriguing properties of adversarial training at scale", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=210&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Intriguing properties of adversarial training at scale"}, "summary": "Adversarial training is one of the main defenses against adversarial attacks.\nIn this paper, we provide the first rigorous study on diagnosing elements of\nadversarial training, which reveals two intriguing properties.\n  First, we study the role of normalization. Batch normalization (BN) is a\ncrucial element for achieving state-of-the-art performance on many vision\ntasks, but we show it may prevent networks from obtaining strong robustness in\nadversarial training. One unexpected observation is that, for models trained\nwith BN, simply removing clean images from training data largely boosts\nadversarial robustness, i.e., 18.3%. We relate this phenomenon to the\nhypothesis that clean images and adversarial images are drawn from two\ndifferent domains. This two-domain hypothesis may explain the issue of BN when\ntraining with a mixture of clean and adversarial images, as estimating\nnormalization statistics of this mixture distribution is challenging. Guided by\nthis two-domain hypothesis, we show disentangling the mixture distribution for\nnormalization, i.e., applying separate BNs to clean and adversarial images for\nstatistics estimation, achieves much stronger robustness. Additionally, we find\nthat enforcing BNs to behave consistently at training and testing can further\nenhance robustness.\n  Second, we study the role of network capacity. We find our so-called \"deep\"\nnetworks are still shallow for the task of adversarial learning. Unlike\ntraditional classification tasks where accuracy is only marginally improved by\nadding more layers to \"deep\" networks (e.g., ResNet-152), adversarial training\nexhibits a much stronger demand on deeper networks to achieve higher\nadversarial robustness. This robustness improvement can be observed\nsubstantially and consistently even by pushing the network capacity to an\nunprecedented scale, i.e., ResNet-638.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=210&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Adversarial training is one of the main defenses against adversarial attacks.\nIn this paper, we provide the first rigorous study on diagnosing elements of\nadversarial training, which reveals two intriguing properties.\n  First, we study the role of normalization. Batch normalization (BN) is a\ncrucial element for achieving state-of-the-art performance on many vision\ntasks, but we show it may prevent networks from obtaining strong robustness in\nadversarial training. One unexpected observation is that, for models trained\nwith BN, simply removing clean images from training data largely boosts\nadversarial robustness, i.e., 18.3%. We relate this phenomenon to the\nhypothesis that clean images and adversarial images are drawn from two\ndifferent domains. This two-domain hypothesis may explain the issue of BN when\ntraining with a mixture of clean and adversarial images, as estimating\nnormalization statistics of this mixture distribution is challenging. Guided by\nthis two-domain hypothesis, we show disentangling the mixture distribution for\nnormalization, i.e., applying separate BNs to clean and adversarial images for\nstatistics estimation, achieves much stronger robustness. Additionally, we find\nthat enforcing BNs to behave consistently at training and testing can further\nenhance robustness.\n  Second, we study the role of network capacity. We find our so-called \"deep\"\nnetworks are still shallow for the task of adversarial learning. Unlike\ntraditional classification tasks where accuracy is only marginally improved by\nadding more layers to \"deep\" networks (e.g., ResNet-152), adversarial training\nexhibits a much stronger demand on deeper networks to achieve higher\nadversarial robustness. This robustness improvement can be observed\nsubstantially and consistently even by pushing the network capacity to an\nunprecedented scale, i.e., ResNet-638."}, "authors": ["Cihang Xie", "Alan Yuille"], "author_detail": {"name": "Alan Yuille"}, "author": "Alan Yuille", "arxiv_comment": "To appear in ICLR 2020", "links": [{"href": "http://arxiv.org/abs/1906.03787v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1906.03787v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1906.03787v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1906.03787v2", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1910.07454v3", "guidislink": true, "updated": "2019-11-21T11:01:34Z", "updated_parsed": [2019, 11, 21, 11, 1, 34, 3, 325, 0], "published": "2019-10-16T16:22:58Z", "published_parsed": [2019, 10, 16, 16, 22, 58, 2, 289, 0], "title": "An Exponential Learning Rate Schedule for Deep Learning", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=210&max_results=10&sortBy=relevance&sortOrder=descending", "value": "An Exponential Learning Rate Schedule for Deep Learning"}, "summary": "Intriguing empirical evidence exists that deep learning can work well with\nexoticschedules for varying the learning rate. This paper suggests that the\nphenomenon may be due to Batch Normalization or BN, which is ubiquitous and\nprovides benefits in optimization and generalization across all standard\narchitectures. The following new results are shown about BN with weight decay\nand momentum (in other words, the typical use case which was not considered in\nearlier theoretical analyses of stand-alone BN.\n  1. Training can be done using SGD with momentum and an exponentially\nincreasing learning rate schedule, i.e., learning rate increases by some $(1\n+\\alpha)$ factor in every epoch for some $\\alpha >0$. (Precise statement in the\npaper.) To the best of our knowledge this is the first time such a rate\nschedule has been successfully used, let alone for highly successful\narchitectures. As expected, such training rapidly blows up network weights, but\nthe net stays well-behaved due to normalization.\n  2. Mathematical explanation of the success of the above rate schedule: a\nrigorous proof that it is equivalent to the standard setting of BN + SGD +\nStandardRate Tuning + Weight Decay + Momentum. This equivalence holds for other\nnormalization layers as well, Group Normalization, LayerNormalization, Instance\nNorm, etc.\n  3. A worked-out toy example illustrating the above linkage of\nhyper-parameters. Using either weight decay or BN alone reaches global minimum,\nbut convergence fails when both are used.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=210&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Intriguing empirical evidence exists that deep learning can work well with\nexoticschedules for varying the learning rate. This paper suggests that the\nphenomenon may be due to Batch Normalization or BN, which is ubiquitous and\nprovides benefits in optimization and generalization across all standard\narchitectures. The following new results are shown about BN with weight decay\nand momentum (in other words, the typical use case which was not considered in\nearlier theoretical analyses of stand-alone BN.\n  1. Training can be done using SGD with momentum and an exponentially\nincreasing learning rate schedule, i.e., learning rate increases by some $(1\n+\\alpha)$ factor in every epoch for some $\\alpha >0$. (Precise statement in the\npaper.) To the best of our knowledge this is the first time such a rate\nschedule has been successfully used, let alone for highly successful\narchitectures. As expected, such training rapidly blows up network weights, but\nthe net stays well-behaved due to normalization.\n  2. Mathematical explanation of the success of the above rate schedule: a\nrigorous proof that it is equivalent to the standard setting of BN + SGD +\nStandardRate Tuning + Weight Decay + Momentum. This equivalence holds for other\nnormalization layers as well, Group Normalization, LayerNormalization, Instance\nNorm, etc.\n  3. A worked-out toy example illustrating the above linkage of\nhyper-parameters. Using either weight decay or BN alone reaches global minimum,\nbut convergence fails when both are used."}, "authors": ["Zhiyuan Li", "Sanjeev Arora"], "author_detail": {"name": "Sanjeev Arora"}, "author": "Sanjeev Arora", "links": [{"href": "http://arxiv.org/abs/1910.07454v3", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1910.07454v3", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.ML", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1910.07454v3", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1910.07454v3", "arxiv_comment": null, "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/2001.06838v2", "guidislink": true, "updated": "2020-04-08T10:06:09Z", "updated_parsed": [2020, 4, 8, 10, 6, 9, 2, 99, 0], "published": "2020-01-19T14:41:22Z", "published_parsed": [2020, 1, 19, 14, 41, 22, 6, 19, 0], "title": "Towards Stabilizing Batch Statistics in Backward Propagation of Batch\n  Normalization", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=210&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Towards Stabilizing Batch Statistics in Backward Propagation of Batch\n  Normalization"}, "summary": "Batch Normalization (BN) is one of the most widely used techniques in Deep\nLearning field. But its performance can awfully degrade with insufficient batch\nsize. This weakness limits the usage of BN on many computer vision tasks like\ndetection or segmentation, where batch size is usually small due to the\nconstraint of memory consumption. Therefore many modified normalization\ntechniques have been proposed, which either fail to restore the performance of\nBN completely, or have to introduce additional nonlinear operations in\ninference procedure and increase huge consumption. In this paper, we reveal\nthat there are two extra batch statistics involved in backward propagation of\nBN, on which has never been well discussed before. The extra batch statistics\nassociated with gradients also can severely affect the training of deep neural\nnetwork. Based on our analysis, we propose a novel normalization method, named\nMoving Average Batch Normalization (MABN). MABN can completely restore the\nperformance of vanilla BN in small batch cases, without introducing any\nadditional nonlinear operations in inference procedure. We prove the benefits\nof MABN by both theoretical analysis and experiments. Our experiments\ndemonstrate the effectiveness of MABN in multiple computer vision tasks\nincluding ImageNet and COCO. The code has been released in\nhttps://github.com/megvii-model/MABN.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=210&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Batch Normalization (BN) is one of the most widely used techniques in Deep\nLearning field. But its performance can awfully degrade with insufficient batch\nsize. This weakness limits the usage of BN on many computer vision tasks like\ndetection or segmentation, where batch size is usually small due to the\nconstraint of memory consumption. Therefore many modified normalization\ntechniques have been proposed, which either fail to restore the performance of\nBN completely, or have to introduce additional nonlinear operations in\ninference procedure and increase huge consumption. In this paper, we reveal\nthat there are two extra batch statistics involved in backward propagation of\nBN, on which has never been well discussed before. The extra batch statistics\nassociated with gradients also can severely affect the training of deep neural\nnetwork. Based on our analysis, we propose a novel normalization method, named\nMoving Average Batch Normalization (MABN). MABN can completely restore the\nperformance of vanilla BN in small batch cases, without introducing any\nadditional nonlinear operations in inference procedure. We prove the benefits\nof MABN by both theoretical analysis and experiments. Our experiments\ndemonstrate the effectiveness of MABN in multiple computer vision tasks\nincluding ImageNet and COCO. The code has been released in\nhttps://github.com/megvii-model/MABN."}, "authors": ["Junjie Yan", "Ruosi Wan", "Xiangyu Zhang", "Wei Zhang", "Yichen Wei", "Jian Sun"], "author_detail": {"name": "Jian Sun"}, "author": "Jian Sun", "arxiv_comment": "ICLR2020; https://github.com/megvii-model/MABN", "links": [{"href": "http://arxiv.org/abs/2001.06838v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2001.06838v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.ML", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2001.06838v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2001.06838v2", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/2101.07123v1", "guidislink": true, "updated": "2021-01-18T15:33:26Z", "updated_parsed": [2021, 1, 18, 15, 33, 26, 0, 18, 0], "published": "2021-01-18T15:33:26Z", "published_parsed": [2021, 1, 18, 15, 33, 26, 0, 18, 0], "title": "Learning Successor States and Goal-Dependent Values: A Mathematical\n  Viewpoint", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=220&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Learning Successor States and Goal-Dependent Values: A Mathematical\n  Viewpoint"}, "summary": "In reinforcement learning, temporal difference-based algorithms can be\nsample-inefficient: for instance, with sparse rewards, no learning occurs until\na reward is observed. This can be remedied by learning richer objects, such as\na model of the environment, or successor states. Successor states model the\nexpected future state occupancy from any given state for a given policy and are\nrelated to goal-dependent value functions, which learn how to reach arbitrary\nstates. We formally derive the temporal difference algorithm for successor\nstate and goal-dependent value function learning, either for discrete or for\ncontinuous environments with function approximation. Especially, we provide\nfinite-variance estimators even in continuous environments, where the reward\nfor exactly reaching a goal state becomes infinitely sparse. Successor states\nsatisfy more than just the Bellman equation: a backward Bellman operator and a\nBellman-Newton (BN) operator encode path compositionality in the environment.\nThe BN operator is akin to second-order gradient descent methods and provides\nthe true update of the value function when acquiring more observations, with\nexplicit tabular bounds. In the tabular case and with infinitesimal learning\nrates, mixing the usual and backward Bellman operators provably improves\neigenvalues for asymptotic convergence, and the asymptotic convergence of the\nBN operator is provably better than TD, with a rate independent from the\nenvironment. However, the BN method is more complex and less robust to sampling\nnoise. Finally, a forward-backward (FB) finite-rank parameterization of\nsuccessor states enjoys reduced variance and improved samplability, provides a\ndirect model of the value function, has fully understood fixed points\ncorresponding to long-range dependencies, approximates the BN method, and\nprovides two canonical representations of states as a byproduct.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=220&max_results=10&sortBy=relevance&sortOrder=descending", "value": "In reinforcement learning, temporal difference-based algorithms can be\nsample-inefficient: for instance, with sparse rewards, no learning occurs until\na reward is observed. This can be remedied by learning richer objects, such as\na model of the environment, or successor states. Successor states model the\nexpected future state occupancy from any given state for a given policy and are\nrelated to goal-dependent value functions, which learn how to reach arbitrary\nstates. We formally derive the temporal difference algorithm for successor\nstate and goal-dependent value function learning, either for discrete or for\ncontinuous environments with function approximation. Especially, we provide\nfinite-variance estimators even in continuous environments, where the reward\nfor exactly reaching a goal state becomes infinitely sparse. Successor states\nsatisfy more than just the Bellman equation: a backward Bellman operator and a\nBellman-Newton (BN) operator encode path compositionality in the environment.\nThe BN operator is akin to second-order gradient descent methods and provides\nthe true update of the value function when acquiring more observations, with\nexplicit tabular bounds. In the tabular case and with infinitesimal learning\nrates, mixing the usual and backward Bellman operators provably improves\neigenvalues for asymptotic convergence, and the asymptotic convergence of the\nBN operator is provably better than TD, with a rate independent from the\nenvironment. However, the BN method is more complex and less robust to sampling\nnoise. Finally, a forward-backward (FB) finite-rank parameterization of\nsuccessor states enjoys reduced variance and improved samplability, provides a\ndirect model of the value function, has fully understood fixed points\ncorresponding to long-range dependencies, approximates the BN method, and\nprovides two canonical representations of states as a byproduct."}, "authors": ["Lonard Blier", "Corentin Tallec", "Yann Ollivier"], "author_detail": {"name": "Yann Ollivier"}, "author": "Yann Ollivier", "links": [{"href": "http://arxiv.org/abs/2101.07123v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2101.07123v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2101.07123v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2101.07123v1", "arxiv_comment": null, "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/2012.02782v2", "guidislink": true, "updated": "2020-12-09T01:26:51Z", "updated_parsed": [2020, 12, 9, 1, 26, 51, 2, 344, 0], "published": "2020-12-04T18:57:52Z", "published_parsed": [2020, 12, 4, 18, 57, 52, 4, 339, 0], "title": "Batch Group Normalization", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=220&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Batch Group Normalization"}, "summary": "Deep Convolutional Neural Networks (DCNNs) are hard and time-consuming to\ntrain. Normalization is one of the effective solutions. Among previous\nnormalization methods, Batch Normalization (BN) performs well at medium and\nlarge batch sizes and is with good generalizability to multiple vision tasks,\nwhile its performance degrades significantly at small batch sizes. In this\npaper, we find that BN saturates at extreme large batch sizes, i.e., 128 images\nper worker, i.e., GPU, as well and propose that the degradation/saturation of\nBN at small/extreme large batch sizes is caused by noisy/confused statistic\ncalculation. Hence without adding new trainable parameters, using\nmultiple-layer or multi-iteration information, or introducing extra\ncomputation, Batch Group Normalization (BGN) is proposed to solve the\nnoisy/confused statistic calculation of BN at small/extreme large batch sizes\nwith introducing the channel, height and width dimension to compensate. The\ngroup technique in Group Normalization (GN) is used and a hyper-parameter G is\nused to control the number of feature instances used for statistic calculation,\nhence to offer neither noisy nor confused statistic for different batch sizes.\nWe empirically demonstrate that BGN consistently outperforms BN, Instance\nNormalization (IN), Layer Normalization (LN), GN, and Positional Normalization\n(PN), across a wide spectrum of vision tasks, including image classification,\nNeural Architecture Search (NAS), adversarial learning, Few Shot Learning (FSL)\nand Unsupervised Domain Adaptation (UDA), indicating its good performance,\nrobust stability to batch size and wide generalizability. For example, for\ntraining ResNet-50 on ImageNet with a batch size of 2, BN achieves Top1\naccuracy of 66.512% while BGN achieves 76.096% with notable improvement.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=220&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Deep Convolutional Neural Networks (DCNNs) are hard and time-consuming to\ntrain. Normalization is one of the effective solutions. Among previous\nnormalization methods, Batch Normalization (BN) performs well at medium and\nlarge batch sizes and is with good generalizability to multiple vision tasks,\nwhile its performance degrades significantly at small batch sizes. In this\npaper, we find that BN saturates at extreme large batch sizes, i.e., 128 images\nper worker, i.e., GPU, as well and propose that the degradation/saturation of\nBN at small/extreme large batch sizes is caused by noisy/confused statistic\ncalculation. Hence without adding new trainable parameters, using\nmultiple-layer or multi-iteration information, or introducing extra\ncomputation, Batch Group Normalization (BGN) is proposed to solve the\nnoisy/confused statistic calculation of BN at small/extreme large batch sizes\nwith introducing the channel, height and width dimension to compensate. The\ngroup technique in Group Normalization (GN) is used and a hyper-parameter G is\nused to control the number of feature instances used for statistic calculation,\nhence to offer neither noisy nor confused statistic for different batch sizes.\nWe empirically demonstrate that BGN consistently outperforms BN, Instance\nNormalization (IN), Layer Normalization (LN), GN, and Positional Normalization\n(PN), across a wide spectrum of vision tasks, including image classification,\nNeural Architecture Search (NAS), adversarial learning, Few Shot Learning (FSL)\nand Unsupervised Domain Adaptation (UDA), indicating its good performance,\nrobust stability to batch size and wide generalizability. For example, for\ntraining ResNet-50 on ImageNet with a batch size of 2, BN achieves Top1\naccuracy of 66.512% while BGN achieves 76.096% with notable improvement."}, "authors": ["Xiao-Yun Zhou", "Jiacheng Sun", "Nanyang Ye", "Xu Lan", "Qijun Luo", "Bo-Lin Lai", "Pedro Esperanca", "Guang-Zhong Yang", "Zhenguo Li"], "author_detail": {"name": "Zhenguo Li"}, "author": "Zhenguo Li", "arxiv_comment": "8 pages", "links": [{"href": "http://arxiv.org/abs/2012.02782v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2012.02782v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2012.02782v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2012.02782v2", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1202.3713v1", "guidislink": true, "updated": "2012-02-14T16:41:17Z", "updated_parsed": [2012, 2, 14, 16, 41, 17, 1, 45, 0], "published": "2012-02-14T16:41:17Z", "published_parsed": [2012, 2, 14, 16, 41, 17, 1, 45, 0], "title": "Bayesian network learning with cutting planes", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=240&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Bayesian network learning with cutting planes"}, "summary": "The problem of learning the structure of Bayesian networks from complete\ndiscrete data with a limit on parent set size is considered. Learning is cast\nexplicitly as an optimisation problem where the goal is to find a BN structure\nwhich maximises log marginal likelihood (BDe score). Integer programming,\nspecifically the SCIP framework, is used to solve this optimisation problem.\nAcyclicity constraints are added to the integer program (IP) during solving in\nthe form of cutting planes. Finding good cutting planes is the key to the\nsuccess of the approach -the search for such cutting planes is effected using a\nsub-IP. Results show that this is a particularly fast method for exact BN\nlearning.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=240&max_results=10&sortBy=relevance&sortOrder=descending", "value": "The problem of learning the structure of Bayesian networks from complete\ndiscrete data with a limit on parent set size is considered. Learning is cast\nexplicitly as an optimisation problem where the goal is to find a BN structure\nwhich maximises log marginal likelihood (BDe score). Integer programming,\nspecifically the SCIP framework, is used to solve this optimisation problem.\nAcyclicity constraints are added to the integer program (IP) during solving in\nthe form of cutting planes. Finding good cutting planes is the key to the\nsuccess of the approach -the search for such cutting planes is effected using a\nsub-IP. Results show that this is a particularly fast method for exact BN\nlearning."}, "authors": ["James Cussens"], "author_detail": {"name": "James Cussens"}, "author": "James Cussens", "links": [{"href": "http://arxiv.org/abs/1202.3713v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1202.3713v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1202.3713v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1202.3713v1", "arxiv_comment": null, "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1211.4433v1", "guidislink": true, "updated": "2012-11-19T14:19:38Z", "updated_parsed": [2012, 11, 19, 14, 19, 38, 0, 324, 0], "published": "2012-11-19T14:19:38Z", "published_parsed": [2012, 11, 19, 14, 19, 38, 0, 324, 0], "title": "An upper bound for the crossing number of bubble-sort graph Bn", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=240&max_results=10&sortBy=relevance&sortOrder=descending", "value": "An upper bound for the crossing number of bubble-sort graph Bn"}, "summary": "The crossing number of a graph G is the minimum number of pairwise\nintersections of edges in a drawing of G. Motivated by the recent work [Faria,\nL., Figueiredo, C.M.H. de, Sykora, O., Vrt'o, I.: An improved upper bound on\nthe crossing number of the hypercube. J. Graph Theory 59, 145-161 (2008)], we\ngive an upper bound of the crossing number of n-dimensional bubble-sort graph\nBn.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=240&max_results=10&sortBy=relevance&sortOrder=descending", "value": "The crossing number of a graph G is the minimum number of pairwise\nintersections of edges in a drawing of G. Motivated by the recent work [Faria,\nL., Figueiredo, C.M.H. de, Sykora, O., Vrt'o, I.: An improved upper bound on\nthe crossing number of the hypercube. J. Graph Theory 59, 145-161 (2008)], we\ngive an upper bound of the crossing number of n-dimensional bubble-sort graph\nBn."}, "authors": ["Baigong Zheng", "Yuansheng Yang", "Xirong Xu"], "author_detail": {"name": "Xirong Xu"}, "author": "Xirong Xu", "arxiv_comment": "20 pages, 10 figures", "links": [{"href": "http://arxiv.org/abs/1211.4433v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1211.4433v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.DM", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.DM", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "math.CO", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1211.4433v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1211.4433v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1710.09681v1", "guidislink": true, "updated": "2017-10-25T02:00:49Z", "updated_parsed": [2017, 10, 25, 2, 0, 49, 2, 298, 0], "published": "2017-10-25T02:00:49Z", "published_parsed": [2017, 10, 25, 2, 0, 49, 2, 298, 0], "title": "Reconstruct the Logical Network from the Transition Matrix", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=240&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Reconstruct the Logical Network from the Transition Matrix"}, "summary": "Reconstructing the logical network from the transition matrix is benefit for\nlearning the logical meaning of the algebraic result from the algebraic\nrepresentation of a BN. And so far there has no method to convert the matrix\nexpression back to the logic expression for a BN with an arbitrary topology\nstructure. Based on the canonical form and Karnaugh map, we propose a method\nfor reconstructing the logical network from the transition matrix of a Boolean\nnetwork in this paper.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=240&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Reconstructing the logical network from the transition matrix is benefit for\nlearning the logical meaning of the algebraic result from the algebraic\nrepresentation of a BN. And so far there has no method to convert the matrix\nexpression back to the logic expression for a BN with an arbitrary topology\nstructure. Based on the canonical form and Karnaugh map, we propose a method\nfor reconstructing the logical network from the transition matrix of a Boolean\nnetwork in this paper."}, "authors": ["Cailu Wang", "Yuegang Tao"], "author_detail": {"name": "Yuegang Tao"}, "author": "Yuegang Tao", "links": [{"href": "http://arxiv.org/abs/1710.09681v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1710.09681v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.SY", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.SY", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "math.OC", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1710.09681v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1710.09681v1", "arxiv_comment": null, "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1807.06305v1", "guidislink": true, "updated": "2018-07-17T09:48:37Z", "updated_parsed": [2018, 7, 17, 9, 48, 37, 1, 198, 0], "published": "2018-07-17T09:48:37Z", "published_parsed": [2018, 7, 17, 9, 48, 37, 1, 198, 0], "title": "Unifying Inference for Bayesian and Petri Nets", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=240&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Unifying Inference for Bayesian and Petri Nets"}, "summary": "Recent work by the authors equips Petri occurrence nets (PN) with probability\ndistributions which fully replace nondeterminism. To avoid the so-called\nconfusion problem, the construction imposes additional causal dependencies\nwhich restrict choices within certain subnets called structural branching cells\n(s-cells). Bayesian nets (BN) are usually structured as partial orders where\nnodes define conditional probability distributions. In the paper, we unify the\ntwo structures in terms of Symmetric Monoidal Categories (SMC), so that we can\napply to PN ordinary analysis techniques developed for BN. Interestingly, it\nturns out that PN which cannot be SMC-decomposed are exactly s-cells. This\nresult confirms the importance for Petri nets of both SMC and s-cells.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=240&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Recent work by the authors equips Petri occurrence nets (PN) with probability\ndistributions which fully replace nondeterminism. To avoid the so-called\nconfusion problem, the construction imposes additional causal dependencies\nwhich restrict choices within certain subnets called structural branching cells\n(s-cells). Bayesian nets (BN) are usually structured as partial orders where\nnodes define conditional probability distributions. In the paper, we unify the\ntwo structures in terms of Symmetric Monoidal Categories (SMC), so that we can\napply to PN ordinary analysis techniques developed for BN. Interestingly, it\nturns out that PN which cannot be SMC-decomposed are exactly s-cells. This\nresult confirms the importance for Petri nets of both SMC and s-cells."}, "authors": ["Roberto Bruni", "Hernn Melgratti", "Ugo Montanari"], "author_detail": {"name": "Ugo Montanari"}, "author": "Ugo Montanari", "arxiv_comment": "27 pages", "links": [{"href": "http://arxiv.org/abs/1807.06305v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1807.06305v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LO", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LO", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "18B99", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "F.1.1; F.1.2", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1807.06305v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1807.06305v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1408.0765v2", "guidislink": true, "updated": "2014-08-20T02:04:25Z", "updated_parsed": [2014, 8, 20, 2, 4, 25, 2, 232, 0], "published": "2014-08-04T18:47:24Z", "published_parsed": [2014, 8, 4, 18, 47, 24, 0, 216, 0], "title": "Modulation Classification via Gibbs Sampling Based on a Latent Dirichlet\n  Bayesian Network", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=250&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Modulation Classification via Gibbs Sampling Based on a Latent Dirichlet\n  Bayesian Network"}, "summary": "A novel Bayesian modulation classification scheme is proposed for a\nsingle-antenna system over frequency-selective fading channels. The method is\nbased on Gibbs sampling as applied to a latent Dirichlet Bayesian network (BN).\nThe use of the proposed latent Dirichlet BN provides a systematic solution to\nthe convergence problem encountered by the conventional Gibbs sampling approach\nfor modulation classification. The method generalizes, and is shown to improve\nupon, the state of the art.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=250&max_results=10&sortBy=relevance&sortOrder=descending", "value": "A novel Bayesian modulation classification scheme is proposed for a\nsingle-antenna system over frequency-selective fading channels. The method is\nbased on Gibbs sampling as applied to a latent Dirichlet Bayesian network (BN).\nThe use of the proposed latent Dirichlet BN provides a systematic solution to\nthe convergence problem encountered by the conventional Gibbs sampling approach\nfor modulation classification. The method generalizes, and is shown to improve\nupon, the state of the art."}, "authors": ["Yu Liu", "Osvaldo Simeone", "Alexander M. Haimovich", "Wei Su"], "author_detail": {"name": "Wei Su"}, "author": "Wei Su", "links": [{"title": "doi", "href": "http://dx.doi.org/10.1109/LSP.2014.2327193", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/1408.0765v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1408.0765v2", "rel": "related", "type": "application/pdf"}], "arxiv_comment": "Contains corrections with respect to the version to appear on IEEE\n  Signal Processing Letters (see Fig. 2)", "arxiv_primary_category": {"term": "cs.IT", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.IT", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "math.IT", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1408.0765v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1408.0765v2", "journal_reference": null, "doi": "10.1109/LSP.2014.2327193"}
{"id": "http://arxiv.org/abs/1810.05466v1", "guidislink": true, "updated": "2018-10-12T12:10:10Z", "updated_parsed": [2018, 10, 12, 12, 10, 10, 4, 285, 0], "published": "2018-10-12T12:10:10Z", "published_parsed": [2018, 10, 12, 12, 10, 10, 4, 285, 0], "title": "Mode Normalization", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=250&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Mode Normalization"}, "summary": "Normalization methods are a central building block in the deep learning\ntoolbox. They accelerate and stabilize training, while decreasing the\ndependence on manually tuned learning rate schedules. When learning from\nmulti-modal distributions, the effectiveness of batch normalization (BN),\narguably the most prominent normalization method, is reduced. As a remedy, we\npropose a more flexible approach: by extending the normalization to more than a\nsingle mean and variance, we detect modes of data on-the-fly, jointly\nnormalizing samples that share common features. We demonstrate that our method\noutperforms BN and other widely used normalization techniques in several\nexperiments, including single and multi-task datasets.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=250&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Normalization methods are a central building block in the deep learning\ntoolbox. They accelerate and stabilize training, while decreasing the\ndependence on manually tuned learning rate schedules. When learning from\nmulti-modal distributions, the effectiveness of batch normalization (BN),\narguably the most prominent normalization method, is reduced. As a remedy, we\npropose a more flexible approach: by extending the normalization to more than a\nsingle mean and variance, we detect modes of data on-the-fly, jointly\nnormalizing samples that share common features. We demonstrate that our method\noutperforms BN and other widely used normalization techniques in several\nexperiments, including single and multi-task datasets."}, "authors": ["Lucas Deecke", "Iain Murray", "Hakan Bilen"], "author_detail": {"name": "Hakan Bilen"}, "author": "Hakan Bilen", "links": [{"href": "http://arxiv.org/abs/1810.05466v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1810.05466v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.ML", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1810.05466v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1810.05466v1", "arxiv_comment": null, "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1811.00639v1", "guidislink": true, "updated": "2018-11-01T21:30:39Z", "updated_parsed": [2018, 11, 1, 21, 30, 39, 3, 305, 0], "published": "2018-11-01T21:30:39Z", "published_parsed": [2018, 11, 1, 21, 30, 39, 3, 305, 0], "title": "Stochastic Normalizations as Bayesian Learning", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=250&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Stochastic Normalizations as Bayesian Learning"}, "summary": "In this work we investigate the reasons why Batch Normalization (BN) improves\nthe generalization performance of deep networks. We argue that one major\nreason, distinguishing it from data-independent normalization methods, is\nrandomness of batch statistics. This randomness appears in the parameters\nrather than in activations and admits an interpretation as a practical Bayesian\nlearning. We apply this idea to other (deterministic) normalization techniques\nthat are oblivious to the batch size. We show that their generalization\nperformance can be improved significantly by Bayesian learning of the same\nform. We obtain test performance comparable to BN and, at the same time, better\nvalidation losses suitable for subsequent output uncertainty estimation through\napproximate Bayesian posterior.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=250&max_results=10&sortBy=relevance&sortOrder=descending", "value": "In this work we investigate the reasons why Batch Normalization (BN) improves\nthe generalization performance of deep networks. We argue that one major\nreason, distinguishing it from data-independent normalization methods, is\nrandomness of batch statistics. This randomness appears in the parameters\nrather than in activations and admits an interpretation as a practical Bayesian\nlearning. We apply this idea to other (deterministic) normalization techniques\nthat are oblivious to the batch size. We show that their generalization\nperformance can be improved significantly by Bayesian learning of the same\nform. We obtain test performance comparable to BN and, at the same time, better\nvalidation losses suitable for subsequent output uncertainty estimation through\napproximate Bayesian posterior."}, "authors": ["Alexander Shekhovtsov", "Boris Flach"], "author_detail": {"name": "Boris Flach"}, "author": "Boris Flach", "arxiv_comment": "Accepted to ACCV 2018", "links": [{"href": "http://arxiv.org/abs/1811.00639v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1811.00639v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.NE", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.ML", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1811.00639v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1811.00639v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1812.00342v1", "guidislink": true, "updated": "2018-12-02T06:41:28Z", "updated_parsed": [2018, 12, 2, 6, 41, 28, 6, 336, 0], "published": "2018-12-02T06:41:28Z", "published_parsed": [2018, 12, 2, 6, 41, 28, 6, 336, 0], "title": "Analysis on Gradient Propagation in Batch Normalized Residual Networks", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=250&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Analysis on Gradient Propagation in Batch Normalized Residual Networks"}, "summary": "We conduct mathematical analysis on the effect of batch normalization (BN) on\ngradient backpropogation in residual network training, which is believed to\nplay a critical role in addressing the gradient vanishing/explosion problem, in\nthis work. By analyzing the mean and variance behavior of the input and the\ngradient in the forward and backward passes through the BN and residual\nbranches, respectively, we show that they work together to confine the gradient\nvariance to a certain range across residual blocks in backpropagation. As a\nresult, the gradient vanishing/explosion problem is avoided. We also show the\nrelative importance of batch normalization w.r.t. the residual branches in\nresidual networks.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=250&max_results=10&sortBy=relevance&sortOrder=descending", "value": "We conduct mathematical analysis on the effect of batch normalization (BN) on\ngradient backpropogation in residual network training, which is believed to\nplay a critical role in addressing the gradient vanishing/explosion problem, in\nthis work. By analyzing the mean and variance behavior of the input and the\ngradient in the forward and backward passes through the BN and residual\nbranches, respectively, we show that they work together to confine the gradient\nvariance to a certain range across residual blocks in backpropagation. As a\nresult, the gradient vanishing/explosion problem is avoided. We also show the\nrelative importance of batch normalization w.r.t. the residual branches in\nresidual networks."}, "authors": ["Abhishek Panigrahi", "Yueru Chen", "C. -C. Jay Kuo"], "author_detail": {"name": "C. -C. Jay Kuo"}, "author": "C. -C. Jay Kuo", "links": [{"href": "http://arxiv.org/abs/1812.00342v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1812.00342v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.ML", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1812.00342v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1812.00342v1", "arxiv_comment": null, "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1106.0253v1", "guidislink": true, "updated": "2011-06-01T16:40:57Z", "updated_parsed": [2011, 6, 1, 16, 40, 57, 2, 152, 0], "published": "2011-06-01T16:40:57Z", "published_parsed": [2011, 6, 1, 16, 40, 57, 2, 152, 0], "title": "AIS-BN: An Adaptive Importance Sampling Algorithm for Evidential\n  Reasoning in Large Bayesian Networks", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=260&max_results=10&sortBy=relevance&sortOrder=descending", "value": "AIS-BN: An Adaptive Importance Sampling Algorithm for Evidential\n  Reasoning in Large Bayesian Networks"}, "summary": "Stochastic sampling algorithms, while an attractive alternative to exact\nalgorithms in very large Bayesian network models, have been observed to perform\npoorly in evidential reasoning with extremely unlikely evidence. To address\nthis problem, we propose an adaptive importance sampling algorithm, AIS-BN,\nthat shows promising convergence rates even under extreme conditions and seems\nto outperform the existing sampling algorithms consistently. Three sources of\nthis performance improvement are (1) two heuristics for initialization of the\nimportance function that are based on the theoretical properties of importance\nsampling in finite-dimensional integrals and the structural advantages of\nBayesian networks, (2) a smooth learning method for the importance function,\nand (3) a dynamic weighting function for combining samples from different\nstages of the algorithm. We tested the performance of the AIS-BN algorithm\nalong with two state of the art general purpose sampling algorithms, likelihood\nweighting (Fung and Chang, 1989; Shachter and Peot, 1989) and self-importance\nsampling (Shachter and Peot, 1989). We used in our tests three large real\nBayesian network models available to the scientific community: the CPCS network\n(Pradhan et al., 1994), the PathFinder network (Heckerman, Horvitz, and\nNathwani, 1990), and the ANDES network (Conati, Gertner, VanLehn, and Druzdzel,\n1997), with evidence as unlikely as 10^-41. While the AIS-BN algorithm always\nperformed better than the other two algorithms, in the majority of the test\ncases it achieved orders of magnitude improvement in precision of the results.\nImprovement in speed given a desired precision is even more dramatic, although\nwe are unable to report numerical results here, as the other algorithms almost\nnever achieved the precision reached even by the first few iterations of the\nAIS-BN algorithm.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=260&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Stochastic sampling algorithms, while an attractive alternative to exact\nalgorithms in very large Bayesian network models, have been observed to perform\npoorly in evidential reasoning with extremely unlikely evidence. To address\nthis problem, we propose an adaptive importance sampling algorithm, AIS-BN,\nthat shows promising convergence rates even under extreme conditions and seems\nto outperform the existing sampling algorithms consistently. Three sources of\nthis performance improvement are (1) two heuristics for initialization of the\nimportance function that are based on the theoretical properties of importance\nsampling in finite-dimensional integrals and the structural advantages of\nBayesian networks, (2) a smooth learning method for the importance function,\nand (3) a dynamic weighting function for combining samples from different\nstages of the algorithm. We tested the performance of the AIS-BN algorithm\nalong with two state of the art general purpose sampling algorithms, likelihood\nweighting (Fung and Chang, 1989; Shachter and Peot, 1989) and self-importance\nsampling (Shachter and Peot, 1989). We used in our tests three large real\nBayesian network models available to the scientific community: the CPCS network\n(Pradhan et al., 1994), the PathFinder network (Heckerman, Horvitz, and\nNathwani, 1990), and the ANDES network (Conati, Gertner, VanLehn, and Druzdzel,\n1997), with evidence as unlikely as 10^-41. While the AIS-BN algorithm always\nperformed better than the other two algorithms, in the majority of the test\ncases it achieved orders of magnitude improvement in precision of the results.\nImprovement in speed given a desired precision is even more dramatic, although\nwe are unable to report numerical results here, as the other algorithms almost\nnever achieved the precision reached even by the first few iterations of the\nAIS-BN algorithm."}, "authors": ["J. Cheng", "M. J. Druzdzel"], "author_detail": {"name": "M. J. Druzdzel"}, "author": "M. J. Druzdzel", "links": [{"title": "doi", "href": "http://dx.doi.org/10.1613/jair.764", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/1106.0253v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1106.0253v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1106.0253v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1106.0253v1", "arxiv_comment": null, "journal_reference": "Journal Of Artificial Intelligence Research, Volume 13, pages\n  155-188, 2000", "doi": "10.1613/jair.764"}
{"id": "http://arxiv.org/abs/1305.6046v1", "guidislink": true, "updated": "2013-05-26T18:16:52Z", "updated_parsed": [2013, 5, 26, 18, 16, 52, 6, 146, 0], "published": "2013-05-26T18:16:52Z", "published_parsed": [2013, 5, 26, 18, 16, 52, 6, 146, 0], "title": "Supervised Feature Selection for Diagnosis of Coronary Artery Disease\n  Based on Genetic Algorithm", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=260&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Supervised Feature Selection for Diagnosis of Coronary Artery Disease\n  Based on Genetic Algorithm"}, "summary": "Feature Selection (FS) has become the focus of much research on decision\nsupport systems areas for which data sets with tremendous number of variables\nare analyzed. In this paper we present a new method for the diagnosis of\nCoronary Artery Diseases (CAD) founded on Genetic Algorithm (GA) wrapped Bayes\nNaive (BN) based FS. Basically, CAD dataset contains two classes defined with\n13 features. In GA BN algorithm, GA generates in each iteration a subset of\nattributes that will be evaluated using the BN in the second step of the\nselection procedure. The final set of attribute contains the most relevant\nfeature model that increases the accuracy. The algorithm in this case produces\n85.50% classification accuracy in the diagnosis of CAD. Thus, the asset of the\nAlgorithm is then compared with the use of Support Vector Machine (SVM),\nMultiLayer Perceptron (MLP) and C4.5 decision tree Algorithm. The result of\nclassification accuracy for those algorithms are respectively 83.5%, 83.16% and\n80.85%. Consequently, the GA wrapped BN Algorithm is correspondingly compared\nwith other FS algorithms. The Obtained results have shown very promising\noutcomes for the diagnosis of CAD.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=260&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Feature Selection (FS) has become the focus of much research on decision\nsupport systems areas for which data sets with tremendous number of variables\nare analyzed. In this paper we present a new method for the diagnosis of\nCoronary Artery Diseases (CAD) founded on Genetic Algorithm (GA) wrapped Bayes\nNaive (BN) based FS. Basically, CAD dataset contains two classes defined with\n13 features. In GA BN algorithm, GA generates in each iteration a subset of\nattributes that will be evaluated using the BN in the second step of the\nselection procedure. The final set of attribute contains the most relevant\nfeature model that increases the accuracy. The algorithm in this case produces\n85.50% classification accuracy in the diagnosis of CAD. Thus, the asset of the\nAlgorithm is then compared with the use of Support Vector Machine (SVM),\nMultiLayer Perceptron (MLP) and C4.5 decision tree Algorithm. The result of\nclassification accuracy for those algorithms are respectively 83.5%, 83.16% and\n80.85%. Consequently, the GA wrapped BN Algorithm is correspondingly compared\nwith other FS algorithms. The Obtained results have shown very promising\noutcomes for the diagnosis of CAD."}, "authors": ["Sidahmed Mokeddem", "Baghdad Atmani", "Mostefa Mokaddem"], "author_detail": {"name": "Mostefa Mokaddem"}, "author": "Mostefa Mokaddem", "links": [{"title": "doi", "href": "http://dx.doi.org/10.5121/csit.2013.3305", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/1305.6046v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1305.6046v1", "rel": "related", "type": "application/pdf"}], "arxiv_comment": "First International Conference on Computational Science and\n  Engineering (CSE-2013), May 18 ~ 19, 2013, Dubai, UAE. Volume Editors:\n  Sundarapandian Vaidyanathan, Dhinaharan Nagamalai", "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.CE", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1305.6046v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1305.6046v1", "journal_reference": null, "doi": "10.5121/csit.2013.3305"}
{"id": "http://arxiv.org/abs/1804.09858v1", "guidislink": true, "updated": "2018-04-26T02:28:34Z", "updated_parsed": [2018, 4, 26, 2, 28, 34, 3, 116, 0], "published": "2018-04-26T02:28:34Z", "published_parsed": [2018, 4, 26, 2, 28, 34, 3, 116, 0], "title": "Generative Model for Heterogeneous Inference", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=260&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Generative Model for Heterogeneous Inference"}, "summary": "Generative models (GMs) such as Generative Adversary Network (GAN) and\nVariational Auto-Encoder (VAE) have thrived these years and achieved high\nquality results in generating new samples. Especially in Computer Vision, GMs\nhave been used in image inpainting, denoising and completion, which can be\ntreated as the inference from observed pixels to corrupted pixels. However,\nimages are hierarchically structured which are quite different from many\nreal-world inference scenarios with non-hierarchical features. These inference\nscenarios contain heterogeneous stochastic variables and irregular mutual\ndependences. Traditionally they are modeled by Bayesian Network (BN). However,\nthe learning and inference of BN model are NP-hard thus the number of\nstochastic variables in BN is highly constrained. In this paper, we adapt\ntypical GMs to enable heterogeneous learning and inference in polynomial\ntime.We also propose an extended autoregressive (EAR) model and an EAR with\nadversary loss (EARA) model and give theoretical results on their\neffectiveness. Experiments on several BN datasets show that our proposed EAR\nmodel achieves the best performance in most cases compared to other GMs. Except\nfor black box analysis, we've also done a serial of experiments on Markov\nborder inference of GMs for white box analysis and give theoretical results.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=260&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Generative models (GMs) such as Generative Adversary Network (GAN) and\nVariational Auto-Encoder (VAE) have thrived these years and achieved high\nquality results in generating new samples. Especially in Computer Vision, GMs\nhave been used in image inpainting, denoising and completion, which can be\ntreated as the inference from observed pixels to corrupted pixels. However,\nimages are hierarchically structured which are quite different from many\nreal-world inference scenarios with non-hierarchical features. These inference\nscenarios contain heterogeneous stochastic variables and irregular mutual\ndependences. Traditionally they are modeled by Bayesian Network (BN). However,\nthe learning and inference of BN model are NP-hard thus the number of\nstochastic variables in BN is highly constrained. In this paper, we adapt\ntypical GMs to enable heterogeneous learning and inference in polynomial\ntime.We also propose an extended autoregressive (EAR) model and an EAR with\nadversary loss (EARA) model and give theoretical results on their\neffectiveness. Experiments on several BN datasets show that our proposed EAR\nmodel achieves the best performance in most cases compared to other GMs. Except\nfor black box analysis, we've also done a serial of experiments on Markov\nborder inference of GMs for white box analysis and give theoretical results."}, "authors": ["Honggang Zhou", "Yunchun Li", "Hailong Yang", "Wei Li", "Jie Jia"], "author_detail": {"name": "Jie Jia"}, "author": "Jie Jia", "links": [{"href": "http://arxiv.org/abs/1804.09858v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1804.09858v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.ML", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1804.09858v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1804.09858v1", "arxiv_comment": null, "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1409.7930v5", "guidislink": true, "updated": "2015-02-09T13:01:07Z", "updated_parsed": [2015, 2, 9, 13, 1, 7, 0, 40, 0], "published": "2014-09-28T16:36:06Z", "published_parsed": [2014, 9, 28, 16, 36, 6, 6, 271, 0], "title": "Cognitive Learning of Statistical Primary Patterns via Bayesian Network", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=270&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Cognitive Learning of Statistical Primary Patterns via Bayesian Network"}, "summary": "In cognitive radio (CR) technology, the trend of sensing is no longer to only\ndetect the presence of active primary users. A large number of applications\ndemand for more comprehensive knowledge on primary user behaviors in spatial,\ntemporal, and frequency domains. To satisfy such requirements, we study the\nstatistical relationship among primary users by introducing a Bayesian network\n(BN) based framework. How to learn such a BN structure is a long standing\nissue, not fully understood even in the statistical learning community.\nBesides, another key problem in this learning scenario is that the CR has to\nidentify how many variables are in the BN, which is usually considered as prior\nknowledge in statistical learning applications. To solve such two issues\nsimultaneously, this paper proposes a BN structure learning scheme consisting\nof an efficient structure learning algorithm and a blind variable\nidentification scheme. The proposed approach incurs significantly lower\ncomputational complexity compared with previous ones, and is capable of\ndetermining the structure without assuming much prior knowledge about\nvariables. With this result, cognitive users could efficiently understand the\nstatistical pattern of primary networks, such that more efficient cognitive\nprotocols could be designed across different network layers.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=270&max_results=10&sortBy=relevance&sortOrder=descending", "value": "In cognitive radio (CR) technology, the trend of sensing is no longer to only\ndetect the presence of active primary users. A large number of applications\ndemand for more comprehensive knowledge on primary user behaviors in spatial,\ntemporal, and frequency domains. To satisfy such requirements, we study the\nstatistical relationship among primary users by introducing a Bayesian network\n(BN) based framework. How to learn such a BN structure is a long standing\nissue, not fully understood even in the statistical learning community.\nBesides, another key problem in this learning scenario is that the CR has to\nidentify how many variables are in the BN, which is usually considered as prior\nknowledge in statistical learning applications. To solve such two issues\nsimultaneously, this paper proposes a BN structure learning scheme consisting\nof an efficient structure learning algorithm and a blind variable\nidentification scheme. The proposed approach incurs significantly lower\ncomputational complexity compared with previous ones, and is capable of\ndetermining the structure without assuming much prior knowledge about\nvariables. With this result, cognitive users could efficiently understand the\nstatistical pattern of primary networks, such that more efficient cognitive\nprotocols could be designed across different network layers."}, "authors": ["Weijia Han", "Huiyan Sang", "Min Sheng", "Jiandong Li", "Shuguang Cui"], "author_detail": {"name": "Shuguang Cui"}, "author": "Shuguang Cui", "arxiv_comment": "This paper has been refreshed with a new version", "links": [{"href": "http://arxiv.org/abs/1409.7930v5", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1409.7930v5", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1409.7930v5", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1409.7930v5", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1709.01602v1", "guidislink": true, "updated": "2017-09-05T21:41:58Z", "updated_parsed": [2017, 9, 5, 21, 41, 58, 1, 248, 0], "published": "2017-09-05T21:41:58Z", "published_parsed": [2017, 9, 5, 21, 41, 58, 1, 248, 0], "title": "Dynamic Multiscale Tree Learning Using Ensemble Strong Classifiers for\n  Multi-label Segmentation of Medical Images with Lesions", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=270&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Dynamic Multiscale Tree Learning Using Ensemble Strong Classifiers for\n  Multi-label Segmentation of Medical Images with Lesions"}, "summary": "We introduce a dynamic multiscale tree (DMT) architecture that learns how to\nleverage the strengths of different existing classifiers for supervised\nmulti-label image segmentation. Unlike previous works that simply aggregate or\ncascade classifiers for addressing image segmentation and labeling tasks, we\npropose to embed strong classifiers into a tree structure that allows\nbi-directional flow of information between its classifier nodes to gradually\nimprove their performances. Our DMT is a generic classification model that\ninherently embeds different cascades of classifiers while enhancing learning\ntransfer between them to boost up their classification accuracies.\nSpecifically, each node in our DMT can nest a Structured Random Forest (SRF)\nclassifier or a Bayesian Network (BN) classifier. The proposed SRF-BN DMT\narchitecture has several appealing properties. First, while SRF operates at a\npatch-level (regular image region), BN operates at the super-pixel level\n(irregular image region), thereby enabling the DMT to integrate multi-level\nimage knowledge in the learning process. Second, although BN is powerful in\nmodeling dependencies between image elements (superpixels, edges) and their\nfeatures, the learning of its structure and parameters is challenging. On the\nother hand, SRF may fail to accurately detect very irregular object boundaries.\nThe proposed DMT robustly overcomes these limitations for both classifiers\nthrough the ascending and descending flow of contextual information between\neach parent node and its children nodes. Third, we train DMT using different\nscales, where we progressively decrease the patch and superpixel sizes as we go\ndeeper along the tree edges nearing its leaf nodes. Last, DMT demonstrates its\noutperformance in comparison to several state-of-the-art segmentation methods\nfor multi-labeling of brain images with gliomas.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=270&max_results=10&sortBy=relevance&sortOrder=descending", "value": "We introduce a dynamic multiscale tree (DMT) architecture that learns how to\nleverage the strengths of different existing classifiers for supervised\nmulti-label image segmentation. Unlike previous works that simply aggregate or\ncascade classifiers for addressing image segmentation and labeling tasks, we\npropose to embed strong classifiers into a tree structure that allows\nbi-directional flow of information between its classifier nodes to gradually\nimprove their performances. Our DMT is a generic classification model that\ninherently embeds different cascades of classifiers while enhancing learning\ntransfer between them to boost up their classification accuracies.\nSpecifically, each node in our DMT can nest a Structured Random Forest (SRF)\nclassifier or a Bayesian Network (BN) classifier. The proposed SRF-BN DMT\narchitecture has several appealing properties. First, while SRF operates at a\npatch-level (regular image region), BN operates at the super-pixel level\n(irregular image region), thereby enabling the DMT to integrate multi-level\nimage knowledge in the learning process. Second, although BN is powerful in\nmodeling dependencies between image elements (superpixels, edges) and their\nfeatures, the learning of its structure and parameters is challenging. On the\nother hand, SRF may fail to accurately detect very irregular object boundaries.\nThe proposed DMT robustly overcomes these limitations for both classifiers\nthrough the ascending and descending flow of contextual information between\neach parent node and its children nodes. Third, we train DMT using different\nscales, where we progressively decrease the patch and superpixel sizes as we go\ndeeper along the tree edges nearing its leaf nodes. Last, DMT demonstrates its\noutperformance in comparison to several state-of-the-art segmentation methods\nfor multi-labeling of brain images with gliomas."}, "authors": ["Samya Amiri", "Mohamed Ali Mahjoub", "Islem Rekik"], "author_detail": {"name": "Islem Rekik"}, "author": "Islem Rekik", "links": [{"href": "http://arxiv.org/abs/1709.01602v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1709.01602v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1709.01602v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1709.01602v1", "arxiv_comment": null, "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1810.10962v2", "guidislink": true, "updated": "2018-11-02T01:54:47Z", "updated_parsed": [2018, 11, 2, 1, 54, 47, 4, 306, 0], "published": "2018-10-25T16:31:49Z", "published_parsed": [2018, 10, 25, 16, 31, 49, 3, 298, 0], "title": "Batch Normalization Sampling", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=270&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Batch Normalization Sampling"}, "summary": "Deep Neural Networks (DNNs) thrive in recent years in which Batch\nNormalization (BN) plays an indispensable role. However, it has been observed\nthat BN is costly due to the reduction operations. In this paper, we propose\nalleviating this problem through sampling only a small fraction of data for\nnormalization at each iteration. Specifically, we model it as a statistical\nsampling problem and identify that by sampling less correlated data, we can\nlargely reduce the requirement of the number of data for statistics estimation\nin BN, which directly simplifies the reduction operations. Based on this\nconclusion, we propose two sampling strategies, \"Batch Sampling\" (randomly\nselect several samples from each batch) and \"Feature Sampling\" (randomly select\na small patch from each feature map of all samples), that take both\ncomputational efficiency and sample correlation into consideration.\nFurthermore, we introduce an extremely simple variant of BN, termed as Virtual\nDataset Normalization (VDN), that can normalize the activations well with few\nsynthetical random samples. All the proposed methods are evaluated on various\ndatasets and networks, where an overall training speedup by up to 20% on GPU is\npractically achieved without the support of any specialized libraries, and the\nloss on accuracy and convergence rate are negligible. Finally, we extend our\nwork to the \"micro-batch normalization\" problem and yield comparable\nperformance with existing approaches at the case of tiny batch size.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=270&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Deep Neural Networks (DNNs) thrive in recent years in which Batch\nNormalization (BN) plays an indispensable role. However, it has been observed\nthat BN is costly due to the reduction operations. In this paper, we propose\nalleviating this problem through sampling only a small fraction of data for\nnormalization at each iteration. Specifically, we model it as a statistical\nsampling problem and identify that by sampling less correlated data, we can\nlargely reduce the requirement of the number of data for statistics estimation\nin BN, which directly simplifies the reduction operations. Based on this\nconclusion, we propose two sampling strategies, \"Batch Sampling\" (randomly\nselect several samples from each batch) and \"Feature Sampling\" (randomly select\na small patch from each feature map of all samples), that take both\ncomputational efficiency and sample correlation into consideration.\nFurthermore, we introduce an extremely simple variant of BN, termed as Virtual\nDataset Normalization (VDN), that can normalize the activations well with few\nsynthetical random samples. All the proposed methods are evaluated on various\ndatasets and networks, where an overall training speedup by up to 20% on GPU is\npractically achieved without the support of any specialized libraries, and the\nloss on accuracy and convergence rate are negligible. Finally, we extend our\nwork to the \"micro-batch normalization\" problem and yield comparable\nperformance with existing approaches at the case of tiny batch size."}, "authors": ["Zhaodong Chen", "Lei Deng", "Guoqi Li", "Jiawei Sun", "Xing Hu", "Xin Ma", "Yuan Xie"], "author_detail": {"name": "Yuan Xie"}, "author": "Yuan Xie", "links": [{"href": "http://arxiv.org/abs/1810.10962v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1810.10962v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.ML", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1810.10962v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1810.10962v2", "arxiv_comment": null, "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1911.09737v2", "guidislink": true, "updated": "2020-04-01T04:19:08Z", "updated_parsed": [2020, 4, 1, 4, 19, 8, 2, 92, 0], "published": "2019-11-21T20:32:04Z", "published_parsed": [2019, 11, 21, 20, 32, 4, 3, 325, 0], "title": "Filter Response Normalization Layer: Eliminating Batch Dependence in the\n  Training of Deep Neural Networks", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=270&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Filter Response Normalization Layer: Eliminating Batch Dependence in the\n  Training of Deep Neural Networks"}, "summary": "Batch Normalization (BN) uses mini-batch statistics to normalize the\nactivations during training, introducing dependence between mini-batch\nelements. This dependency can hurt the performance if the mini-batch size is\ntoo small, or if the elements are correlated. Several alternatives, such as\nBatch Renormalization and Group Normalization (GN), have been proposed to\naddress this issue. However, they either do not match the performance of BN for\nlarge batches, or still exhibit degradation in performance for smaller batches,\nor introduce artificial constraints on the model architecture. In this paper we\npropose the Filter Response Normalization (FRN) layer, a novel combination of a\nnormalization and an activation function, that can be used as a replacement for\nother normalizations and activations. Our method operates on each activation\nchannel of each batch element independently, eliminating the dependency on\nother batch elements. Our method outperforms BN and other alternatives in a\nvariety of settings for all batch sizes. FRN layer performs $\\approx 0.7-1.0\\%$\nbetter than BN on top-1 validation accuracy with large mini-batch sizes for\nImagenet classification using InceptionV3 and ResnetV2-50 architectures.\nFurther, it performs $>1\\%$ better than GN on the same problem in the small\nmini-batch size regime. For object detection problem on COCO dataset, FRN layer\noutperforms all other methods by at least $0.3-0.5\\%$ in all batch size\nregimes.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=270&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Batch Normalization (BN) uses mini-batch statistics to normalize the\nactivations during training, introducing dependence between mini-batch\nelements. This dependency can hurt the performance if the mini-batch size is\ntoo small, or if the elements are correlated. Several alternatives, such as\nBatch Renormalization and Group Normalization (GN), have been proposed to\naddress this issue. However, they either do not match the performance of BN for\nlarge batches, or still exhibit degradation in performance for smaller batches,\nor introduce artificial constraints on the model architecture. In this paper we\npropose the Filter Response Normalization (FRN) layer, a novel combination of a\nnormalization and an activation function, that can be used as a replacement for\nother normalizations and activations. Our method operates on each activation\nchannel of each batch element independently, eliminating the dependency on\nother batch elements. Our method outperforms BN and other alternatives in a\nvariety of settings for all batch sizes. FRN layer performs $\\approx 0.7-1.0\\%$\nbetter than BN on top-1 validation accuracy with large mini-batch sizes for\nImagenet classification using InceptionV3 and ResnetV2-50 architectures.\nFurther, it performs $>1\\%$ better than GN on the same problem in the small\nmini-batch size regime. For object detection problem on COCO dataset, FRN layer\noutperforms all other methods by at least $0.3-0.5\\%$ in all batch size\nregimes."}, "authors": ["Saurabh Singh", "Shankar Krishnan"], "author_detail": {"name": "Shankar Krishnan"}, "author": "Shankar Krishnan", "links": [{"href": "http://arxiv.org/abs/1911.09737v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1911.09737v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.ML", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1911.09737v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1911.09737v2", "arxiv_comment": null, "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/2010.03316v1", "guidislink": true, "updated": "2020-10-07T10:24:33Z", "updated_parsed": [2020, 10, 7, 10, 24, 33, 2, 281, 0], "published": "2020-10-07T10:24:33Z", "published_parsed": [2020, 10, 7, 10, 24, 33, 2, 281, 0], "title": "Batch Normalization Increases Adversarial Vulnerability: Disentangling\n  Usefulness and Robustness of Model Features", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=280&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Batch Normalization Increases Adversarial Vulnerability: Disentangling\n  Usefulness and Robustness of Model Features"}, "summary": "Batch normalization (BN) has been widely used in modern deep neural networks\n(DNNs) due to fast convergence. BN is observed to increase the model accuracy\nwhile at the cost of adversarial robustness. We conjecture that the increased\nadversarial vulnerability is caused by BN shifting the model to rely more on\nnon-robust features (NRFs). Our exploration finds that other normalization\ntechniques also increase adversarial vulnerability and our conjecture is also\nsupported by analyzing the model corruption robustness and feature\ntransferability. With a classifier DNN defined as a feature set $F$ we propose\na framework for disentangling $F$ robust usefulness into $F$ usefulness and $F$\nrobustness. We adopt a local linearity based metric, termed LIGS, to define and\nquantify $F$ robustness. Measuring the $F$ robustness with the LIGS provides\ndirect insight on the feature robustness shift independent of usefulness.\nMoreover, the LIGS trend during the whole training stage sheds light on the\norder of learned features, i.e. from RFs (robust features) to NRFs, or vice\nversa. Our work analyzes how BN and other factors influence the DNN from the\nfeature perspective. Prior works mainly adopt accuracy to evaluate their\ninfluence regarding $F$ usefulness, while we believe evaluating $F$ robustness\nis equally important, for which our work fills the gap.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=280&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Batch normalization (BN) has been widely used in modern deep neural networks\n(DNNs) due to fast convergence. BN is observed to increase the model accuracy\nwhile at the cost of adversarial robustness. We conjecture that the increased\nadversarial vulnerability is caused by BN shifting the model to rely more on\nnon-robust features (NRFs). Our exploration finds that other normalization\ntechniques also increase adversarial vulnerability and our conjecture is also\nsupported by analyzing the model corruption robustness and feature\ntransferability. With a classifier DNN defined as a feature set $F$ we propose\na framework for disentangling $F$ robust usefulness into $F$ usefulness and $F$\nrobustness. We adopt a local linearity based metric, termed LIGS, to define and\nquantify $F$ robustness. Measuring the $F$ robustness with the LIGS provides\ndirect insight on the feature robustness shift independent of usefulness.\nMoreover, the LIGS trend during the whole training stage sheds light on the\norder of learned features, i.e. from RFs (robust features) to NRFs, or vice\nversa. Our work analyzes how BN and other factors influence the DNN from the\nfeature perspective. Prior works mainly adopt accuracy to evaluate their\ninfluence regarding $F$ usefulness, while we believe evaluating $F$ robustness\nis equally important, for which our work fills the gap."}, "authors": ["Philipp Benz", "Chaoning Zhang", "In So Kweon"], "author_detail": {"name": "In So Kweon"}, "author": "In So Kweon", "links": [{"href": "http://arxiv.org/abs/2010.03316v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2010.03316v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.ML", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2010.03316v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2010.03316v1", "arxiv_comment": null, "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/2012.14743v1", "guidislink": true, "updated": "2020-12-29T13:21:18Z", "updated_parsed": [2020, 12, 29, 13, 21, 18, 1, 364, 0], "published": "2020-12-29T13:21:18Z", "published_parsed": [2020, 12, 29, 13, 21, 18, 1, 364, 0], "title": "BayesCard: A Unified Bayesian Framework for Cardinality Estimation", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=280&max_results=10&sortBy=relevance&sortOrder=descending", "value": "BayesCard: A Unified Bayesian Framework for Cardinality Estimation"}, "summary": "Cardinality estimation is one of the fundamental problems in database\nmanagement systems and it is an essential component in query optimizers.\nTraditional machine-learning-based approaches use probabilistic models such as\nBayesian Networks (BNs) to learn joint distributions on data. Recent research\nadvocates for using deep unsupervised learning and achieves state-of-the-art\nperformance in estimating the cardinality of selection and join queries. Yet\nthe lack of scalability, stability and interpretability of such deep learning\nmodels, makes them unsuitable for real-world databases.\n  Recent advances in probabilistic programming languages (PPLs) allow for a\ndeclarative and efficient specification of probabilistic models such as BNs,\nand achieve state-of-the-art accuracy in various machine learning tasks. In\nthis paper, we present BayesCard, the first framework incorporating the\ntechniques behind PPLs for building BNs along with relational extensions that\ncan accurately estimate the cardinality of selection and join queries in\ndatabase systems with model sizes that are up to three orders of magnitude\nsmaller than deep models'. Furthermore, the more stable performance and better\ninterpretation of BNs make them viable options for practical query optimizers.\nOur experimental results on several single-relation and multi-relation\ndatabases indicate that BayesCard with a reasonable estimation time has a\nbetter estimation accuracy than deep learning models, and has from one to two\norders of magnitude less training cost nevertheless.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=280&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Cardinality estimation is one of the fundamental problems in database\nmanagement systems and it is an essential component in query optimizers.\nTraditional machine-learning-based approaches use probabilistic models such as\nBayesian Networks (BNs) to learn joint distributions on data. Recent research\nadvocates for using deep unsupervised learning and achieves state-of-the-art\nperformance in estimating the cardinality of selection and join queries. Yet\nthe lack of scalability, stability and interpretability of such deep learning\nmodels, makes them unsuitable for real-world databases.\n  Recent advances in probabilistic programming languages (PPLs) allow for a\ndeclarative and efficient specification of probabilistic models such as BNs,\nand achieve state-of-the-art accuracy in various machine learning tasks. In\nthis paper, we present BayesCard, the first framework incorporating the\ntechniques behind PPLs for building BNs along with relational extensions that\ncan accurately estimate the cardinality of selection and join queries in\ndatabase systems with model sizes that are up to three orders of magnitude\nsmaller than deep models'. Furthermore, the more stable performance and better\ninterpretation of BNs make them viable options for practical query optimizers.\nOur experimental results on several single-relation and multi-relation\ndatabases indicate that BayesCard with a reasonable estimation time has a\nbetter estimation accuracy than deep learning models, and has from one to two\norders of magnitude less training cost nevertheless."}, "authors": ["Ziniu Wu", "Amir Shaikhha"], "author_detail": {"name": "Amir Shaikhha"}, "author": "Amir Shaikhha", "links": [{"href": "http://arxiv.org/abs/2012.14743v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2012.14743v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.DB", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.DB", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2012.14743v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2012.14743v1", "arxiv_comment": null, "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/0911.1544v2", "guidislink": true, "updated": "2010-02-02T08:44:06Z", "updated_parsed": [2010, 2, 2, 8, 44, 6, 1, 33, 0], "published": "2009-11-08T16:40:24Z", "published_parsed": [2009, 11, 8, 16, 40, 24, 6, 312, 0], "title": "Towards Power Efficient MAC Protocol for In-Body and On-Body Sensor\n  Networks", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=290&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Towards Power Efficient MAC Protocol for In-Body and On-Body Sensor\n  Networks"}, "summary": "This paper presents an empirical discussion on the design and implementation\nof a power-efficient Medium Access Control (MAC) protocol for in-body and\non-body sensor networks. We analyze the performance of a beacon-enabled IEEE\n802.15.4, PB-TDMA, and S-MAC protocols for on-body sensor networks. We further\npresent a Traffic Based Wakeup Mechanism that utilizes the traffic patterns of\nthe BAN Nodes (BNs) to accommodate the entire BSN traffic. To enable a logical\nconnection between different BNs working on different frequency bands, a method\ncalled Bridging function is proposed. The Bridging function integrates all BNs\nworking on different bands into a complete BSN.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=290&max_results=10&sortBy=relevance&sortOrder=descending", "value": "This paper presents an empirical discussion on the design and implementation\nof a power-efficient Medium Access Control (MAC) protocol for in-body and\non-body sensor networks. We analyze the performance of a beacon-enabled IEEE\n802.15.4, PB-TDMA, and S-MAC protocols for on-body sensor networks. We further\npresent a Traffic Based Wakeup Mechanism that utilizes the traffic patterns of\nthe BAN Nodes (BNs) to accommodate the entire BSN traffic. To enable a logical\nconnection between different BNs working on different frequency bands, a method\ncalled Bridging function is proposed. The Bridging function integrates all BNs\nworking on different bands into a complete BSN."}, "authors": ["Sana Ullah", "Xizhi An", "Kyung Sup Kwak"], "author_detail": {"name": "Kyung Sup Kwak"}, "author": "Kyung Sup Kwak", "links": [{"title": "doi", "href": "http://dx.doi.org/10.1007/978-3-642-01665-3_34", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/0911.1544v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/0911.1544v2", "rel": "related", "type": "application/pdf"}], "arxiv_comment": "11 pages, 5 figures, 3 tables,KES AMSTA 09, LNAI 5559, pp.335-345,\n  Uppsala, June 2009. arXiv admin note: text overlap with arXiv:0911.1507", "arxiv_primary_category": {"term": "cs.NI", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.NI", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/0911.1544v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/0911.1544v2", "journal_reference": null, "doi": "10.1007/978-3-642-01665-3_34"}
{"id": "http://arxiv.org/abs/1206.3283v1", "guidislink": true, "updated": "2012-06-13T15:44:14Z", "updated_parsed": [2012, 6, 13, 15, 44, 14, 2, 165, 0], "published": "2012-06-13T15:44:14Z", "published_parsed": [2012, 6, 13, 15, 44, 14, 2, 165, 0], "title": "Observation Subset Selection as Local Compilation of Performance\n  Profiles", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=300&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Observation Subset Selection as Local Compilation of Performance\n  Profiles"}, "summary": "Deciding what to sense is a crucial task, made harder by dependencies and by\na nonadditive utility function. We develop approximation algorithms for\nselecting an optimal set of measurements, under a dependency structure modeled\nby a tree-shaped Bayesian network (BN). Our approach is a generalization of\ncomposing anytime algorithm represented by conditional performance profiles.\nThis is done by relaxing the input monotonicity assumption, and extending the\nlocal compilation technique to more general classes of performance profiles\n(PPs). We apply the extended scheme to selecting a subset of measurements for\nchoosing a maximum expectation variable in a binary valued BN, and for\nminimizing the worst variance in a Gaussian BN.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=300&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Deciding what to sense is a crucial task, made harder by dependencies and by\na nonadditive utility function. We develop approximation algorithms for\nselecting an optimal set of measurements, under a dependency structure modeled\nby a tree-shaped Bayesian network (BN). Our approach is a generalization of\ncomposing anytime algorithm represented by conditional performance profiles.\nThis is done by relaxing the input monotonicity assumption, and extending the\nlocal compilation technique to more general classes of performance profiles\n(PPs). We apply the extended scheme to selecting a subset of measurements for\nchoosing a maximum expectation variable in a binary valued BN, and for\nminimizing the worst variance in a Gaussian BN."}, "authors": ["Yan Radovilsky", "Solomon Eyal Shimony"], "author_detail": {"name": "Solomon Eyal Shimony"}, "author": "Solomon Eyal Shimony", "arxiv_comment": "Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty\n  in Artificial Intelligence (UAI2008)", "links": [{"href": "http://arxiv.org/abs/1206.3283v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1206.3283v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1206.3283v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1206.3283v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1206.6862v1", "guidislink": true, "updated": "2012-06-27T16:28:06Z", "updated_parsed": [2012, 6, 27, 16, 28, 6, 2, 179, 0], "published": "2012-06-27T16:28:06Z", "published_parsed": [2012, 6, 27, 16, 28, 6, 2, 179, 0], "title": "On the Number of Samples Needed to Learn the Correct Structure of a\n  Bayesian Network", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=300&max_results=10&sortBy=relevance&sortOrder=descending", "value": "On the Number of Samples Needed to Learn the Correct Structure of a\n  Bayesian Network"}, "summary": "Bayesian Networks (BNs) are useful tools giving a natural and compact\nrepresentation of joint probability distributions. In many applications one\nneeds to learn a Bayesian Network (BN) from data. In this context, it is\nimportant to understand the number of samples needed in order to guarantee a\nsuccessful learning. Previous work have studied BNs sample complexity, yet it\nmainly focused on the requirement that the learned distribution will be close\nto the original distribution which generated the data. In this work, we study a\ndifferent aspect of the learning, namely the number of samples needed in order\nto learn the correct structure of the network. We give both asymptotic results,\nvalid in the large sample limit, and experimental results, demonstrating the\nlearning behavior for feasible sample sizes. We show that structure learning is\na more difficult task, compared to approximating the correct distribution, in\nthe sense that it requires a much larger number of samples, regardless of the\ncomputational power available for the learner.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=300&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Bayesian Networks (BNs) are useful tools giving a natural and compact\nrepresentation of joint probability distributions. In many applications one\nneeds to learn a Bayesian Network (BN) from data. In this context, it is\nimportant to understand the number of samples needed in order to guarantee a\nsuccessful learning. Previous work have studied BNs sample complexity, yet it\nmainly focused on the requirement that the learned distribution will be close\nto the original distribution which generated the data. In this work, we study a\ndifferent aspect of the learning, namely the number of samples needed in order\nto learn the correct structure of the network. We give both asymptotic results,\nvalid in the large sample limit, and experimental results, demonstrating the\nlearning behavior for feasible sample sizes. We show that structure learning is\na more difficult task, compared to approximating the correct distribution, in\nthe sense that it requires a much larger number of samples, regardless of the\ncomputational power available for the learner."}, "authors": ["Or Zuk", "Shiri Margel", "Eytan Domany"], "author_detail": {"name": "Eytan Domany"}, "author": "Eytan Domany", "arxiv_comment": "Appears in Proceedings of the Twenty-Second Conference on Uncertainty\n  in Artificial Intelligence (UAI2006)", "links": [{"href": "http://arxiv.org/abs/1206.6862v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1206.6862v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.ML", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1206.6862v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1206.6862v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1206.6877v1", "guidislink": true, "updated": "2012-06-27T16:31:08Z", "updated_parsed": [2012, 6, 27, 16, 31, 8, 2, 179, 0], "published": "2012-06-27T16:31:08Z", "published_parsed": [2012, 6, 27, 16, 31, 8, 2, 179, 0], "title": "Inference in Hybrid Bayesian Networks Using Mixtures of Gaussians", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=300&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Inference in Hybrid Bayesian Networks Using Mixtures of Gaussians"}, "summary": "The main goal of this paper is to describe a method for exact inference in\ngeneral hybrid Bayesian networks (BNs) (with a mixture of discrete and\ncontinuous chance variables). Our method consists of approximating general\nhybrid Bayesian networks by a mixture of Gaussians (MoG) BNs. There exists a\nfast algorithm by Lauritzen-Jensen (LJ) for making exact inferences in MoG\nBayesian networks, and there exists a commercial implementation of this\nalgorithm. However, this algorithm can only be used for MoG BNs. Some\nlimitations of such networks are as follows. All continuous chance variables\nmust have conditional linear Gaussian distributions, and discrete chance nodes\ncannot have continuous parents. The methods described in this paper will enable\nus to use the LJ algorithm for a bigger class of hybrid Bayesian networks. This\nincludes networks with continuous chance nodes with non-Gaussian distributions,\nnetworks with no restrictions on the topology of discrete and continuous\nvariables, networks with conditionally deterministic variables that are a\nnonlinear function of their continuous parents, and networks with continuous\nchance variables whose variances are functions of their parents.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=300&max_results=10&sortBy=relevance&sortOrder=descending", "value": "The main goal of this paper is to describe a method for exact inference in\ngeneral hybrid Bayesian networks (BNs) (with a mixture of discrete and\ncontinuous chance variables). Our method consists of approximating general\nhybrid Bayesian networks by a mixture of Gaussians (MoG) BNs. There exists a\nfast algorithm by Lauritzen-Jensen (LJ) for making exact inferences in MoG\nBayesian networks, and there exists a commercial implementation of this\nalgorithm. However, this algorithm can only be used for MoG BNs. Some\nlimitations of such networks are as follows. All continuous chance variables\nmust have conditional linear Gaussian distributions, and discrete chance nodes\ncannot have continuous parents. The methods described in this paper will enable\nus to use the LJ algorithm for a bigger class of hybrid Bayesian networks. This\nincludes networks with continuous chance nodes with non-Gaussian distributions,\nnetworks with no restrictions on the topology of discrete and continuous\nvariables, networks with conditionally deterministic variables that are a\nnonlinear function of their continuous parents, and networks with continuous\nchance variables whose variances are functions of their parents."}, "authors": ["Prakash P. Shenoy"], "author_detail": {"name": "Prakash P. Shenoy"}, "author": "Prakash P. Shenoy", "arxiv_comment": "Appears in Proceedings of the Twenty-Second Conference on Uncertainty\n  in Artificial Intelligence (UAI2006)", "links": [{"href": "http://arxiv.org/abs/1206.6877v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1206.6877v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.ME", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1206.6877v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1206.6877v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1301.2280v1", "guidislink": true, "updated": "2013-01-10T16:24:19Z", "updated_parsed": [2013, 1, 10, 16, 24, 19, 3, 10, 0], "published": "2013-01-10T16:24:19Z", "published_parsed": [2013, 1, 10, 16, 24, 19, 3, 10, 0], "title": "Estimating Well-Performing Bayesian Networks using Bernoulli Mixtures", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=300&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Estimating Well-Performing Bayesian Networks using Bernoulli Mixtures"}, "summary": "A novel method for estimating Bayesian network (BN) parameters from data is\npresented which provides improved performance on test data. Previous research\nhas shown the value of representing conditional probability distributions\n(CPDs) via neural networks(Neal 1992), noisy-OR gates (Neal 1992, Diez 1993)and\ndecision trees (Friedman and Goldszmidt 1996).The Bernoulli mixture network\n(BMN) explicitly represents the CPDs of discrete BN nodes as mixtures of local\ndistributions,each having a different set of parents.This increases the space\nof possible structures which can be considered,enabling the CPDs to have\nfiner-grained dependencies.The resulting estimation procedure induces a\nmodelthat is better able to emulate the underlying interactions occurring in\nthe data than conventional conditional Bernoulli network models.The results for\nartificially generated data indicate that overfitting is best reduced by\nrestricting the complexity of candidate mixture substructures local to each\nnode. Furthermore, mixtures of very simple substructures can perform almost as\nwell as more complex ones.The BMN is also applied to data collected from an\nonline adventure game with an application to keyhole plan recognition. The\nresults show that the BMN-based model brings a dramatic improvement in\nperformance over a conventional BN model.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=300&max_results=10&sortBy=relevance&sortOrder=descending", "value": "A novel method for estimating Bayesian network (BN) parameters from data is\npresented which provides improved performance on test data. Previous research\nhas shown the value of representing conditional probability distributions\n(CPDs) via neural networks(Neal 1992), noisy-OR gates (Neal 1992, Diez 1993)and\ndecision trees (Friedman and Goldszmidt 1996).The Bernoulli mixture network\n(BMN) explicitly represents the CPDs of discrete BN nodes as mixtures of local\ndistributions,each having a different set of parents.This increases the space\nof possible structures which can be considered,enabling the CPDs to have\nfiner-grained dependencies.The resulting estimation procedure induces a\nmodelthat is better able to emulate the underlying interactions occurring in\nthe data than conventional conditional Bernoulli network models.The results for\nartificially generated data indicate that overfitting is best reduced by\nrestricting the complexity of candidate mixture substructures local to each\nnode. Furthermore, mixtures of very simple substructures can perform almost as\nwell as more complex ones.The BMN is also applied to data collected from an\nonline adventure game with an application to keyhole plan recognition. The\nresults show that the BMN-based model brings a dramatic improvement in\nperformance over a conventional BN model."}, "authors": ["Geoff A. Jarrad"], "author_detail": {"name": "Geoff A. Jarrad"}, "author": "Geoff A. Jarrad", "arxiv_comment": "Appears in Proceedings of the Seventeenth Conference on Uncertainty\n  in Artificial Intelligence (UAI2001)", "links": [{"href": "http://arxiv.org/abs/1301.2280v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1301.2280v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.ML", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1301.2280v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1301.2280v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1301.3902v1", "guidislink": true, "updated": "2013-01-16T15:53:20Z", "updated_parsed": [2013, 1, 16, 15, 53, 20, 2, 16, 0], "published": "2013-01-16T15:53:20Z", "published_parsed": [2013, 1, 16, 15, 53, 20, 2, 16, 0], "title": "Model Criticism of Bayesian Networks with Latent Variables", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=300&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Model Criticism of Bayesian Networks with Latent Variables"}, "summary": "The application of Bayesian networks (BNs) to cognitive assessment and\nintelligent tutoring systems poses new challenges for model construction. When\ncognitive task analyses suggest constructing a BN with several latent\nvariables, empirical model criticism of the latent structure becomes both\ncritical and complex. This paper introduces a methodology for criticizing\nmodels both globally (a BN in its entirety) and locally (observable nodes), and\nexplores its value in identifying several kinds of misfit: node errors, edge\nerrors, state errors, and prior probability errors in the latent structure. The\nresults suggest the indices have potential for detecting model misfit and\nassisting in locating problematic components of the model.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=300&max_results=10&sortBy=relevance&sortOrder=descending", "value": "The application of Bayesian networks (BNs) to cognitive assessment and\nintelligent tutoring systems poses new challenges for model construction. When\ncognitive task analyses suggest constructing a BN with several latent\nvariables, empirical model criticism of the latent structure becomes both\ncritical and complex. This paper introduces a methodology for criticizing\nmodels both globally (a BN in its entirety) and locally (observable nodes), and\nexplores its value in identifying several kinds of misfit: node errors, edge\nerrors, state errors, and prior probability errors in the latent structure. The\nresults suggest the indices have potential for detecting model misfit and\nassisting in locating problematic components of the model."}, "authors": ["David M. Williamson", "Russell Almond", "Robert Mislevy"], "author_detail": {"name": "Robert Mislevy"}, "author": "Robert Mislevy", "arxiv_comment": "Appears in Proceedings of the Sixteenth Conference on Uncertainty in\n  Artificial Intelligence (UAI2000)", "links": [{"href": "http://arxiv.org/abs/1301.3902v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1301.3902v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.AP", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.ME", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1301.3902v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1301.3902v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1302.6839v1", "guidislink": true, "updated": "2013-02-27T14:19:10Z", "updated_parsed": [2013, 2, 27, 14, 19, 10, 2, 58, 0], "published": "2013-02-27T14:19:10Z", "published_parsed": [2013, 2, 27, 14, 19, 10, 2, 58, 0], "title": "Knowledge Engineering for Large Belief Networks", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=300&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Knowledge Engineering for Large Belief Networks"}, "summary": "We present several techniques for knowledge engineering of large belief\nnetworks (BNs) based on the our experiences with a network derived from a large\nmedical knowledge base. The noisyMAX, a generalization of the noisy-OR gate, is\nused to model causal in dependence in a BN with multi-valued variables. We\ndescribe the use of leak probabilities to enforce the closed-world assumption\nin our model. We present Netview, a visualization tool based on causal\nindependence and the use of leak probabilities. The Netview software allows\nknowledge engineers to dynamically view sub-networks for knowledge engineering,\nand it provides version control for editing a BN. Netview generates\nsub-networks in which leak probabilities are dynamically updated to reflect the\nmissing portions of the network.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=300&max_results=10&sortBy=relevance&sortOrder=descending", "value": "We present several techniques for knowledge engineering of large belief\nnetworks (BNs) based on the our experiences with a network derived from a large\nmedical knowledge base. The noisyMAX, a generalization of the noisy-OR gate, is\nused to model causal in dependence in a BN with multi-valued variables. We\ndescribe the use of leak probabilities to enforce the closed-world assumption\nin our model. We present Netview, a visualization tool based on causal\nindependence and the use of leak probabilities. The Netview software allows\nknowledge engineers to dynamically view sub-networks for knowledge engineering,\nand it provides version control for editing a BN. Netview generates\nsub-networks in which leak probabilities are dynamically updated to reflect the\nmissing portions of the network."}, "authors": ["Malcolm Pradhan", "Gregory M. Provan", "Blackford Middleton", "Max Henrion"], "author_detail": {"name": "Max Henrion"}, "author": "Max Henrion", "arxiv_comment": "Appears in Proceedings of the Tenth Conference on Uncertainty in\n  Artificial Intelligence (UAI1994)", "links": [{"href": "http://arxiv.org/abs/1302.6839v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1302.6839v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1302.6839v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1302.6839v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1410.7835v2", "guidislink": true, "updated": "2014-12-09T01:07:36Z", "updated_parsed": [2014, 12, 9, 1, 7, 36, 1, 343, 0], "published": "2014-10-28T23:14:56Z", "published_parsed": [2014, 10, 28, 23, 14, 56, 1, 301, 0], "title": "Fast Learning of Relational Dependency Networks", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=310&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Fast Learning of Relational Dependency Networks"}, "summary": "A Relational Dependency Network (RDN) is a directed graphical model widely\nused for multi-relational data. These networks allow cyclic dependencies,\nnecessary to represent relational autocorrelations. We describe an approach for\nlearning both the RDN's structure and its parameters, given an input relational\ndatabase: First learn a Bayesian network (BN), then transform the Bayesian\nnetwork to an RDN. Thus fast Bayes net learning can provide fast RDN learning.\nThe BN-to-RDN transform comprises a simple, local adjustment of the Bayes net\nstructure and a closed-form transform of the Bayes net parameters. This method\ncan learn an RDN for a dataset with a million tuples in minutes. We empirically\ncompare our approach to state-of-the art RDN learning methods that use\nfunctional gradient boosting, on five benchmark datasets. Learning RDNs via BNs\nscales much better to large datasets than learning RDNs with boosting, and\nprovides competitive accuracy in predictions.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=310&max_results=10&sortBy=relevance&sortOrder=descending", "value": "A Relational Dependency Network (RDN) is a directed graphical model widely\nused for multi-relational data. These networks allow cyclic dependencies,\nnecessary to represent relational autocorrelations. We describe an approach for\nlearning both the RDN's structure and its parameters, given an input relational\ndatabase: First learn a Bayesian network (BN), then transform the Bayesian\nnetwork to an RDN. Thus fast Bayes net learning can provide fast RDN learning.\nThe BN-to-RDN transform comprises a simple, local adjustment of the Bayes net\nstructure and a closed-form transform of the Bayes net parameters. This method\ncan learn an RDN for a dataset with a million tuples in minutes. We empirically\ncompare our approach to state-of-the art RDN learning methods that use\nfunctional gradient boosting, on five benchmark datasets. Learning RDNs via BNs\nscales much better to large datasets than learning RDNs with boosting, and\nprovides competitive accuracy in predictions."}, "authors": ["Oliver Schulte", "Zhensong Qian", "Arthur E. Kirkpatrick", "Xiaoqian Yin", "Yan Sun"], "author_detail": {"name": "Yan Sun"}, "author": "Yan Sun", "arxiv_comment": "17 pages, 2 figures, 3 tables, Accepted as long paper by ILP 2014,\n  September 14- 16th, Nancy, France. Added the Appendix: Proof of Consistency\n  Characterization", "links": [{"href": "http://arxiv.org/abs/1410.7835v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1410.7835v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1410.7835v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1410.7835v2", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1709.09603v3", "guidislink": true, "updated": "2017-10-31T06:42:07Z", "updated_parsed": [2017, 10, 31, 6, 42, 7, 1, 304, 0], "published": "2017-09-27T16:18:00Z", "published_parsed": [2017, 9, 27, 16, 18, 0, 2, 270, 0], "title": "Riemannian approach to batch normalization", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=320&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Riemannian approach to batch normalization"}, "summary": "Batch Normalization (BN) has proven to be an effective algorithm for deep\nneural network training by normalizing the input to each neuron and reducing\nthe internal covariate shift. The space of weight vectors in the BN layer can\nbe naturally interpreted as a Riemannian manifold, which is invariant to linear\nscaling of weights. Following the intrinsic geometry of this manifold provides\na new learning rule that is more efficient and easier to analyze. We also\npropose intuitive and effective gradient clipping and regularization methods\nfor the proposed algorithm by utilizing the geometry of the manifold. The\nresulting algorithm consistently outperforms the original BN on various types\nof network architectures and datasets.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=320&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Batch Normalization (BN) has proven to be an effective algorithm for deep\nneural network training by normalizing the input to each neuron and reducing\nthe internal covariate shift. The space of weight vectors in the BN layer can\nbe naturally interpreted as a Riemannian manifold, which is invariant to linear\nscaling of weights. Following the intrinsic geometry of this manifold provides\na new learning rule that is more efficient and easier to analyze. We also\npropose intuitive and effective gradient clipping and regularization methods\nfor the proposed algorithm by utilizing the geometry of the manifold. The\nresulting algorithm consistently outperforms the original BN on various types\nof network architectures and datasets."}, "authors": ["Minhyung Cho", "Jaehyung Lee"], "author_detail": {"name": "Jaehyung Lee"}, "author": "Jaehyung Lee", "arxiv_comment": "to appear at NIPS 2017", "links": [{"href": "http://arxiv.org/abs/1709.09603v3", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1709.09603v3", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1709.09603v3", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1709.09603v3", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1801.05134v1", "guidislink": true, "updated": "2018-01-16T06:47:59Z", "updated_parsed": [2018, 1, 16, 6, 47, 59, 1, 16, 0], "published": "2018-01-16T06:47:59Z", "published_parsed": [2018, 1, 16, 6, 47, 59, 1, 16, 0], "title": "Understanding the Disharmony between Dropout and Batch Normalization by\n  Variance Shift", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=330&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Understanding the Disharmony between Dropout and Batch Normalization by\n  Variance Shift"}, "summary": "This paper first answers the question \"why do the two most powerful\ntechniques Dropout and Batch Normalization (BN) often lead to a worse\nperformance when they are combined together?\" in both theoretical and\nstatistical aspects. Theoretically, we find that Dropout would shift the\nvariance of a specific neural unit when we transfer the state of that network\nfrom train to test. However, BN would maintain its statistical variance, which\nis accumulated from the entire learning procedure, in the test phase. The\ninconsistency of that variance (we name this scheme as \"variance shift\") causes\nthe unstable numerical behavior in inference that leads to more erroneous\npredictions finally, when applying Dropout before BN. Thorough experiments on\nDenseNet, ResNet, ResNeXt and Wide ResNet confirm our findings. According to\nthe uncovered mechanism, we next explore several strategies that modifies\nDropout and try to overcome the limitations of their combination by avoiding\nthe variance shift risks.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=330&max_results=10&sortBy=relevance&sortOrder=descending", "value": "This paper first answers the question \"why do the two most powerful\ntechniques Dropout and Batch Normalization (BN) often lead to a worse\nperformance when they are combined together?\" in both theoretical and\nstatistical aspects. Theoretically, we find that Dropout would shift the\nvariance of a specific neural unit when we transfer the state of that network\nfrom train to test. However, BN would maintain its statistical variance, which\nis accumulated from the entire learning procedure, in the test phase. The\ninconsistency of that variance (we name this scheme as \"variance shift\") causes\nthe unstable numerical behavior in inference that leads to more erroneous\npredictions finally, when applying Dropout before BN. Thorough experiments on\nDenseNet, ResNet, ResNeXt and Wide ResNet confirm our findings. According to\nthe uncovered mechanism, we next explore several strategies that modifies\nDropout and try to overcome the limitations of their combination by avoiding\nthe variance shift risks."}, "authors": ["Xiang Li", "Shuo Chen", "Xiaolin Hu", "Jian Yang"], "author_detail": {"name": "Jian Yang"}, "author": "Jian Yang", "arxiv_comment": "9 pages, 7 figures", "links": [{"href": "http://arxiv.org/abs/1801.05134v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1801.05134v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.ML", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1801.05134v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1801.05134v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1808.03299v1", "guidislink": true, "updated": "2018-08-09T18:38:16Z", "updated_parsed": [2018, 8, 9, 18, 38, 16, 3, 221, 0], "published": "2018-08-09T18:38:16Z", "published_parsed": [2018, 8, 9, 18, 38, 16, 3, 221, 0], "title": "Code-Mixed Sentiment Analysis Using Machine Learning and Neural Network\n  Approaches", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=330&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Code-Mixed Sentiment Analysis Using Machine Learning and Neural Network\n  Approaches"}, "summary": "Sentiment Analysis for Indian Languages (SAIL)-Code Mixed tools contest aimed\nat identifying the sentence level sentiment polarity of the code-mixed dataset\nof Indian languages pairs (Hi-En, Ben-Hi-En). Hi-En dataset is henceforth\nreferred to as HI-EN and Ben-Hi-En dataset as BN-EN respectively. For this, we\nsubmitted four models for sentiment analysis of code-mixed HI-EN and BN-EN\ndatasets. The first model was an ensemble voting classifier consisting of three\nclassifiers - linear SVM, logistic regression and random forests while the\nsecond one was a linear SVM. Both the models used TF-IDF feature vectors of\ncharacter n-grams where n ranged from 2 to 6. We used scikit-learn (sklearn)\nmachine learning library for implementing both the approaches. Run1 was\nobtained from the voting classifier and Run2 used the linear SVM model for\nproducing the results. Out of the four submitted outputs Run2 outperformed Run1\nin both the datasets. We finished first in the contest for both HI-EN with an\nF-score of 0.569 and BN-EN with an F-score of 0.526.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=330&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Sentiment Analysis for Indian Languages (SAIL)-Code Mixed tools contest aimed\nat identifying the sentence level sentiment polarity of the code-mixed dataset\nof Indian languages pairs (Hi-En, Ben-Hi-En). Hi-En dataset is henceforth\nreferred to as HI-EN and Ben-Hi-En dataset as BN-EN respectively. For this, we\nsubmitted four models for sentiment analysis of code-mixed HI-EN and BN-EN\ndatasets. The first model was an ensemble voting classifier consisting of three\nclassifiers - linear SVM, logistic regression and random forests while the\nsecond one was a linear SVM. Both the models used TF-IDF feature vectors of\ncharacter n-grams where n ranged from 2 to 6. We used scikit-learn (sklearn)\nmachine learning library for implementing both the approaches. Run1 was\nobtained from the voting classifier and Run2 used the linear SVM model for\nproducing the results. Out of the four submitted outputs Run2 outperformed Run1\nin both the datasets. We finished first in the contest for both HI-EN with an\nF-score of 0.569 and BN-EN with an F-score of 0.526."}, "authors": ["Pruthwik Mishra", "Prathyusha Danda", "Pranav Dhakras"], "author_detail": {"name": "Pranav Dhakras"}, "author": "Pranav Dhakras", "arxiv_comment": "6 pages", "links": [{"href": "http://arxiv.org/abs/1808.03299v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1808.03299v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1808.03299v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1808.03299v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1810.00122v2", "guidislink": true, "updated": "2019-05-09T03:04:58Z", "updated_parsed": [2019, 5, 9, 3, 4, 58, 3, 129, 0], "published": "2018-09-29T00:50:21Z", "published_parsed": [2018, 9, 29, 0, 50, 21, 5, 272, 0], "title": "A Quantitative Analysis of the Effect of Batch Normalization on Gradient\n  Descent", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=330&max_results=10&sortBy=relevance&sortOrder=descending", "value": "A Quantitative Analysis of the Effect of Batch Normalization on Gradient\n  Descent"}, "summary": "Despite its empirical success and recent theoretical progress, there\ngenerally lacks a quantitative analysis of the effect of batch normalization\n(BN) on the convergence and stability of gradient descent. In this paper, we\nprovide such an analysis on the simple problem of ordinary least squares (OLS).\nSince precise dynamical properties of gradient descent (GD) is completely known\nfor the OLS problem, it allows us to isolate and compare the additional effects\nof BN. More precisely, we show that unlike GD, gradient descent with BN (BNGD)\nconverges for arbitrary learning rates for the weights, and the convergence\nremains linear under mild conditions. Moreover, we quantify two different\nsources of acceleration of BNGD over GD -- one due to over-parameterization\nwhich improves the effective condition number and another due having a large\nrange of learning rates giving rise to fast descent. These phenomena set BNGD\napart from GD and could account for much of its robustness properties. These\nfindings are confirmed quantitatively by numerical experiments, which further\nshow that many of the uncovered properties of BNGD in OLS are also observed\nqualitatively in more complex supervised learning problems.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=330&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Despite its empirical success and recent theoretical progress, there\ngenerally lacks a quantitative analysis of the effect of batch normalization\n(BN) on the convergence and stability of gradient descent. In this paper, we\nprovide such an analysis on the simple problem of ordinary least squares (OLS).\nSince precise dynamical properties of gradient descent (GD) is completely known\nfor the OLS problem, it allows us to isolate and compare the additional effects\nof BN. More precisely, we show that unlike GD, gradient descent with BN (BNGD)\nconverges for arbitrary learning rates for the weights, and the convergence\nremains linear under mild conditions. Moreover, we quantify two different\nsources of acceleration of BNGD over GD -- one due to over-parameterization\nwhich improves the effective condition number and another due having a large\nrange of learning rates giving rise to fast descent. These phenomena set BNGD\napart from GD and could account for much of its robustness properties. These\nfindings are confirmed quantitatively by numerical experiments, which further\nshow that many of the uncovered properties of BNGD in OLS are also observed\nqualitatively in more complex supervised learning problems."}, "authors": ["Yongqiang Cai", "Qianxiao Li", "Zuowei Shen"], "author_detail": {"name": "Zuowei Shen"}, "author": "Zuowei Shen", "links": [{"href": "http://arxiv.org/abs/1810.00122v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1810.00122v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.ML", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1810.00122v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1810.00122v2", "arxiv_comment": null, "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1811.05039v1", "guidislink": true, "updated": "2018-11-12T23:19:51Z", "updated_parsed": [2018, 11, 12, 23, 19, 51, 0, 316, 0], "published": "2018-11-12T23:19:51Z", "published_parsed": [2018, 11, 12, 23, 19, 51, 0, 316, 0], "title": "Finding All Bayesian Network Structures within a Factor of Optimal", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=330&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Finding All Bayesian Network Structures within a Factor of Optimal"}, "summary": "A Bayesian network is a widely used probabilistic graphical model with\napplications in knowledge discovery and prediction. Learning a Bayesian network\n(BN) from data can be cast as an optimization problem using the well-known\nscore-and-search approach. However, selecting a single model (i.e., the best\nscoring BN) can be misleading or may not achieve the best possible accuracy. An\nalternative to committing to a single model is to perform some form of Bayesian\nor frequentist model averaging, where the space of possible BNs is sampled or\nenumerated in some fashion. Unfortunately, existing approaches for model\naveraging either severely restrict the structure of the Bayesian network or\nhave only been shown to scale to networks with fewer than 30 random variables.\nIn this paper, we propose a novel approach to model averaging inspired by\nperformance guarantees in approximation algorithms. Our approach has two\nprimary advantages. First, our approach only considers credible models in that\nthey are optimal or near-optimal in score. Second, our approach is more\nefficient and scales to significantly larger Bayesian networks than existing\napproaches.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=330&max_results=10&sortBy=relevance&sortOrder=descending", "value": "A Bayesian network is a widely used probabilistic graphical model with\napplications in knowledge discovery and prediction. Learning a Bayesian network\n(BN) from data can be cast as an optimization problem using the well-known\nscore-and-search approach. However, selecting a single model (i.e., the best\nscoring BN) can be misleading or may not achieve the best possible accuracy. An\nalternative to committing to a single model is to perform some form of Bayesian\nor frequentist model averaging, where the space of possible BNs is sampled or\nenumerated in some fashion. Unfortunately, existing approaches for model\naveraging either severely restrict the structure of the Bayesian network or\nhave only been shown to scale to networks with fewer than 30 random variables.\nIn this paper, we propose a novel approach to model averaging inspired by\nperformance guarantees in approximation algorithms. Our approach has two\nprimary advantages. First, our approach only considers credible models in that\nthey are optimal or near-optimal in score. Second, our approach is more\nefficient and scales to significantly larger Bayesian networks than existing\napproaches."}, "authors": ["Zhenyu A. Liao", "Charupriya Sharma", "James Cussens", "Peter van Beek"], "author_detail": {"name": "Peter van Beek"}, "author": "Peter van Beek", "arxiv_comment": "11 pages with supplemental material, to appear at AAAI 2019", "links": [{"href": "http://arxiv.org/abs/1811.05039v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1811.05039v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.ML", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1811.05039v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1811.05039v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1901.10557v1", "guidislink": true, "updated": "2019-01-23T13:21:08Z", "updated_parsed": [2019, 1, 23, 13, 21, 8, 2, 23, 0], "published": "2019-01-23T13:21:08Z", "published_parsed": [2019, 1, 23, 13, 21, 8, 2, 23, 0], "title": "Bayesian Networks based Hybrid Quantum-Classical Machine Learning\n  Approach to Elucidate Gene Regulatory Pathways", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=330&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Bayesian Networks based Hybrid Quantum-Classical Machine Learning\n  Approach to Elucidate Gene Regulatory Pathways"}, "summary": "We report a scalable hybrid quantum-classical machine learning framework to\nbuild Bayesian networks (BN) that captures the conditional dependence and\ncausal relationships of random variables. The generation of a BN consists of\nfinding a directed acyclic graph (DAG) and the associated joint probability\ndistribution of the nodes consistent with a given dataset. This is a\ncombinatorial problem of structural learning of the underlying graph, starting\nfrom a single node and building one arc at a time, that fits a given ensemble\nusing maximum likelihood estimators (MLE). It is cast as an optimization\nproblem that consists of a scoring step performed on a classical computer,\npenalties for acyclicity and number of parents allowed constraints, and a\nsearch step implemented using a quantum annealer. We have assumed uniform\npriors in deriving the Bayesian network that can be relaxed by formulating the\nproblem as an estimation Dirichlet parameters. We demonstrate the utility of\nthe framework by applying to the problem of elucidating the gene regulatory\nnetwork for the MAPK/Raf pathway in human T-cells using proteomics data where\nthe concentration of proteins, nodes of the BN, are interpreted as\nprobabilities.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=330&max_results=10&sortBy=relevance&sortOrder=descending", "value": "We report a scalable hybrid quantum-classical machine learning framework to\nbuild Bayesian networks (BN) that captures the conditional dependence and\ncausal relationships of random variables. The generation of a BN consists of\nfinding a directed acyclic graph (DAG) and the associated joint probability\ndistribution of the nodes consistent with a given dataset. This is a\ncombinatorial problem of structural learning of the underlying graph, starting\nfrom a single node and building one arc at a time, that fits a given ensemble\nusing maximum likelihood estimators (MLE). It is cast as an optimization\nproblem that consists of a scoring step performed on a classical computer,\npenalties for acyclicity and number of parents allowed constraints, and a\nsearch step implemented using a quantum annealer. We have assumed uniform\npriors in deriving the Bayesian network that can be relaxed by formulating the\nproblem as an estimation Dirichlet parameters. We demonstrate the utility of\nthe framework by applying to the problem of elucidating the gene regulatory\nnetwork for the MAPK/Raf pathway in human T-cells using proteomics data where\nthe concentration of proteins, nodes of the BN, are interpreted as\nprobabilities."}, "authors": ["Radhakrishnan Balu", "Ajinkya Borle"], "author_detail": {"name": "Ajinkya Borle"}, "author": "Ajinkya Borle", "links": [{"href": "http://arxiv.org/abs/1901.10557v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1901.10557v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "quant-ph", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.ML", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1901.10557v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1901.10557v1", "arxiv_comment": null, "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1904.03441v1", "guidislink": true, "updated": "2019-04-06T13:10:20Z", "updated_parsed": [2019, 4, 6, 13, 10, 20, 5, 96, 0], "published": "2019-04-06T13:10:20Z", "published_parsed": [2019, 4, 6, 13, 10, 20, 5, 96, 0], "title": "Iterative Normalization: Beyond Standardization towards Efficient\n  Whitening", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=330&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Iterative Normalization: Beyond Standardization towards Efficient\n  Whitening"}, "summary": "Batch Normalization (BN) is ubiquitously employed for accelerating neural\nnetwork training and improving the generalization capability by performing\nstandardization within mini-batches. Decorrelated Batch Normalization (DBN)\nfurther boosts the above effectiveness by whitening. However, DBN relies\nheavily on either a large batch size, or eigen-decomposition that suffers from\npoor efficiency on GPUs. We propose Iterative Normalization (IterNorm), which\nemploys Newton's iterations for much more efficient whitening, while\nsimultaneously avoiding the eigen-decomposition. Furthermore, we develop a\ncomprehensive study to show IterNorm has better trade-off between optimization\nand generalization, with theoretical and experimental support. To this end, we\nexclusively introduce Stochastic Normalization Disturbance (SND), which\nmeasures the inherent stochastic uncertainty of samples when applied to\nnormalization operations. With the support of SND, we provide natural\nexplanations to several phenomena from the perspective of optimization, e.g.,\nwhy group-wise whitening of DBN generally outperforms full-whitening and why\nthe accuracy of BN degenerates with reduced batch sizes. We demonstrate the\nconsistently improved performance of IterNorm with extensive experiments on\nCIFAR-10 and ImageNet over BN and DBN.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=330&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Batch Normalization (BN) is ubiquitously employed for accelerating neural\nnetwork training and improving the generalization capability by performing\nstandardization within mini-batches. Decorrelated Batch Normalization (DBN)\nfurther boosts the above effectiveness by whitening. However, DBN relies\nheavily on either a large batch size, or eigen-decomposition that suffers from\npoor efficiency on GPUs. We propose Iterative Normalization (IterNorm), which\nemploys Newton's iterations for much more efficient whitening, while\nsimultaneously avoiding the eigen-decomposition. Furthermore, we develop a\ncomprehensive study to show IterNorm has better trade-off between optimization\nand generalization, with theoretical and experimental support. To this end, we\nexclusively introduce Stochastic Normalization Disturbance (SND), which\nmeasures the inherent stochastic uncertainty of samples when applied to\nnormalization operations. With the support of SND, we provide natural\nexplanations to several phenomena from the perspective of optimization, e.g.,\nwhy group-wise whitening of DBN generally outperforms full-whitening and why\nthe accuracy of BN degenerates with reduced batch sizes. We demonstrate the\nconsistently improved performance of IterNorm with extensive experiments on\nCIFAR-10 and ImageNet over BN and DBN."}, "authors": ["Lei Huang", "Yi Zhou", "Fan Zhu", "Li Liu", "Ling Shao"], "author_detail": {"name": "Ling Shao"}, "author": "Ling Shao", "arxiv_comment": "Accepted to CVPR 2019. The Code is available at\n  https://github.com/huangleiBuaa/IterNorm", "links": [{"href": "http://arxiv.org/abs/1904.03441v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1904.03441v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1904.03441v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1904.03441v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1908.00636v3", "guidislink": true, "updated": "2020-01-09T16:57:29Z", "updated_parsed": [2020, 1, 9, 16, 57, 29, 3, 9, 0], "published": "2019-08-01T21:28:46Z", "published_parsed": [2019, 8, 1, 21, 28, 46, 3, 213, 0], "title": "Optimize TSK Fuzzy Systems for Classification Problems: Mini-Batch\n  Gradient Descent with Uniform Regularization and Batch Normalization", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=340&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Optimize TSK Fuzzy Systems for Classification Problems: Mini-Batch\n  Gradient Descent with Uniform Regularization and Batch Normalization"}, "summary": "Takagi-Sugeno-Kang (TSK) fuzzy systems are flexible and interpretable machine\nlearning models; however, they may not be easily optimized when the data size\nis large, and/or the data dimensionality is high. This paper proposes a\nmini-batch gradient descent (MBGD) based algorithm to efficiently and\neffectively train TSK fuzzy classifiers. It integrates two novel techniques: 1)\nuniform regularization (UR), which forces the rules to have similar average\ncontributions to the output, and hence to increase the generalization\nperformance of the TSK classifier; and, 2) batch normalization (BN), which\nextends BN from deep neural networks to TSK fuzzy classifiers to expedite the\nconvergence and improve the generalization performance. Experiments on 12 UCI\ndatasets from various application domains, with varying size and\ndimensionality, demonstrated that UR and BN are effective individually, and\nintegrating them can further improve the classification performance.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=340&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Takagi-Sugeno-Kang (TSK) fuzzy systems are flexible and interpretable machine\nlearning models; however, they may not be easily optimized when the data size\nis large, and/or the data dimensionality is high. This paper proposes a\nmini-batch gradient descent (MBGD) based algorithm to efficiently and\neffectively train TSK fuzzy classifiers. It integrates two novel techniques: 1)\nuniform regularization (UR), which forces the rules to have similar average\ncontributions to the output, and hence to increase the generalization\nperformance of the TSK classifier; and, 2) batch normalization (BN), which\nextends BN from deep neural networks to TSK fuzzy classifiers to expedite the\nconvergence and improve the generalization performance. Experiments on 12 UCI\ndatasets from various application domains, with varying size and\ndimensionality, demonstrated that UR and BN are effective individually, and\nintegrating them can further improve the classification performance."}, "authors": ["Yuqi Cui", "Jian Huang", "Dongrui Wu"], "author_detail": {"name": "Dongrui Wu"}, "author": "Dongrui Wu", "links": [{"title": "doi", "href": "http://dx.doi.org/10.1109/TFUZZ.2020.2967282", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/1908.00636v3", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1908.00636v3", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.ML", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1908.00636v3", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1908.00636v3", "arxiv_comment": null, "journal_reference": "IEEE Trans. on Fuzzy Systems, 28(12):3065-3075, 2020", "doi": "10.1109/TFUZZ.2020.2967282"}
{"id": "http://arxiv.org/abs/2002.10801v3", "guidislink": true, "updated": "2020-07-29T13:30:54Z", "updated_parsed": [2020, 7, 29, 13, 30, 54, 2, 211, 0], "published": "2020-02-25T11:40:27Z", "published_parsed": [2020, 2, 25, 11, 40, 27, 1, 56, 0], "title": "Layer-wise Conditioning Analysis in Exploring the Learning Dynamics of\n  DNNs", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=340&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Layer-wise Conditioning Analysis in Exploring the Learning Dynamics of\n  DNNs"}, "summary": "Conditioning analysis uncovers the landscape of an optimization objective by\nexploring the spectrum of its curvature matrix. This has been well explored\ntheoretically for linear models. We extend this analysis to deep neural\nnetworks (DNNs) in order to investigate their learning dynamics. To this end,\nwe propose layer-wise conditioning analysis, which explores the optimization\nlandscape with respect to each layer independently. Such an analysis is\ntheoretically supported under mild assumptions that approximately hold in\npractice. Based on our analysis, we show that batch normalization (BN) can\nstabilize the training, but sometimes result in the false impression of a local\nminimum, which has detrimental effects on the learning. Besides, we\nexperimentally observe that BN can improve the layer-wise conditioning of the\noptimization problem. Finally, we find that the last linear layer of a very\ndeep residual network displays ill-conditioned behavior. We solve this problem\nby only adding one BN layer before the last linear layer, which achieves\nimproved performance over the original and pre-activation residual networks.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=340&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Conditioning analysis uncovers the landscape of an optimization objective by\nexploring the spectrum of its curvature matrix. This has been well explored\ntheoretically for linear models. We extend this analysis to deep neural\nnetworks (DNNs) in order to investigate their learning dynamics. To this end,\nwe propose layer-wise conditioning analysis, which explores the optimization\nlandscape with respect to each layer independently. Such an analysis is\ntheoretically supported under mild assumptions that approximately hold in\npractice. Based on our analysis, we show that batch normalization (BN) can\nstabilize the training, but sometimes result in the false impression of a local\nminimum, which has detrimental effects on the learning. Besides, we\nexperimentally observe that BN can improve the layer-wise conditioning of the\noptimization problem. Finally, we find that the last linear layer of a very\ndeep residual network displays ill-conditioned behavior. We solve this problem\nby only adding one BN layer before the last linear layer, which achieves\nimproved performance over the original and pre-activation residual networks."}, "authors": ["Lei Huang", "Jie Qin", "Li Liu", "Fan Zhu", "Ling Shao"], "author_detail": {"name": "Ling Shao"}, "author": "Ling Shao", "arxiv_comment": "Accepted to ECCV 2020. The code is available at:\n  https://github.com/huangleiBuaa/LayerwiseCA", "links": [{"href": "http://arxiv.org/abs/2002.10801v3", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2002.10801v3", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2002.10801v3", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2002.10801v3", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/2005.14381v2", "guidislink": true, "updated": "2020-08-18T06:17:56Z", "updated_parsed": [2020, 8, 18, 6, 17, 56, 1, 231, 0], "published": "2020-05-29T04:42:28Z", "published_parsed": [2020, 5, 29, 4, 42, 28, 4, 150, 0], "title": "Bayesian network structure learning with causal effects in the presence\n  of latent variables", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=340&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Bayesian network structure learning with causal effects in the presence\n  of latent variables"}, "summary": "Latent variables may lead to spurious relationships that can be\nmisinterpreted as causal relationships. In Bayesian Networks (BNs), this\nchallenge is known as learning under causal insufficiency. Structure learning\nalgorithms that assume causal insufficiency tend to reconstruct the ancestral\ngraph of a BN, where bi-directed edges represent confounding and directed edges\nrepresent direct or ancestral relationships. This paper describes a hybrid\nstructure learning algorithm, called CCHM, which combines the constraint-based\npart of cFCI with hill-climbing score-based learning. The score-based process\nincorporates Pearl s do-calculus to measure causal effects and orientate edges\nthat would otherwise remain undirected, under the assumption the BN is a linear\nStructure Equation Model where data follow a multivariate Gaussian\ndistribution. Experiments based on both randomised and well-known networks show\nthat CCHM improves the state-of-the-art in terms of reconstructing the true\nancestral graph.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=340&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Latent variables may lead to spurious relationships that can be\nmisinterpreted as causal relationships. In Bayesian Networks (BNs), this\nchallenge is known as learning under causal insufficiency. Structure learning\nalgorithms that assume causal insufficiency tend to reconstruct the ancestral\ngraph of a BN, where bi-directed edges represent confounding and directed edges\nrepresent direct or ancestral relationships. This paper describes a hybrid\nstructure learning algorithm, called CCHM, which combines the constraint-based\npart of cFCI with hill-climbing score-based learning. The score-based process\nincorporates Pearl s do-calculus to measure causal effects and orientate edges\nthat would otherwise remain undirected, under the assumption the BN is a linear\nStructure Equation Model where data follow a multivariate Gaussian\ndistribution. Experiments based on both randomised and well-known networks show\nthat CCHM improves the state-of-the-art in terms of reconstructing the true\nancestral graph."}, "authors": ["Kiattikun Chobtham", "Anthony C. Constantinou"], "author_detail": {"name": "Anthony C. Constantinou"}, "author": "Anthony C. Constantinou", "links": [{"href": "http://arxiv.org/abs/2005.14381v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2005.14381v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.ML", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2005.14381v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2005.14381v2", "arxiv_comment": null, "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/2008.13618v1", "guidislink": true, "updated": "2020-08-27T19:56:27Z", "updated_parsed": [2020, 8, 27, 19, 56, 27, 3, 240, 0], "published": "2020-08-27T19:56:27Z", "published_parsed": [2020, 8, 27, 19, 56, 27, 3, 240, 0], "title": "Learning All Credible Bayesian Network Structures for Model Averaging", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=350&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Learning All Credible Bayesian Network Structures for Model Averaging"}, "summary": "A Bayesian network is a widely used probabilistic graphical model with\napplications in knowledge discovery and prediction. Learning a Bayesian network\n(BN) from data can be cast as an optimization problem using the well-known\nscore-and-search approach. However, selecting a single model (i.e., the best\nscoring BN) can be misleading or may not achieve the best possible accuracy. An\nalternative to committing to a single model is to perform some form of Bayesian\nor frequentist model averaging, where the space of possible BNs is sampled or\nenumerated in some fashion. Unfortunately, existing approaches for model\naveraging either severely restrict the structure of the Bayesian network or\nhave only been shown to scale to networks with fewer than 30 random variables.\nIn this paper, we propose a novel approach to model averaging inspired by\nperformance guarantees in approximation algorithms. Our approach has two\nprimary advantages. First, our approach only considers credible models in that\nthey are optimal or near-optimal in score. Second, our approach is more\nefficient and scales to significantly larger Bayesian networks than existing\napproaches.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=350&max_results=10&sortBy=relevance&sortOrder=descending", "value": "A Bayesian network is a widely used probabilistic graphical model with\napplications in knowledge discovery and prediction. Learning a Bayesian network\n(BN) from data can be cast as an optimization problem using the well-known\nscore-and-search approach. However, selecting a single model (i.e., the best\nscoring BN) can be misleading or may not achieve the best possible accuracy. An\nalternative to committing to a single model is to perform some form of Bayesian\nor frequentist model averaging, where the space of possible BNs is sampled or\nenumerated in some fashion. Unfortunately, existing approaches for model\naveraging either severely restrict the structure of the Bayesian network or\nhave only been shown to scale to networks with fewer than 30 random variables.\nIn this paper, we propose a novel approach to model averaging inspired by\nperformance guarantees in approximation algorithms. Our approach has two\nprimary advantages. First, our approach only considers credible models in that\nthey are optimal or near-optimal in score. Second, our approach is more\nefficient and scales to significantly larger Bayesian networks than existing\napproaches."}, "authors": ["Zhenyu A. Liao", "Charupriya Sharma", "James Cussens", "Peter van Beek"], "author_detail": {"name": "Peter van Beek"}, "author": "Peter van Beek", "arxiv_comment": "under review by JMLR. arXiv admin note: substantial text overlap with\n  arXiv:1811.05039", "links": [{"href": "http://arxiv.org/abs/2008.13618v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2008.13618v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2008.13618v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2008.13618v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/2010.03630v3", "guidislink": true, "updated": "2020-12-31T03:59:07Z", "updated_parsed": [2020, 12, 31, 3, 59, 7, 3, 366, 0], "published": "2020-10-07T19:56:47Z", "published_parsed": [2020, 10, 7, 19, 56, 47, 2, 281, 0], "title": "Revisiting Batch Normalization for Improving Corruption Robustness", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=350&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Revisiting Batch Normalization for Improving Corruption Robustness"}, "summary": "The performance of DNNs trained on clean images has been shown to decrease\nwhen the test images have common corruptions. In this work, we interpret\ncorruption robustness as a domain shift and propose to rectify batch\nnormalization (BN) statistics for improving model robustness. This is motivated\nby perceiving the shift from the clean domain to the corruption domain as a\nstyle shift that is represented by the BN statistics. We find that simply\nestimating and adapting the BN statistics on a few (32 for instance)\nrepresentation samples, without retraining the model, improves the corruption\nrobustness by a large margin on several benchmark datasets with a wide range of\nmodel architectures. For example, on ImageNet-C, statistics adaptation improves\nthe top1 accuracy of ResNet50 from 39.2% to 48.7%. Moreover, we find that this\ntechnique can further improve state-of-the-art robust models from 58.1% to\n63.3%.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=350&max_results=10&sortBy=relevance&sortOrder=descending", "value": "The performance of DNNs trained on clean images has been shown to decrease\nwhen the test images have common corruptions. In this work, we interpret\ncorruption robustness as a domain shift and propose to rectify batch\nnormalization (BN) statistics for improving model robustness. This is motivated\nby perceiving the shift from the clean domain to the corruption domain as a\nstyle shift that is represented by the BN statistics. We find that simply\nestimating and adapting the BN statistics on a few (32 for instance)\nrepresentation samples, without retraining the model, improves the corruption\nrobustness by a large margin on several benchmark datasets with a wide range of\nmodel architectures. For example, on ImageNet-C, statistics adaptation improves\nthe top1 accuracy of ResNet50 from 39.2% to 48.7%. Moreover, we find that this\ntechnique can further improve state-of-the-art robust models from 58.1% to\n63.3%."}, "authors": ["Philipp Benz", "Chaoning Zhang", "Adil Karjauv", "In So Kweon"], "author_detail": {"name": "In So Kweon"}, "author": "In So Kweon", "arxiv_comment": "Accepted at WACV 2021", "links": [{"href": "http://arxiv.org/abs/2010.03630v3", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2010.03630v3", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2010.03630v3", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2010.03630v3", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/2010.07160v1", "guidislink": true, "updated": "2020-10-14T15:25:39Z", "updated_parsed": [2020, 10, 14, 15, 25, 39, 2, 288, 0], "published": "2020-10-14T15:25:39Z", "published_parsed": [2020, 10, 14, 15, 25, 39, 2, 288, 0], "title": "WeightAlign: Normalizing Activations by Weight Alignment", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=350&max_results=10&sortBy=relevance&sortOrder=descending", "value": "WeightAlign: Normalizing Activations by Weight Alignment"}, "summary": "Batch normalization (BN) allows training very deep networks by normalizing\nactivations by mini-batch sample statistics which renders BN unstable for small\nbatch sizes. Current small-batch solutions such as Instance Norm, Layer Norm,\nand Group Norm use channel statistics which can be computed even for a single\nsample. Such methods are less stable than BN as they critically depend on the\nstatistics of a single input sample. To address this problem, we propose a\nnormalization of activation without sample statistics. We present WeightAlign:\na method that normalizes the weights by the mean and scaled standard derivation\ncomputed within a filter, which normalizes activations without computing any\nsample statistics. Our proposed method is independent of batch size and stable\nover a wide range of batch sizes. Because weight statistics are orthogonal to\nsample statistics, we can directly combine WeightAlign with any method for\nactivation normalization. We experimentally demonstrate these benefits for\nclassification on CIFAR-10, CIFAR-100, ImageNet, for semantic segmentation on\nPASCAL VOC 2012 and for domain adaptation on Office-31.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=350&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Batch normalization (BN) allows training very deep networks by normalizing\nactivations by mini-batch sample statistics which renders BN unstable for small\nbatch sizes. Current small-batch solutions such as Instance Norm, Layer Norm,\nand Group Norm use channel statistics which can be computed even for a single\nsample. Such methods are less stable than BN as they critically depend on the\nstatistics of a single input sample. To address this problem, we propose a\nnormalization of activation without sample statistics. We present WeightAlign:\na method that normalizes the weights by the mean and scaled standard derivation\ncomputed within a filter, which normalizes activations without computing any\nsample statistics. Our proposed method is independent of batch size and stable\nover a wide range of batch sizes. Because weight statistics are orthogonal to\nsample statistics, we can directly combine WeightAlign with any method for\nactivation normalization. We experimentally demonstrate these benefits for\nclassification on CIFAR-10, CIFAR-100, ImageNet, for semantic segmentation on\nPASCAL VOC 2012 and for domain adaptation on Office-31."}, "authors": ["Xiangwei Shi", "Yunqiang Li", "Xin Liu", "Jan van Gemert"], "author_detail": {"name": "Jan van Gemert"}, "author": "Jan van Gemert", "arxiv_comment": "The first three authors contributed equally; accepted by ICPR 2020", "links": [{"href": "http://arxiv.org/abs/2010.07160v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2010.07160v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2010.07160v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2010.07160v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/2010.11773v1", "guidislink": true, "updated": "2020-10-22T14:47:55Z", "updated_parsed": [2020, 10, 22, 14, 47, 55, 3, 296, 0], "published": "2020-10-22T14:47:55Z", "published_parsed": [2020, 10, 22, 14, 47, 55, 3, 296, 0], "title": "On Resource-Efficient Bayesian Network Classifiers and Deep Neural\n  Networks", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=350&max_results=10&sortBy=relevance&sortOrder=descending", "value": "On Resource-Efficient Bayesian Network Classifiers and Deep Neural\n  Networks"}, "summary": "We present two methods to reduce the complexity of Bayesian network (BN)\nclassifiers. First, we introduce quantization-aware training using the\nstraight-through gradient estimator to quantize the parameters of BNs to few\nbits. Second, we extend a recently proposed differentiable tree-augmented naive\nBayes (TAN) structure learning approach by also considering the model size.\nBoth methods are motivated by recent developments in the deep learning\ncommunity, and they provide effective means to trade off between model size and\nprediction accuracy, which is demonstrated in extensive experiments.\nFurthermore, we contrast quantized BN classifiers with quantized deep neural\nnetworks (DNNs) for small-scale scenarios which have hardly been investigated\nin the literature. We show Pareto optimal models with respect to model size,\nnumber of operations, and test error and find that both model classes are\nviable options.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=350&max_results=10&sortBy=relevance&sortOrder=descending", "value": "We present two methods to reduce the complexity of Bayesian network (BN)\nclassifiers. First, we introduce quantization-aware training using the\nstraight-through gradient estimator to quantize the parameters of BNs to few\nbits. Second, we extend a recently proposed differentiable tree-augmented naive\nBayes (TAN) structure learning approach by also considering the model size.\nBoth methods are motivated by recent developments in the deep learning\ncommunity, and they provide effective means to trade off between model size and\nprediction accuracy, which is demonstrated in extensive experiments.\nFurthermore, we contrast quantized BN classifiers with quantized deep neural\nnetworks (DNNs) for small-scale scenarios which have hardly been investigated\nin the literature. We show Pareto optimal models with respect to model size,\nnumber of operations, and test error and find that both model classes are\nviable options."}, "authors": ["Wolfgang Roth", "Gnther Schindler", "Holger Frning", "Franz Pernkopf"], "author_detail": {"name": "Franz Pernkopf"}, "author": "Franz Pernkopf", "arxiv_comment": "Accepted at ICPR 2020", "links": [{"href": "http://arxiv.org/abs/2010.11773v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2010.11773v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.ML", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2010.11773v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2010.11773v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/2011.14150v1", "guidislink": true, "updated": "2020-11-28T15:42:36Z", "updated_parsed": [2020, 11, 28, 15, 42, 36, 5, 333, 0], "published": "2020-11-28T15:42:36Z", "published_parsed": [2020, 11, 28, 15, 42, 36, 5, 333, 0], "title": "Batch Normalization with Enhanced Linear Transformation", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=350&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Batch Normalization with Enhanced Linear Transformation"}, "summary": "Batch normalization (BN) is a fundamental unit in modern deep networks, in\nwhich a linear transformation module was designed for improving BN's\nflexibility of fitting complex data distributions. In this paper, we\ndemonstrate properly enhancing this linear transformation module can\neffectively improve the ability of BN. Specifically, rather than using a single\nneuron, we propose to additionally consider each neuron's neighborhood for\ncalculating the outputs of the linear transformation. Our method, named BNET,\ncan be implemented with 2-3 lines of code in most deep learning libraries.\nDespite the simplicity, BNET brings consistent performance gains over a wide\nrange of backbones and visual benchmarks. Moreover, we verify that BNET\naccelerates the convergence of network training and enhances spatial\ninformation by assigning the important neurons with larger weights accordingly.\nThe code is available at https://github.com/yuhuixu1993/BNET.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=350&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Batch normalization (BN) is a fundamental unit in modern deep networks, in\nwhich a linear transformation module was designed for improving BN's\nflexibility of fitting complex data distributions. In this paper, we\ndemonstrate properly enhancing this linear transformation module can\neffectively improve the ability of BN. Specifically, rather than using a single\nneuron, we propose to additionally consider each neuron's neighborhood for\ncalculating the outputs of the linear transformation. Our method, named BNET,\ncan be implemented with 2-3 lines of code in most deep learning libraries.\nDespite the simplicity, BNET brings consistent performance gains over a wide\nrange of backbones and visual benchmarks. Moreover, we verify that BNET\naccelerates the convergence of network training and enhances spatial\ninformation by assigning the important neurons with larger weights accordingly.\nThe code is available at https://github.com/yuhuixu1993/BNET."}, "authors": ["Yuhui Xu", "Lingxi Xie", "Cihang Xie", "Jieru Mei", "Siyuan Qiao", "Wei Shen", "Hongkai Xiong", "Alan Yuille"], "author_detail": {"name": "Alan Yuille"}, "author": "Alan Yuille", "arxiv_comment": "12 pages. The code is available at\n  https://github.com/yuhuixu1993/BNET", "links": [{"href": "http://arxiv.org/abs/2011.14150v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2011.14150v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2011.14150v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2011.14150v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/2012.09429v1", "guidislink": true, "updated": "2020-12-17T07:42:40Z", "updated_parsed": [2020, 12, 17, 7, 42, 40, 3, 352, 0], "published": "2020-12-17T07:42:40Z", "published_parsed": [2020, 12, 17, 7, 42, 40, 3, 352, 0], "title": "A Fast Algorithm for Heart Disease Prediction using Bayesian Network\n  Model", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=360&max_results=10&sortBy=relevance&sortOrder=descending", "value": "A Fast Algorithm for Heart Disease Prediction using Bayesian Network\n  Model"}, "summary": "Cardiovascular disease is the number one cause of death all over the world.\nData mining can help to retrieve valuable knowledge from available data from\nthe health sector. It helps to train a model to predict patients' health which\nwill be faster as compared to clinical experimentation. Various implementation\nof machine learning algorithms such as Logistic Regression, K-Nearest Neighbor,\nNaive Bayes (NB), Support Vector Machine, etc. have been applied on Cleveland\nheart datasets but there has been a limit to modeling using Bayesian Network\n(BN). This research applied BN modeling to discover the relationship between 14\nrelevant attributes of the Cleveland heart data collected from The UCI\nrepository. The aim is to check how the dependency between attributes affects\nthe performance of the classifier. The BN produces a reliable and transparent\ngraphical representation between the attributes with the ability to predict new\nscenarios. The model has an accuracy of 85%. It was concluded that the model\noutperformed the NB classifier which has an accuracy of 80%.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=360&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Cardiovascular disease is the number one cause of death all over the world.\nData mining can help to retrieve valuable knowledge from available data from\nthe health sector. It helps to train a model to predict patients' health which\nwill be faster as compared to clinical experimentation. Various implementation\nof machine learning algorithms such as Logistic Regression, K-Nearest Neighbor,\nNaive Bayes (NB), Support Vector Machine, etc. have been applied on Cleveland\nheart datasets but there has been a limit to modeling using Bayesian Network\n(BN). This research applied BN modeling to discover the relationship between 14\nrelevant attributes of the Cleveland heart data collected from The UCI\nrepository. The aim is to check how the dependency between attributes affects\nthe performance of the classifier. The BN produces a reliable and transparent\ngraphical representation between the attributes with the ability to predict new\nscenarios. The model has an accuracy of 85%. It was concluded that the model\noutperformed the NB classifier which has an accuracy of 80%."}, "authors": ["Mistura Muibideen", "Rajesh Prasad"], "author_detail": {"name": "Rajesh Prasad"}, "author": "Rajesh Prasad", "links": [{"href": "http://arxiv.org/abs/2012.09429v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2012.09429v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "I.2; G.3", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2012.09429v1", "affiliation": "Department of Computer Science African University of Science and Technology, Abuja, Nigeria", "arxiv_url": "http://arxiv.org/abs/2012.09429v1", "arxiv_comment": null, "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/2101.02916v1", "guidislink": true, "updated": "2021-01-08T08:53:07Z", "updated_parsed": [2021, 1, 8, 8, 53, 7, 4, 8, 0], "published": "2021-01-08T08:53:07Z", "published_parsed": [2021, 1, 8, 8, 53, 7, 4, 8, 0], "title": "Towards Accelerating Training of Batch Normalization: A Manifold\n  Perspective", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=360&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Towards Accelerating Training of Batch Normalization: A Manifold\n  Perspective"}, "summary": "Batch normalization (BN) has become a crucial component across diverse deep\nneural networks. The network with BN is invariant to positively linear\nre-scaling of weights, which makes there exist infinite functionally equivalent\nnetworks with various scales of weights. However, optimizing these equivalent\nnetworks with the first-order method such as stochastic gradient descent will\nconverge to different local optima owing to different gradients across\ntraining. To alleviate this, we propose a quotient manifold \\emph{PSI\nmanifold}, in which all the equivalent weights of the network with BN are\nregarded as the same one element. Then, gradient descent and stochastic\ngradient descent on the PSI manifold are also constructed. The two algorithms\nguarantee that every group of equivalent weights (caused by positively\nre-scaling) converge to the equivalent optima. Besides that, we give the\nconvergence rate of the proposed algorithms on PSI manifold and justify that\nthey accelerate training compared with the algorithms on the Euclidean weight\nspace. Empirical studies show that our algorithms can consistently achieve\nbetter performances over various experimental settings.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=360&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Batch normalization (BN) has become a crucial component across diverse deep\nneural networks. The network with BN is invariant to positively linear\nre-scaling of weights, which makes there exist infinite functionally equivalent\nnetworks with various scales of weights. However, optimizing these equivalent\nnetworks with the first-order method such as stochastic gradient descent will\nconverge to different local optima owing to different gradients across\ntraining. To alleviate this, we propose a quotient manifold \\emph{PSI\nmanifold}, in which all the equivalent weights of the network with BN are\nregarded as the same one element. Then, gradient descent and stochastic\ngradient descent on the PSI manifold are also constructed. The two algorithms\nguarantee that every group of equivalent weights (caused by positively\nre-scaling) converge to the equivalent optima. Besides that, we give the\nconvergence rate of the proposed algorithms on PSI manifold and justify that\nthey accelerate training compared with the algorithms on the Euclidean weight\nspace. Empirical studies show that our algorithms can consistently achieve\nbetter performances over various experimental settings."}, "authors": ["Mingyang Yi", "Qi Meng", "Wei Chen", "Zhi-Ming Ma"], "author_detail": {"name": "Zhi-Ming Ma"}, "author": "Zhi-Ming Ma", "links": [{"href": "http://arxiv.org/abs/2101.02916v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2101.02916v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2101.02916v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2101.02916v1", "arxiv_comment": null, "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1210.4900v1", "guidislink": true, "updated": "2012-10-16T17:50:37Z", "updated_parsed": [2012, 10, 16, 17, 50, 37, 1, 290, 0], "published": "2012-10-16T17:50:37Z", "published_parsed": [2012, 10, 16, 17, 50, 37, 1, 290, 0], "title": "Probability and Asset Updating using Bayesian Networks for Combinatorial\n  Prediction Markets", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=370&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Probability and Asset Updating using Bayesian Networks for Combinatorial\n  Prediction Markets"}, "summary": "A market-maker-based prediction market lets forecasters aggregate information\nby editing a consensus probability distribution either directly or by trading\nsecurities that pay off contingent on an event of interest. Combinatorial\nprediction markets allow trading on any event that can be specified as a\ncombination of a base set of events. However, explicitly representing the full\njoint distribution is infeasible for markets with more than a few base events.\nA factored representation such as a Bayesian network (BN) can achieve tractable\ncomputation for problems with many related variables. Standard BN inference\nalgorithms, such as the junction tree algorithm, can be used to update a\nrepresentation of the entire joint distribution given a change to any local\nconditional probability. However, in order to let traders reuse assets from\nprior trades while never allowing assets to become negative, a BN based\nprediction market also needs to update a representation of each user's assets\nand find the conditional state in which a user has minimum assets. Users also\nfind it useful to see their expected assets given an edit outcome. We show how\nto generalize the junction tree algorithm to perform all these computations.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=370&max_results=10&sortBy=relevance&sortOrder=descending", "value": "A market-maker-based prediction market lets forecasters aggregate information\nby editing a consensus probability distribution either directly or by trading\nsecurities that pay off contingent on an event of interest. Combinatorial\nprediction markets allow trading on any event that can be specified as a\ncombination of a base set of events. However, explicitly representing the full\njoint distribution is infeasible for markets with more than a few base events.\nA factored representation such as a Bayesian network (BN) can achieve tractable\ncomputation for problems with many related variables. Standard BN inference\nalgorithms, such as the junction tree algorithm, can be used to update a\nrepresentation of the entire joint distribution given a change to any local\nconditional probability. However, in order to let traders reuse assets from\nprior trades while never allowing assets to become negative, a BN based\nprediction market also needs to update a representation of each user's assets\nand find the conditional state in which a user has minimum assets. Users also\nfind it useful to see their expected assets given an edit outcome. We show how\nto generalize the junction tree algorithm to perform all these computations."}, "authors": ["Wei Sun", "Robin Hanson", "Kathryn Blackmond Laskey", "Charles Twardy"], "author_detail": {"name": "Charles Twardy"}, "author": "Charles Twardy", "arxiv_comment": "Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty\n  in Artificial Intelligence (UAI2012)", "links": [{"href": "http://arxiv.org/abs/1210.4900v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1210.4900v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "q-fin.TR", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1210.4900v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1210.4900v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1506.06868v1", "guidislink": true, "updated": "2015-06-23T05:39:34Z", "updated_parsed": [2015, 6, 23, 5, 39, 34, 1, 174, 0], "published": "2015-06-23T05:39:34Z", "published_parsed": [2015, 6, 23, 5, 39, 34, 1, 174, 0], "title": "Learning Discriminative Bayesian Networks from High-dimensional\n  Continuous Neuroimaging Data", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=380&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Learning Discriminative Bayesian Networks from High-dimensional\n  Continuous Neuroimaging Data"}, "summary": "Due to its causal semantics, Bayesian networks (BN) have been widely employed\nto discover the underlying data relationship in exploratory studies, such as\nbrain research. Despite its success in modeling the probability distribution of\nvariables, BN is naturally a generative model, which is not necessarily\ndiscriminative. This may cause the ignorance of subtle but critical network\nchanges that are of investigation values across populations. In this paper, we\npropose to improve the discriminative power of BN models for continuous\nvariables from two different perspectives. This brings two general\ndiscriminative learning frameworks for Gaussian Bayesian networks (GBN). In the\nfirst framework, we employ Fisher kernel to bridge the generative models of GBN\nand the discriminative classifiers of SVMs, and convert the GBN parameter\nlearning to Fisher kernel learning via minimizing a generalization error bound\nof SVMs. In the second framework, we employ the max-margin criterion and build\nit directly upon GBN models to explicitly optimize the classification\nperformance of the GBNs. The advantages and disadvantages of the two frameworks\nare discussed and experimentally compared. Both of them demonstrate strong\npower in learning discriminative parameters of GBNs for neuroimaging based\nbrain network analysis, as well as maintaining reasonable representation\ncapacity. The contributions of this paper also include a new Directed Acyclic\nGraph (DAG) constraint with theoretical guarantee to ensure the graph validity\nof GBN.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=380&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Due to its causal semantics, Bayesian networks (BN) have been widely employed\nto discover the underlying data relationship in exploratory studies, such as\nbrain research. Despite its success in modeling the probability distribution of\nvariables, BN is naturally a generative model, which is not necessarily\ndiscriminative. This may cause the ignorance of subtle but critical network\nchanges that are of investigation values across populations. In this paper, we\npropose to improve the discriminative power of BN models for continuous\nvariables from two different perspectives. This brings two general\ndiscriminative learning frameworks for Gaussian Bayesian networks (GBN). In the\nfirst framework, we employ Fisher kernel to bridge the generative models of GBN\nand the discriminative classifiers of SVMs, and convert the GBN parameter\nlearning to Fisher kernel learning via minimizing a generalization error bound\nof SVMs. In the second framework, we employ the max-margin criterion and build\nit directly upon GBN models to explicitly optimize the classification\nperformance of the GBNs. The advantages and disadvantages of the two frameworks\nare discussed and experimentally compared. Both of them demonstrate strong\npower in learning discriminative parameters of GBNs for neuroimaging based\nbrain network analysis, as well as maintaining reasonable representation\ncapacity. The contributions of this paper also include a new Directed Acyclic\nGraph (DAG) constraint with theoretical guarantee to ensure the graph validity\nof GBN."}, "authors": ["Luping Zhou", "Lei Wang", "Lingqiao Liu", "Philip Ogunbona", "Dinggang Shen"], "author_detail": {"name": "Dinggang Shen"}, "author": "Dinggang Shen", "arxiv_comment": "16 pages and 5 figures for the article (excluding appendix)", "links": [{"href": "http://arxiv.org/abs/1506.06868v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1506.06868v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1506.06868v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1506.06868v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1807.09441v3", "guidislink": true, "updated": "2020-03-23T03:31:11Z", "updated_parsed": [2020, 3, 23, 3, 31, 11, 0, 83, 0], "published": "2018-07-25T05:51:15Z", "published_parsed": [2018, 7, 25, 5, 51, 15, 2, 206, 0], "title": "Two at Once: Enhancing Learning and Generalization Capacities via\n  IBN-Net", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=380&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Two at Once: Enhancing Learning and Generalization Capacities via\n  IBN-Net"}, "summary": "Convolutional neural networks (CNNs) have achieved great successes in many\ncomputer vision problems. Unlike existing works that designed CNN architectures\nto improve performance on a single task of a single domain and not\ngeneralizable, we present IBN-Net, a novel convolutional architecture, which\nremarkably enhances a CNN's modeling ability on one domain (e.g. Cityscapes) as\nwell as its generalization capacity on another domain (e.g. GTA5) without\nfinetuning. IBN-Net carefully integrates Instance Normalization (IN) and Batch\nNormalization (BN) as building blocks, and can be wrapped into many advanced\ndeep networks to improve their performances. This work has three key\ncontributions. (1) By delving into IN and BN, we disclose that IN learns\nfeatures that are invariant to appearance changes, such as colors, styles, and\nvirtuality/reality, while BN is essential for preserving content related\ninformation. (2) IBN-Net can be applied to many advanced deep architectures,\nsuch as DenseNet, ResNet, ResNeXt, and SENet, and consistently improve their\nperformance without increasing computational cost. (3) When applying the\ntrained networks to new domains, e.g. from GTA5 to Cityscapes, IBN-Net achieves\ncomparable improvements as domain adaptation methods, even without using data\nfrom the target domain. With IBN-Net, we won the 1st place on the WAD 2018\nChallenge Drivable Area track, with an mIoU of 86.18%.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=380&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Convolutional neural networks (CNNs) have achieved great successes in many\ncomputer vision problems. Unlike existing works that designed CNN architectures\nto improve performance on a single task of a single domain and not\ngeneralizable, we present IBN-Net, a novel convolutional architecture, which\nremarkably enhances a CNN's modeling ability on one domain (e.g. Cityscapes) as\nwell as its generalization capacity on another domain (e.g. GTA5) without\nfinetuning. IBN-Net carefully integrates Instance Normalization (IN) and Batch\nNormalization (BN) as building blocks, and can be wrapped into many advanced\ndeep networks to improve their performances. This work has three key\ncontributions. (1) By delving into IN and BN, we disclose that IN learns\nfeatures that are invariant to appearance changes, such as colors, styles, and\nvirtuality/reality, while BN is essential for preserving content related\ninformation. (2) IBN-Net can be applied to many advanced deep architectures,\nsuch as DenseNet, ResNet, ResNeXt, and SENet, and consistently improve their\nperformance without increasing computational cost. (3) When applying the\ntrained networks to new domains, e.g. from GTA5 to Cityscapes, IBN-Net achieves\ncomparable improvements as domain adaptation methods, even without using data\nfrom the target domain. With IBN-Net, we won the 1st place on the WAD 2018\nChallenge Drivable Area track, with an mIoU of 86.18%."}, "authors": ["Xingang Pan", "Ping Luo", "Jianping Shi", "Xiaoou Tang"], "author_detail": {"name": "Xiaoou Tang"}, "author": "Xiaoou Tang", "arxiv_comment": "Accepted for publication at ECCV 2018", "links": [{"href": "http://arxiv.org/abs/1807.09441v3", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1807.09441v3", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1807.09441v3", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1807.09441v3", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1809.03783v3", "guidislink": true, "updated": "2019-01-12T21:12:14Z", "updated_parsed": [2019, 1, 12, 21, 12, 14, 5, 12, 0], "published": "2018-09-11T10:27:45Z", "published_parsed": [2018, 9, 11, 10, 27, 45, 1, 254, 0], "title": "Normalization in Training U-Net for 2D Biomedical Semantic Segmentation", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=380&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Normalization in Training U-Net for 2D Biomedical Semantic Segmentation"}, "summary": "2D biomedical semantic segmentation is important for robotic vision in\nsurgery. Segmentation methods based on Deep Convolutional Neural Network (DCNN)\ncan out-perform conventional methods in terms of both accuracy and levels of\nautomation. One common issue in training a DCNN for biomedical semantic\nsegmentation is the internal covariate shift where the training of\nconvolutional kernels is encumbered by the distribution change of input\nfeatures, hence both the training speed and performance are decreased. Batch\nNormalization (BN) is the first proposed method for addressing internal\ncovariate shift and is widely used. Instance Normalization (IN) and Layer\nNormalization (LN) have also been proposed. Group Normalization (GN) is\nproposed more recently and has not yet been applied to 2D biomedical semantic\nsegmentation, however, no specific validations on GN were given. Most DCNNs for\nbiomedical semantic segmentation adopt BN as the normalization method by\ndefault, without reviewing its performance. In this paper, four normalization\nmethods - BN, IN, LN and GN are compared in details, specifically for 2D\nbiomedical semantic segmentation. U-Net is adopted as the basic DCNN structure.\nThree datasets regarding the Right Ventricle (RV), aorta, and Left Ventricle\n(LV) are used for the validation. The results show that detailed subdivision of\nthe feature map, i.e. GN with a large group number or IN, achieves higher\naccuracy. This accuracy improvement mainly comes from better model\ngeneralization. Codes are uploaded and maintained at Xiao-Yun Zhou's Github.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=380&max_results=10&sortBy=relevance&sortOrder=descending", "value": "2D biomedical semantic segmentation is important for robotic vision in\nsurgery. Segmentation methods based on Deep Convolutional Neural Network (DCNN)\ncan out-perform conventional methods in terms of both accuracy and levels of\nautomation. One common issue in training a DCNN for biomedical semantic\nsegmentation is the internal covariate shift where the training of\nconvolutional kernels is encumbered by the distribution change of input\nfeatures, hence both the training speed and performance are decreased. Batch\nNormalization (BN) is the first proposed method for addressing internal\ncovariate shift and is widely used. Instance Normalization (IN) and Layer\nNormalization (LN) have also been proposed. Group Normalization (GN) is\nproposed more recently and has not yet been applied to 2D biomedical semantic\nsegmentation, however, no specific validations on GN were given. Most DCNNs for\nbiomedical semantic segmentation adopt BN as the normalization method by\ndefault, without reviewing its performance. In this paper, four normalization\nmethods - BN, IN, LN and GN are compared in details, specifically for 2D\nbiomedical semantic segmentation. U-Net is adopted as the basic DCNN structure.\nThree datasets regarding the Right Ventricle (RV), aorta, and Left Ventricle\n(LV) are used for the validation. The results show that detailed subdivision of\nthe feature map, i.e. GN with a large group number or IN, achieves higher\naccuracy. This accuracy improvement mainly comes from better model\ngeneralization. Codes are uploaded and maintained at Xiao-Yun Zhou's Github."}, "authors": ["Xiao-Yun Zhou", "Guang-Zhong Yang"], "author_detail": {"name": "Guang-Zhong Yang"}, "author": "Guang-Zhong Yang", "arxiv_comment": "5 figures, 5 tables, published on IEEE Robotics and Automation\n  Letters 2019", "links": [{"href": "http://arxiv.org/abs/1809.03783v3", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1809.03783v3", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1809.03783v3", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1809.03783v3", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1802.03133v2", "guidislink": true, "updated": "2018-02-28T02:01:50Z", "updated_parsed": [2018, 2, 28, 2, 1, 50, 2, 59, 0], "published": "2018-02-09T05:19:16Z", "published_parsed": [2018, 2, 9, 5, 19, 16, 4, 40, 0], "title": "Batch Kalman Normalization: Towards Training Deep Neural Networks with\n  Micro-Batches", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=390&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Batch Kalman Normalization: Towards Training Deep Neural Networks with\n  Micro-Batches"}, "summary": "As an indispensable component, Batch Normalization (BN) has successfully\nimproved the training of deep neural networks (DNNs) with mini-batches, by\nnormalizing the distribution of the internal representation for each hidden\nlayer. However, the effectiveness of BN would diminish with scenario of\nmicro-batch (e.g., less than 10 samples in a mini-batch), since the estimated\nstatistics in a mini-batch are not reliable with insufficient samples. In this\npaper, we present a novel normalization method, called Batch Kalman\nNormalization (BKN), for improving and accelerating the training of DNNs,\nparticularly under the context of micro-batches. Specifically, unlike the\nexisting solutions treating each hidden layer as an isolated system, BKN treats\nall the layers in a network as a whole system, and estimates the statistics of\na certain layer by considering the distributions of all its preceding layers,\nmimicking the merits of Kalman Filtering. BKN has two appealing properties.\nFirst, it enables more stable training and faster convergence compared to\nprevious works. Second, training DNNs using BKN performs substantially better\nthan those using BN and its variants, especially when very small mini-batches\nare presented. On the image classification benchmark of ImageNet, using BKN\npowered networks we improve upon the best-published model-zoo results: reaching\n74.0% top-1 val accuracy for InceptionV2. More importantly, using BKN achieves\nthe comparable accuracy with extremely smaller batch size, such as 64 times\nsmaller on CIFAR-10/100 and 8 times smaller on ImageNet.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=390&max_results=10&sortBy=relevance&sortOrder=descending", "value": "As an indispensable component, Batch Normalization (BN) has successfully\nimproved the training of deep neural networks (DNNs) with mini-batches, by\nnormalizing the distribution of the internal representation for each hidden\nlayer. However, the effectiveness of BN would diminish with scenario of\nmicro-batch (e.g., less than 10 samples in a mini-batch), since the estimated\nstatistics in a mini-batch are not reliable with insufficient samples. In this\npaper, we present a novel normalization method, called Batch Kalman\nNormalization (BKN), for improving and accelerating the training of DNNs,\nparticularly under the context of micro-batches. Specifically, unlike the\nexisting solutions treating each hidden layer as an isolated system, BKN treats\nall the layers in a network as a whole system, and estimates the statistics of\na certain layer by considering the distributions of all its preceding layers,\nmimicking the merits of Kalman Filtering. BKN has two appealing properties.\nFirst, it enables more stable training and faster convergence compared to\nprevious works. Second, training DNNs using BKN performs substantially better\nthan those using BN and its variants, especially when very small mini-batches\nare presented. On the image classification benchmark of ImageNet, using BKN\npowered networks we improve upon the best-published model-zoo results: reaching\n74.0% top-1 val accuracy for InceptionV2. More importantly, using BKN achieves\nthe comparable accuracy with extremely smaller batch size, such as 64 times\nsmaller on CIFAR-10/100 and 8 times smaller on ImageNet."}, "authors": ["Guangrun Wang", "Jiefeng Peng", "Ping Luo", "Xinjiang Wang", "Liang Lin"], "author_detail": {"name": "Liang Lin"}, "author": "Liang Lin", "arxiv_comment": "We presented how to improve and accelerate the training of DNNs,\n  particularly under the context of micro-batches. (Submitted to IJCAI 2018)", "links": [{"href": "http://arxiv.org/abs/1802.03133v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1802.03133v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.ML", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1802.03133v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1802.03133v2", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1903.10520v2", "guidislink": true, "updated": "2020-08-09T21:25:15Z", "updated_parsed": [2020, 8, 9, 21, 25, 15, 6, 222, 0], "published": "2019-03-25T18:00:05Z", "published_parsed": [2019, 3, 25, 18, 0, 5, 0, 84, 0], "title": "Micro-Batch Training with Batch-Channel Normalization and Weight\n  Standardization", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=390&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Micro-Batch Training with Batch-Channel Normalization and Weight\n  Standardization"}, "summary": "Batch Normalization (BN) has become an out-of-box technique to improve deep\nnetwork training. However, its effectiveness is limited for micro-batch\ntraining, i.e., each GPU typically has only 1-2 images for training, which is\ninevitable for many computer vision tasks, e.g., object detection and semantic\nsegmentation, constrained by memory consumption. To address this issue, we\npropose Weight Standardization (WS) and Batch-Channel Normalization (BCN) to\nbring two success factors of BN into micro-batch training: 1) the smoothing\neffects on the loss landscape and 2) the ability to avoid harmful elimination\nsingularities along the training trajectory. WS standardizes the weights in\nconvolutional layers to smooth the loss landscape by reducing the Lipschitz\nconstants of the loss and the gradients; BCN combines batch and channel\nnormalizations and leverages estimated statistics of the activations in\nconvolutional layers to keep networks away from elimination singularities. We\nvalidate WS and BCN on comprehensive computer vision tasks, including image\nclassification, object detection, instance segmentation, video recognition and\nsemantic segmentation. All experimental results consistently show that WS and\nBCN improve micro-batch training significantly. Moreover, using WS and BCN with\nmicro-batch training is even able to match or outperform the performances of BN\nwith large-batch training.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=390&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Batch Normalization (BN) has become an out-of-box technique to improve deep\nnetwork training. However, its effectiveness is limited for micro-batch\ntraining, i.e., each GPU typically has only 1-2 images for training, which is\ninevitable for many computer vision tasks, e.g., object detection and semantic\nsegmentation, constrained by memory consumption. To address this issue, we\npropose Weight Standardization (WS) and Batch-Channel Normalization (BCN) to\nbring two success factors of BN into micro-batch training: 1) the smoothing\neffects on the loss landscape and 2) the ability to avoid harmful elimination\nsingularities along the training trajectory. WS standardizes the weights in\nconvolutional layers to smooth the loss landscape by reducing the Lipschitz\nconstants of the loss and the gradients; BCN combines batch and channel\nnormalizations and leverages estimated statistics of the activations in\nconvolutional layers to keep networks away from elimination singularities. We\nvalidate WS and BCN on comprehensive computer vision tasks, including image\nclassification, object detection, instance segmentation, video recognition and\nsemantic segmentation. All experimental results consistently show that WS and\nBCN improve micro-batch training significantly. Moreover, using WS and BCN with\nmicro-batch training is even able to match or outperform the performances of BN\nwith large-batch training."}, "authors": ["Siyuan Qiao", "Huiyu Wang", "Chenxi Liu", "Wei Shen", "Alan Yuille"], "author_detail": {"name": "Alan Yuille"}, "author": "Alan Yuille", "links": [{"href": "http://arxiv.org/abs/1903.10520v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1903.10520v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1903.10520v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1903.10520v2", "arxiv_comment": null, "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1904.03392v5", "guidislink": true, "updated": "2020-07-28T17:30:11Z", "updated_parsed": [2020, 7, 28, 17, 30, 11, 1, 210, 0], "published": "2019-04-06T09:17:51Z", "published_parsed": [2019, 4, 6, 9, 17, 51, 5, 96, 0], "title": "Effective and Efficient Dropout for Deep Convolutional Neural Networks", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=390&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Effective and Efficient Dropout for Deep Convolutional Neural Networks"}, "summary": "Convolutional Neural networks (CNNs) based applications have become\nubiquitous, where proper regularization is greatly needed. To prevent large\nneural network models from overfitting, dropout has been widely used as an\nefficient regularization technique in practice. However, many recent works show\nthat the standard dropout is ineffective or even detrimental to the training of\nCNNs. In this paper, we revisit this issue and examine various dropout variants\nin an attempt to improve existing dropout-based regularization techniques for\nCNNs. We attribute the failure of standard dropout to the conflict between the\nstochasticity of dropout and its following Batch Normalization (BN), and\npropose to reduce the conflict by placing dropout operations right before the\nconvolutional operation instead of BN, or totally address this issue by\nreplacing BN with Group Normalization (GN). We further introduce a structurally\nmore suited dropout variant Drop-Conv2d, which provides more efficient and\neffective regularization for deep CNNs. These dropout variants can be readily\nintegrated into the building blocks of CNNs and implemented in existing deep\nlearning platforms. Extensive experiments on benchmark datasets including\nCIFAR, SVHN and ImageNet are conducted to compare the existing building blocks\nand the proposed ones with dropout training. Results show that our building\nblocks improve over state-of-the-art CNNs significantly, which is mainly due to\nthe better regularization and implicit model ensemble effect.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=390&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Convolutional Neural networks (CNNs) based applications have become\nubiquitous, where proper regularization is greatly needed. To prevent large\nneural network models from overfitting, dropout has been widely used as an\nefficient regularization technique in practice. However, many recent works show\nthat the standard dropout is ineffective or even detrimental to the training of\nCNNs. In this paper, we revisit this issue and examine various dropout variants\nin an attempt to improve existing dropout-based regularization techniques for\nCNNs. We attribute the failure of standard dropout to the conflict between the\nstochasticity of dropout and its following Batch Normalization (BN), and\npropose to reduce the conflict by placing dropout operations right before the\nconvolutional operation instead of BN, or totally address this issue by\nreplacing BN with Group Normalization (GN). We further introduce a structurally\nmore suited dropout variant Drop-Conv2d, which provides more efficient and\neffective regularization for deep CNNs. These dropout variants can be readily\nintegrated into the building blocks of CNNs and implemented in existing deep\nlearning platforms. Extensive experiments on benchmark datasets including\nCIFAR, SVHN and ImageNet are conducted to compare the existing building blocks\nand the proposed ones with dropout training. Results show that our building\nblocks improve over state-of-the-art CNNs significantly, which is mainly due to\nthe better regularization and implicit model ensemble effect."}, "authors": ["Shaofeng Cai", "Yao Shu", "Gang Chen", "Beng Chin Ooi", "Wei Wang", "Meihui Zhang"], "author_detail": {"name": "Meihui Zhang"}, "author": "Meihui Zhang", "arxiv_comment": "12 pages, 10 figures", "links": [{"href": "http://arxiv.org/abs/1904.03392v5", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1904.03392v5", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1904.03392v5", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1904.03392v5", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/2003.07182v1", "guidislink": true, "updated": "2020-03-12T23:31:11Z", "updated_parsed": [2020, 3, 12, 23, 31, 11, 3, 72, 0], "published": "2020-03-12T23:31:11Z", "published_parsed": [2020, 3, 12, 23, 31, 11, 3, 72, 0], "title": "Causal datasheet: An approximate guide to practically assess Bayesian\n  networks in the real world", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=400&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Causal datasheet: An approximate guide to practically assess Bayesian\n  networks in the real world"}, "summary": "In solving real-world problems like changing healthcare-seeking behaviors,\ndesigning interventions to improve downstream outcomes requires an\nunderstanding of the causal links within the system. Causal Bayesian Networks\n(BN) have been proposed as one such powerful method. In real-world\napplications, however, confidence in the results of BNs are often moderate at\nbest. This is due in part to the inability to validate against some ground\ntruth, as the DAG is not available. This is especially problematic if the\nlearned DAG conflicts with pre-existing domain doctrine. At the policy level,\none must justify insights generated by such analysis, preferably accompanying\nthem with uncertainty estimation. Here we propose a causal extension to the\ndatasheet concept proposed by Gebru et al (2018) to include approximate BN\nperformance expectations for any given dataset. To generate the results for a\nprototype Causal Datasheet, we constructed over 30,000 synthetic datasets with\nproperties mirroring characteristics of real data. We then recorded the results\ngiven by state-of-the-art structure learning algorithms. These results were\nused to populate the Causal Datasheet, and recommendations were automatically\ngenerated dependent on expected performance. As a proof of concept, we used our\nCausal Datasheet Generation Tool (CDG-T) to assign expected performance\nexpectations to a maternal health survey we conducted in Uttar Pradesh, India.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=400&max_results=10&sortBy=relevance&sortOrder=descending", "value": "In solving real-world problems like changing healthcare-seeking behaviors,\ndesigning interventions to improve downstream outcomes requires an\nunderstanding of the causal links within the system. Causal Bayesian Networks\n(BN) have been proposed as one such powerful method. In real-world\napplications, however, confidence in the results of BNs are often moderate at\nbest. This is due in part to the inability to validate against some ground\ntruth, as the DAG is not available. This is especially problematic if the\nlearned DAG conflicts with pre-existing domain doctrine. At the policy level,\none must justify insights generated by such analysis, preferably accompanying\nthem with uncertainty estimation. Here we propose a causal extension to the\ndatasheet concept proposed by Gebru et al (2018) to include approximate BN\nperformance expectations for any given dataset. To generate the results for a\nprototype Causal Datasheet, we constructed over 30,000 synthetic datasets with\nproperties mirroring characteristics of real data. We then recorded the results\ngiven by state-of-the-art structure learning algorithms. These results were\nused to populate the Causal Datasheet, and recommendations were automatically\ngenerated dependent on expected performance. As a proof of concept, we used our\nCausal Datasheet Generation Tool (CDG-T) to assign expected performance\nexpectations to a maternal health survey we conducted in Uttar Pradesh, India."}, "authors": ["Bradley Butcher", "Vincent S. Huang", "Jeremy Reffin", "Sema K. Sgaier", "Grace Charles", "Novi Quadrianto"], "author_detail": {"name": "Novi Quadrianto"}, "author": "Novi Quadrianto", "links": [{"href": "http://arxiv.org/abs/2003.07182v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2003.07182v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.PF", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.ML", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2003.07182v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2003.07182v1", "arxiv_comment": null, "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/2008.03414v1", "guidislink": true, "updated": "2020-08-08T01:51:08Z", "updated_parsed": [2020, 8, 8, 1, 51, 8, 5, 221, 0], "published": "2020-08-08T01:51:08Z", "published_parsed": [2020, 8, 8, 1, 51, 8, 5, 221, 0], "title": "Using UNet and PSPNet to explore the reusability principle of CNN\n  parameters", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=400&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Using UNet and PSPNet to explore the reusability principle of CNN\n  parameters"}, "summary": "How to reduce the requirement on training dataset size is a hot topic in deep\nlearning community. One straightforward way is to reuse some pre-trained\nparameters. Some previous work like Deep transfer learning reuse the model\nparameters trained for the first task as the starting point for the second\ntask, and semi-supervised learning is trained upon a combination of labeled and\nunlabeled data. However, the fundamental reason of the success of these methods\nis unclear. In this paper, the reusability of parameters in each layer of a\ndeep convolutional neural network is experimentally quantified by using a\nnetwork to do segmentation and auto-encoder task. This paper proves that\nnetwork parameters can be reused for two reasons: first, the network features\nare general; Second, there is little difference between the pre-trained\nparameters and the ideal network parameters. Through the use of parameter\nreplacement and comparison, we demonstrate that reusability is different in\nBN(Batch Normalization)[7] layer and Convolution layer and some observations:\n(1)Running mean and running variance plays an important role than Weight and\nBias in BN layer.(2)The weight and bias can be reused in BN layers.( 3) The\nnetwork is very sensitive to the weight of convolutional layer.(4) The bias in\nConvolution layers are not sensitive, and it can be reused directly.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=400&max_results=10&sortBy=relevance&sortOrder=descending", "value": "How to reduce the requirement on training dataset size is a hot topic in deep\nlearning community. One straightforward way is to reuse some pre-trained\nparameters. Some previous work like Deep transfer learning reuse the model\nparameters trained for the first task as the starting point for the second\ntask, and semi-supervised learning is trained upon a combination of labeled and\nunlabeled data. However, the fundamental reason of the success of these methods\nis unclear. In this paper, the reusability of parameters in each layer of a\ndeep convolutional neural network is experimentally quantified by using a\nnetwork to do segmentation and auto-encoder task. This paper proves that\nnetwork parameters can be reused for two reasons: first, the network features\nare general; Second, there is little difference between the pre-trained\nparameters and the ideal network parameters. Through the use of parameter\nreplacement and comparison, we demonstrate that reusability is different in\nBN(Batch Normalization)[7] layer and Convolution layer and some observations:\n(1)Running mean and running variance plays an important role than Weight and\nBias in BN layer.(2)The weight and bias can be reused in BN layers.( 3) The\nnetwork is very sensitive to the weight of convolutional layer.(4) The bias in\nConvolution layers are not sensitive, and it can be reused directly."}, "authors": ["Wei Wang"], "author_detail": {"name": "Wei Wang"}, "author": "Wei Wang", "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2008.03411", "links": [{"href": "http://arxiv.org/abs/2008.03414v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2008.03414v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "eess.IV", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2008.03414v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2008.03414v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/2101.08467v1", "guidislink": true, "updated": "2021-01-21T07:07:00Z", "updated_parsed": [2021, 1, 21, 7, 7, 0, 3, 21, 0], "published": "2021-01-21T07:07:00Z", "published_parsed": [2021, 1, 21, 7, 7, 0, 3, 21, 0], "title": "CM-NAS: Rethinking Cross-Modality Neural Architectures for\n  Visible-Infrared Person Re-Identification", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=410&max_results=10&sortBy=relevance&sortOrder=descending", "value": "CM-NAS: Rethinking Cross-Modality Neural Architectures for\n  Visible-Infrared Person Re-Identification"}, "summary": "Visible-Infrared person re-identification (VI-ReID) aims at matching\ncross-modality pedestrian images, breaking through the limitation of\nsingle-modality person ReID in dark environment. In order to mitigate the\nimpact of large modality discrepancy, existing works manually design various\ntwo-stream architectures to separately learn modality-specific and\nmodality-sharable representations. Such a manual design routine, however,\nhighly depends on massive experiments and empirical practice, which is time\nconsuming and labor intensive. In this paper, we systematically study the\nmanually designed architectures, and identify that appropriately splitting\nBatch Normalization (BN) layers to learn modality-specific representations will\nbring a great boost towards cross-modality matching. Based on this observation,\nthe essential objective is to find the optimal splitting scheme for each BN\nlayer. To this end, we propose a novel method, named Cross-Modality Neural\nArchitecture Search (CM-NAS). It consists of a BN-oriented search space in\nwhich the standard optimization can be fulfilled subject to the cross-modality\ntask. Besides, in order to better guide the search process, we further\nformulate a new Correlation Consistency based Class-specific Maximum Mean\nDiscrepancy (C3MMD) loss. Apart from the modality discrepancy, it also concerns\nthe similarity correlations, which have been overlooked before, in the two\nmodalities. Resorting to these advantages, our method outperforms\nstate-of-the-art counterparts in extensive experiments, improving the\nRank-1/mAP by 6.70%/6.13% on SYSU-MM01 and 12.17%/11.23% on RegDB. The source\ncode will be released soon.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=410&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Visible-Infrared person re-identification (VI-ReID) aims at matching\ncross-modality pedestrian images, breaking through the limitation of\nsingle-modality person ReID in dark environment. In order to mitigate the\nimpact of large modality discrepancy, existing works manually design various\ntwo-stream architectures to separately learn modality-specific and\nmodality-sharable representations. Such a manual design routine, however,\nhighly depends on massive experiments and empirical practice, which is time\nconsuming and labor intensive. In this paper, we systematically study the\nmanually designed architectures, and identify that appropriately splitting\nBatch Normalization (BN) layers to learn modality-specific representations will\nbring a great boost towards cross-modality matching. Based on this observation,\nthe essential objective is to find the optimal splitting scheme for each BN\nlayer. To this end, we propose a novel method, named Cross-Modality Neural\nArchitecture Search (CM-NAS). It consists of a BN-oriented search space in\nwhich the standard optimization can be fulfilled subject to the cross-modality\ntask. Besides, in order to better guide the search process, we further\nformulate a new Correlation Consistency based Class-specific Maximum Mean\nDiscrepancy (C3MMD) loss. Apart from the modality discrepancy, it also concerns\nthe similarity correlations, which have been overlooked before, in the two\nmodalities. Resorting to these advantages, our method outperforms\nstate-of-the-art counterparts in extensive experiments, improving the\nRank-1/mAP by 6.70%/6.13% on SYSU-MM01 and 12.17%/11.23% on RegDB. The source\ncode will be released soon."}, "authors": ["Chaoyou Fu", "Yibo Hu", "Xiang Wu", "Hailin Shi", "Tao Mei", "Ran He"], "author_detail": {"name": "Ran He"}, "author": "Ran He", "links": [{"href": "http://arxiv.org/abs/2101.08467v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2101.08467v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2101.08467v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2101.08467v1", "arxiv_comment": null, "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1812.03049v1", "guidislink": true, "updated": "2018-12-07T14:53:03Z", "updated_parsed": [2018, 12, 7, 14, 53, 3, 4, 341, 0], "published": "2018-12-07T14:53:03Z", "published_parsed": [2018, 12, 7, 14, 53, 3, 4, 341, 0], "title": "On Batch Orthogonalization Layers", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=430&max_results=10&sortBy=relevance&sortOrder=descending", "value": "On Batch Orthogonalization Layers"}, "summary": "Batch normalization has become ubiquitous in many state-of-the-art nets. It\naccelerates training and yields good performance results. However, there are\nvarious other alternatives to normalization, e.g. orthonormalization. The\nobjective of this paper is to explore the possible alternatives to channel\nnormalization with orthonormalization layers. The performance of the algorithms\nare compared together with BN with prescribed performance measures.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=430&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Batch normalization has become ubiquitous in many state-of-the-art nets. It\naccelerates training and yields good performance results. However, there are\nvarious other alternatives to normalization, e.g. orthonormalization. The\nobjective of this paper is to explore the possible alternatives to channel\nnormalization with orthonormalization layers. The performance of the algorithms\nare compared together with BN with prescribed performance measures."}, "authors": ["Blanchette", "Laganire"], "author_detail": {"name": "Laganire"}, "author": "Laganire", "links": [{"href": "http://arxiv.org/abs/1812.03049v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1812.03049v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.ML", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1812.03049v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1812.03049v1", "arxiv_comment": null, "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1212.2500v1", "guidislink": true, "updated": "2012-10-19T15:07:12Z", "updated_parsed": [2012, 10, 19, 15, 7, 12, 4, 293, 0], "published": "2012-10-19T15:07:12Z", "published_parsed": [2012, 10, 19, 15, 7, 12, 4, 293, 0], "title": "On Local Optima in Learning Bayesian Networks", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=470&max_results=10&sortBy=relevance&sortOrder=descending", "value": "On Local Optima in Learning Bayesian Networks"}, "summary": "This paper proposes and evaluates the k-greedy equivalence search algorithm\n(KES) for learning Bayesian networks (BNs) from complete data. The main\ncharacteristic of KES is that it allows a trade-off between greediness and\nrandomness, thus exploring different good local optima. When greediness is set\nat maximum, KES corresponds to the greedy equivalence search algorithm (GES).\nWhen greediness is kept at minimum, we prove that under mild assumptions KES\nasymptotically returns any inclusion optimal BN with nonzero probability.\nExperimental results for both synthetic and real data are reported showing that\nKES often finds a better local optima than GES. Moreover, we use KES to\nexperimentally confirm that the number of different local optima is often huge.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=470&max_results=10&sortBy=relevance&sortOrder=descending", "value": "This paper proposes and evaluates the k-greedy equivalence search algorithm\n(KES) for learning Bayesian networks (BNs) from complete data. The main\ncharacteristic of KES is that it allows a trade-off between greediness and\nrandomness, thus exploring different good local optima. When greediness is set\nat maximum, KES corresponds to the greedy equivalence search algorithm (GES).\nWhen greediness is kept at minimum, we prove that under mild assumptions KES\nasymptotically returns any inclusion optimal BN with nonzero probability.\nExperimental results for both synthetic and real data are reported showing that\nKES often finds a better local optima than GES. Moreover, we use KES to\nexperimentally confirm that the number of different local optima is often huge."}, "authors": ["Jens D. Nielsen", "Tomas Kocka", "Jose M. Pena"], "author_detail": {"name": "Jose M. Pena"}, "author": "Jose M. Pena", "arxiv_comment": "Appears in Proceedings of the Nineteenth Conference on Uncertainty in\n  Artificial Intelligence (UAI2003)", "links": [{"href": "http://arxiv.org/abs/1212.2500v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1212.2500v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.ML", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1212.2500v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1212.2500v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1302.3552v1", "guidislink": true, "updated": "2013-02-13T14:11:37Z", "updated_parsed": [2013, 2, 13, 14, 11, 37, 2, 44, 0], "published": "2013-02-13T14:11:37Z", "published_parsed": [2013, 2, 13, 14, 11, 37, 2, 44, 0], "title": "A Structurally and Temporally Extended Bayesian Belief Network Model:\n  Definitions, Properties, and Modeling Techniques", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=470&max_results=10&sortBy=relevance&sortOrder=descending", "value": "A Structurally and Temporally Extended Bayesian Belief Network Model:\n  Definitions, Properties, and Modeling Techniques"}, "summary": "We developed the language of Modifiable Temporal Belief Networks (MTBNs) as a\nstructural and temporal extension of Bayesian Belief Networks (BNs) to\nfacilitate normative temporal and causal modeling under uncertainty. In this\npaper we present definitions of the model, its components, and its fundamental\nproperties. We also discuss how to represent various types of temporal\nknowledge, with an emphasis on hybrid temporal-explicit time modeling, dynamic\nstructures, avoiding causal temporal inconsistencies, and dealing with models\nthat involve simultaneously actions (decisions) and causal and non-causal\nassociations. We examine the relationships among BNs, Modifiable Belief\nNetworks, and MTBNs with a single temporal granularity, and suggest areas of\napplication suitable to each one of them.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=470&max_results=10&sortBy=relevance&sortOrder=descending", "value": "We developed the language of Modifiable Temporal Belief Networks (MTBNs) as a\nstructural and temporal extension of Bayesian Belief Networks (BNs) to\nfacilitate normative temporal and causal modeling under uncertainty. In this\npaper we present definitions of the model, its components, and its fundamental\nproperties. We also discuss how to represent various types of temporal\nknowledge, with an emphasis on hybrid temporal-explicit time modeling, dynamic\nstructures, avoiding causal temporal inconsistencies, and dealing with models\nthat involve simultaneously actions (decisions) and causal and non-causal\nassociations. We examine the relationships among BNs, Modifiable Belief\nNetworks, and MTBNs with a single temporal granularity, and suggest areas of\napplication suitable to each one of them."}, "authors": ["Constantin F. Aliferis", "Gregory F. Cooper"], "author_detail": {"name": "Gregory F. Cooper"}, "author": "Gregory F. Cooper", "arxiv_comment": "Appears in Proceedings of the Twelfth Conference on Uncertainty in\n  Artificial Intelligence (UAI1996)", "links": [{"href": "http://arxiv.org/abs/1302.3552v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1302.3552v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1302.3552v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1302.3552v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1304.1097v1", "guidislink": true, "updated": "2013-03-27T13:55:59Z", "updated_parsed": [2013, 3, 27, 13, 55, 59, 2, 86, 0], "published": "2013-03-27T13:55:59Z", "published_parsed": [2013, 3, 27, 13, 55, 59, 2, 86, 0], "title": "A Randomized Approximation Algorithm of Logic Sampling", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=470&max_results=10&sortBy=relevance&sortOrder=descending", "value": "A Randomized Approximation Algorithm of Logic Sampling"}, "summary": "In recent years, researchers in decision analysis and artificial intelligence\n(AI) have used Bayesian belief networks to build models of expert opinion.\nUsing standard methods drawn from the theory of computational complexity,\nworkers in the field have shown that the problem of exact probabilistic\ninference on belief networks almost certainly requires exponential computation\nin the worst ease [3]. We have previously described a randomized approximation\nscheme, called BN-RAS, for computation on belief networks [ 1, 2, 4]. We gave\nprecise analytic bounds on the convergence of BN-RAS and showed how to trade\nrunning time for accuracy in the evaluation of posterior marginal\nprobabilities. We now extend our previous results and demonstrate the\ngenerality of our framework by applying similar mathematical techniques to the\nanalysis of convergence for logic sampling [7], an alternative simulation\nalgorithm for probabilistic inference.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=470&max_results=10&sortBy=relevance&sortOrder=descending", "value": "In recent years, researchers in decision analysis and artificial intelligence\n(AI) have used Bayesian belief networks to build models of expert opinion.\nUsing standard methods drawn from the theory of computational complexity,\nworkers in the field have shown that the problem of exact probabilistic\ninference on belief networks almost certainly requires exponential computation\nin the worst ease [3]. We have previously described a randomized approximation\nscheme, called BN-RAS, for computation on belief networks [ 1, 2, 4]. We gave\nprecise analytic bounds on the convergence of BN-RAS and showed how to trade\nrunning time for accuracy in the evaluation of posterior marginal\nprobabilities. We now extend our previous results and demonstrate the\ngenerality of our framework by applying similar mathematical techniques to the\nanalysis of convergence for logic sampling [7], an alternative simulation\nalgorithm for probabilistic inference."}, "authors": ["R. Martin Chavez", "Gregory F. Cooper"], "author_detail": {"name": "Gregory F. Cooper"}, "author": "Gregory F. Cooper", "arxiv_comment": "Appears in Proceedings of the Sixth Conference on Uncertainty in\n  Artificial Intelligence (UAI1990)", "links": [{"href": "http://arxiv.org/abs/1304.1097v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1304.1097v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1304.1097v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1304.1097v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1306.4890v1", "guidislink": true, "updated": "2013-06-20T14:35:22Z", "updated_parsed": [2013, 6, 20, 14, 35, 22, 3, 171, 0], "published": "2013-06-20T14:35:22Z", "published_parsed": [2013, 6, 20, 14, 35, 22, 3, 171, 0], "title": "Key Phrase Extraction of Lightly Filtered Broadcast News", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=480&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Key Phrase Extraction of Lightly Filtered Broadcast News"}, "summary": "This paper explores the impact of light filtering on automatic key phrase\nextraction (AKE) applied to Broadcast News (BN). Key phrases are words and\nexpressions that best characterize the content of a document. Key phrases are\noften used to index the document or as features in further processing. This\nmakes improvements in AKE accuracy particularly important. We hypothesized that\nfiltering out marginally relevant sentences from a document would improve AKE\naccuracy. Our experiments confirmed this hypothesis. Elimination of as little\nas 10% of the document sentences lead to a 2% improvement in AKE precision and\nrecall. AKE is built over MAUI toolkit that follows a supervised learning\napproach. We trained and tested our AKE method on a gold standard made of 8 BN\nprograms containing 110 manually annotated news stories. The experiments were\nconducted within a Multimedia Monitoring Solution (MMS) system for TV and radio\nnews/programs, running daily, and monitoring 12 TV and 4 radio channels.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=480&max_results=10&sortBy=relevance&sortOrder=descending", "value": "This paper explores the impact of light filtering on automatic key phrase\nextraction (AKE) applied to Broadcast News (BN). Key phrases are words and\nexpressions that best characterize the content of a document. Key phrases are\noften used to index the document or as features in further processing. This\nmakes improvements in AKE accuracy particularly important. We hypothesized that\nfiltering out marginally relevant sentences from a document would improve AKE\naccuracy. Our experiments confirmed this hypothesis. Elimination of as little\nas 10% of the document sentences lead to a 2% improvement in AKE precision and\nrecall. AKE is built over MAUI toolkit that follows a supervised learning\napproach. We trained and tested our AKE method on a gold standard made of 8 BN\nprograms containing 110 manually annotated news stories. The experiments were\nconducted within a Multimedia Monitoring Solution (MMS) system for TV and radio\nnews/programs, running daily, and monitoring 12 TV and 4 radio channels."}, "authors": ["Luis Marujo", "Ricardo Ribeiro", "David Martins de Matos", "Joo P. Neto", "Anatole Gershman", "Jaime Carbonell"], "author_detail": {"name": "Jaime Carbonell"}, "author": "Jaime Carbonell", "arxiv_comment": "In 15th International Conference on Text, Speech and Dialogue (TSD\n  2012)", "links": [{"href": "http://arxiv.org/abs/1306.4890v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1306.4890v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.IR", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1306.4890v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1306.4890v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1411.6651v1", "guidislink": true, "updated": "2014-11-24T21:27:37Z", "updated_parsed": [2014, 11, 24, 21, 27, 37, 0, 328, 0], "published": "2014-11-24T21:27:37Z", "published_parsed": [2014, 11, 24, 21, 27, 37, 0, 328, 0], "title": "A Greedy, Flexible Algorithm to Learn an Optimal Bayesian Network\n  Structure", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=480&max_results=10&sortBy=relevance&sortOrder=descending", "value": "A Greedy, Flexible Algorithm to Learn an Optimal Bayesian Network\n  Structure"}, "summary": "In this report paper we first present a report of the Advanced Machine\nLearning Course Project on the provided data set and then present a novel\nheuristic algorithm for exact Bayesian network (BN) structure discovery that\nuses decomposable scoring functions. Our algorithm follows a different approach\nto solve the problem of BN structure discovery than the previously used methods\nsuch as Dynamic Programming (DP) and Branch and Bound to reduce the search\nspace and find the global optima space for the problem. The algorithm we\npropose has some degree of flexibility that can make it more or less greedy.\nThe more the algorithm is set to be greedy, the more the speed of the algorithm\nwill be, and the less optimal the final structure. Our algorithm runs in a much\nless time than the previously known methods and guarantees to have an\noptimality of close to 99%. Therefore, it sacrifices less than one percent of\nscore of an optimal structure in order to gain a much lower running time and\nmake the algorithm feasible for large data sets (we may note that we never used\nany toolbox except for result validation)", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=480&max_results=10&sortBy=relevance&sortOrder=descending", "value": "In this report paper we first present a report of the Advanced Machine\nLearning Course Project on the provided data set and then present a novel\nheuristic algorithm for exact Bayesian network (BN) structure discovery that\nuses decomposable scoring functions. Our algorithm follows a different approach\nto solve the problem of BN structure discovery than the previously used methods\nsuch as Dynamic Programming (DP) and Branch and Bound to reduce the search\nspace and find the global optima space for the problem. The algorithm we\npropose has some degree of flexibility that can make it more or less greedy.\nThe more the algorithm is set to be greedy, the more the speed of the algorithm\nwill be, and the less optimal the final structure. Our algorithm runs in a much\nless time than the previously known methods and guarantees to have an\noptimality of close to 99%. Therefore, it sacrifices less than one percent of\nscore of an optimal structure in order to gain a much lower running time and\nmake the algorithm feasible for large data sets (we may note that we never used\nany toolbox except for result validation)"}, "authors": ["Amir Arsalan Soltani"], "author_detail": {"name": "Amir Arsalan Soltani"}, "author": "Amir Arsalan Soltani", "arxiv_comment": "This was my Advanced Machine Learning's course project in Spring 2014", "links": [{"href": "http://arxiv.org/abs/1411.6651v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1411.6651v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.ML", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1411.6651v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1411.6651v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1610.06160v1", "guidislink": true, "updated": "2016-10-19T19:34:48Z", "updated_parsed": [2016, 10, 19, 19, 34, 48, 2, 293, 0], "published": "2016-10-19T19:34:48Z", "published_parsed": [2016, 10, 19, 19, 34, 48, 2, 293, 0], "title": "Streaming Normalization: Towards Simpler and More Biologically-plausible\n  Normalizations for Online and Recurrent Learning", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=490&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Streaming Normalization: Towards Simpler and More Biologically-plausible\n  Normalizations for Online and Recurrent Learning"}, "summary": "We systematically explored a spectrum of normalization algorithms related to\nBatch Normalization (BN) and propose a generalized formulation that\nsimultaneously solves two major limitations of BN: (1) online learning and (2)\nrecurrent learning. Our proposal is simpler and more biologically-plausible.\nUnlike previous approaches, our technique can be applied out of the box to all\nlearning scenarios (e.g., online learning, batch learning, fully-connected,\nconvolutional, feedforward, recurrent and mixed --- recurrent and\nconvolutional) and compare favorably with existing approaches. We also propose\nLp Normalization for normalizing by different orders of statistical moments. In\nparticular, L1 normalization is well-performing, simple to implement, fast to\ncompute, more biologically-plausible and thus ideal for GPU or hardware\nimplementations.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=490&max_results=10&sortBy=relevance&sortOrder=descending", "value": "We systematically explored a spectrum of normalization algorithms related to\nBatch Normalization (BN) and propose a generalized formulation that\nsimultaneously solves two major limitations of BN: (1) online learning and (2)\nrecurrent learning. Our proposal is simpler and more biologically-plausible.\nUnlike previous approaches, our technique can be applied out of the box to all\nlearning scenarios (e.g., online learning, batch learning, fully-connected,\nconvolutional, feedforward, recurrent and mixed --- recurrent and\nconvolutional) and compare favorably with existing approaches. We also propose\nLp Normalization for normalizing by different orders of statistical moments. In\nparticular, L1 normalization is well-performing, simple to implement, fast to\ncompute, more biologically-plausible and thus ideal for GPU or hardware\nimplementations."}, "authors": ["Qianli Liao", "Kenji Kawaguchi", "Tomaso Poggio"], "author_detail": {"name": "Tomaso Poggio"}, "author": "Tomaso Poggio", "links": [{"href": "http://arxiv.org/abs/1610.06160v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1610.06160v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.NE", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1610.06160v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1610.06160v1", "arxiv_comment": null, "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1610.09799v1", "guidislink": true, "updated": "2016-10-31T06:13:31Z", "updated_parsed": [2016, 10, 31, 6, 13, 31, 0, 305, 0], "published": "2016-10-31T06:13:31Z", "published_parsed": [2016, 10, 31, 6, 13, 31, 0, 305, 0], "title": "Experiments with POS Tagging Code-mixed Indian Social Media Text", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=490&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Experiments with POS Tagging Code-mixed Indian Social Media Text"}, "summary": "This paper presents Centre for Development of Advanced Computing Mumbai's\n(CDACM) submission to the NLP Tools Contest on Part-Of-Speech (POS) Tagging For\nCode-mixed Indian Social Media Text (POSCMISMT) 2015 (collocated with ICON\n2015). We submitted results for Hindi (hi), Bengali (bn), and Telugu (te)\nlanguages mixed with English (en). In this paper, we have described our\napproaches to the POS tagging techniques, we exploited for this task. Machine\nlearning has been used to POS tag the mixed language text. For POS tagging,\ndistributed representations of words in vector space (word2vec) for feature\nextraction and Log-linear models have been tried. We report our work on all\nthree languages hi, bn, and te mixed with en.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=490&max_results=10&sortBy=relevance&sortOrder=descending", "value": "This paper presents Centre for Development of Advanced Computing Mumbai's\n(CDACM) submission to the NLP Tools Contest on Part-Of-Speech (POS) Tagging For\nCode-mixed Indian Social Media Text (POSCMISMT) 2015 (collocated with ICON\n2015). We submitted results for Hindi (hi), Bengali (bn), and Telugu (te)\nlanguages mixed with English (en). In this paper, we have described our\napproaches to the POS tagging techniques, we exploited for this task. Machine\nlearning has been used to POS tag the mixed language text. For POS tagging,\ndistributed representations of words in vector space (word2vec) for feature\nextraction and Log-linear models have been tried. We report our work on all\nthree languages hi, bn, and te mixed with en."}, "authors": ["Prakash B. Pimpale", "Raj Nath Patel"], "author_detail": {"name": "Raj Nath Patel"}, "author": "Raj Nath Patel", "arxiv_comment": "3 Pages, Published in the Proceedings of the Tool Contest on POS\n  Tagging for Code-mixed Indian Social Media (Facebook, Twitter, and Whatsapp)\n  Text", "links": [{"href": "http://arxiv.org/abs/1610.09799v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1610.09799v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1610.09799v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1610.09799v1", "journal_reference": "In the Proceedings of the 12th International Conference on Natural\n  Language Processing (ICON 2015)", "doi": null}
{"id": "http://arxiv.org/abs/1703.03038v1", "guidislink": true, "updated": "2017-03-08T21:29:52Z", "updated_parsed": [2017, 3, 8, 21, 29, 52, 2, 67, 0], "published": "2017-03-08T21:29:52Z", "published_parsed": [2017, 3, 8, 21, 29, 52, 2, 67, 0], "title": "Parallel Implementation of Efficient Search Schemes for the Inference of\n  Cancer Progression Models", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=490&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Parallel Implementation of Efficient Search Schemes for the Inference of\n  Cancer Progression Models"}, "summary": "The emergence and development of cancer is a consequence of the accumulation\nover time of genomic mutations involving a specific set of genes, which\nprovides the cancer clones with a functional selective advantage. In this work,\nwe model the order of accumulation of such mutations during the progression,\nwhich eventually leads to the disease, by means of probabilistic graphic\nmodels, i.e., Bayesian Networks (BNs). We investigate how to perform the task\nof learning the structure of such BNs, according to experimental evidence,\nadopting a global optimization meta-heuristics. In particular, in this work we\nrely on Genetic Algorithms, and to strongly reduce the execution time of the\ninference -- which can also involve multiple repetitions to collect\nstatistically significant assessments of the data -- we distribute the\ncalculations using both multi-threading and a multi-node architecture. The\nresults show that our approach is characterized by good accuracy and\nspecificity; we also demonstrate its feasibility, thanks to a 84x reduction of\nthe overall execution time with respect to a traditional sequential\nimplementation.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=490&max_results=10&sortBy=relevance&sortOrder=descending", "value": "The emergence and development of cancer is a consequence of the accumulation\nover time of genomic mutations involving a specific set of genes, which\nprovides the cancer clones with a functional selective advantage. In this work,\nwe model the order of accumulation of such mutations during the progression,\nwhich eventually leads to the disease, by means of probabilistic graphic\nmodels, i.e., Bayesian Networks (BNs). We investigate how to perform the task\nof learning the structure of such BNs, according to experimental evidence,\nadopting a global optimization meta-heuristics. In particular, in this work we\nrely on Genetic Algorithms, and to strongly reduce the execution time of the\ninference -- which can also involve multiple repetitions to collect\nstatistically significant assessments of the data -- we distribute the\ncalculations using both multi-threading and a multi-node architecture. The\nresults show that our approach is characterized by good accuracy and\nspecificity; we also demonstrate its feasibility, thanks to a 84x reduction of\nthe overall execution time with respect to a traditional sequential\nimplementation."}, "authors": ["Daniele Ramazzotti", "Marco S. Nobile", "Paolo Cazzaniga", "Giancarlo Mauri", "Marco Antoniotti"], "author_detail": {"name": "Marco Antoniotti"}, "author": "Marco Antoniotti", "links": [{"title": "doi", "href": "http://dx.doi.org/10.1109/CIBCB.2016.7758109", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/1703.03038v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1703.03038v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.ML", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1703.03038v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1703.03038v1", "arxiv_comment": null, "journal_reference": null, "doi": "10.1109/CIBCB.2016.7758109"}
{"id": "http://arxiv.org/abs/1708.04317v1", "guidislink": true, "updated": "2017-08-14T20:47:35Z", "updated_parsed": [2017, 8, 14, 20, 47, 35, 0, 226, 0], "published": "2017-08-14T20:47:35Z", "published_parsed": [2017, 8, 14, 20, 47, 35, 0, 226, 0], "title": "An ELU Network with Total Variation for Image Denoising", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=490&max_results=10&sortBy=relevance&sortOrder=descending", "value": "An ELU Network with Total Variation for Image Denoising"}, "summary": "In this paper, we propose a novel convolutional neural network (CNN) for\nimage denoising, which uses exponential linear unit (ELU) as the activation\nfunction. We investigate the suitability by analyzing ELU's connection with\ntrainable nonlinear reaction diffusion model (TNRD) and residual denoising. On\nthe other hand, batch normalization (BN) is indispensable for residual\ndenoising and convergence purpose. However, direct stacking of BN and ELU\ndegrades the performance of CNN. To mitigate this issue, we design an\ninnovative combination of activation layer and normalization layer to exploit\nand leverage the ELU network, and discuss the corresponding rationale.\nMoreover, inspired by the fact that minimizing total variation (TV) can be\napplied to image denoising, we propose a TV regularized L2 loss to evaluate the\ntraining effect during the iterations. Finally, we conduct extensive\nexperiments, showing that our model outperforms some recent and popular\napproaches on Gaussian denoising with specific or randomized noise levels for\nboth gray and color images.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=490&max_results=10&sortBy=relevance&sortOrder=descending", "value": "In this paper, we propose a novel convolutional neural network (CNN) for\nimage denoising, which uses exponential linear unit (ELU) as the activation\nfunction. We investigate the suitability by analyzing ELU's connection with\ntrainable nonlinear reaction diffusion model (TNRD) and residual denoising. On\nthe other hand, batch normalization (BN) is indispensable for residual\ndenoising and convergence purpose. However, direct stacking of BN and ELU\ndegrades the performance of CNN. To mitigate this issue, we design an\ninnovative combination of activation layer and normalization layer to exploit\nand leverage the ELU network, and discuss the corresponding rationale.\nMoreover, inspired by the fact that minimizing total variation (TV) can be\napplied to image denoising, we propose a TV regularized L2 loss to evaluate the\ntraining effect during the iterations. Finally, we conduct extensive\nexperiments, showing that our model outperforms some recent and popular\napproaches on Gaussian denoising with specific or randomized noise levels for\nboth gray and color images."}, "authors": ["Tianyang Wang", "Zhengrui Qin", "Michelle Zhu"], "author_detail": {"name": "Michelle Zhu"}, "author": "Michelle Zhu", "arxiv_comment": "10 pages, Accepted by the 24th International Conference on Neural\n  Information Processing (2017)", "links": [{"href": "http://arxiv.org/abs/1708.04317v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1708.04317v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1708.04317v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1708.04317v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1611.10187v1", "guidislink": true, "updated": "2016-11-30T14:50:59Z", "updated_parsed": [2016, 11, 30, 14, 50, 59, 2, 335, 0], "published": "2016-11-30T14:50:59Z", "published_parsed": [2016, 11, 30, 14, 50, 59, 2, 335, 0], "title": "A Bayesian Network Approach to Assess and Predict Software Quality Using\n  Activity-Based Quality Models (Conference Version)", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=490&max_results=10&sortBy=relevance&sortOrder=descending", "value": "A Bayesian Network Approach to Assess and Predict Software Quality Using\n  Activity-Based Quality Models (Conference Version)"}, "summary": "Assessing and predicting the complex concept of software quality is still\nchallenging in practice as well as research. Activity-based quality models\nbreak down this complex con- cept into more concrete definitions, more\nprecisely facts about the system, process and environment and their impact on\nac- tivities performed on and with the system. However, these models lack an\noperationalisation that allows to use them in assessment and prediction of\nquality. Bayesian Networks (BN) have been shown to be a viable means for\nassessment and prediction incorporating variables with uncertainty. This paper\ndescribes how activity-based quality models can be used to derive BN models for\nquality assessment and pre- diction. The proposed approach is demonstrated in a\nproof of concept using publicly available data.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=490&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Assessing and predicting the complex concept of software quality is still\nchallenging in practice as well as research. Activity-based quality models\nbreak down this complex con- cept into more concrete definitions, more\nprecisely facts about the system, process and environment and their impact on\nac- tivities performed on and with the system. However, these models lack an\noperationalisation that allows to use them in assessment and prediction of\nquality. Bayesian Networks (BN) have been shown to be a viable means for\nassessment and prediction incorporating variables with uncertainty. This paper\ndescribes how activity-based quality models can be used to derive BN models for\nquality assessment and pre- diction. The proposed approach is demonstrated in a\nproof of concept using publicly available data."}, "authors": ["Stefan Wagner"], "author_detail": {"name": "Stefan Wagner"}, "author": "Stefan Wagner", "links": [{"title": "doi", "href": "http://dx.doi.org/10.1145/1540438.1540447", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/1611.10187v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1611.10187v1", "rel": "related", "type": "application/pdf"}], "arxiv_comment": "9 pages, 3 figures", "arxiv_primary_category": {"term": "cs.SE", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.SE", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "D.2.8; D.2.9", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1611.10187v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1611.10187v1", "journal_reference": "Proceedings of the 5th International Conference on Predictor\n  Models in Software Engineering (PROMISE '09). ACM, 2009", "doi": "10.1145/1540438.1540447"}
{"id": "http://arxiv.org/abs/1704.08676v2", "guidislink": true, "updated": "2018-08-03T20:16:37Z", "updated_parsed": [2018, 8, 3, 20, 16, 37, 4, 215, 0], "published": "2017-04-27T17:40:22Z", "published_parsed": [2017, 4, 27, 17, 40, 22, 3, 117, 0], "title": "Learning the structure of Bayesian Networks: A quantitative assessment\n  of the effect of different algorithmic schemes", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=500&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Learning the structure of Bayesian Networks: A quantitative assessment\n  of the effect of different algorithmic schemes"}, "summary": "One of the most challenging tasks when adopting Bayesian Networks (BNs) is\nthe one of learning their structure from data. This task is complicated by the\nhuge search space of possible solutions, and by the fact that the problem is\nNP-hard. Hence, full enumeration of all the possible solutions is not always\nfeasible and approximations are often required. However, to the best of our\nknowledge, a quantitative analysis of the performance and characteristics of\nthe different heuristics to solve this problem has never been done before.\n  For this reason, in this work, we provide a detailed comparison of many\ndifferent state-of-the-arts methods for structural learning on simulated data\nconsidering both BNs with discrete and continuous variables, and with different\nrates of noise in the data. In particular, we investigate the performance of\ndifferent widespread scores and algorithmic approaches proposed for the\ninference and the statistical pitfalls within them.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=500&max_results=10&sortBy=relevance&sortOrder=descending", "value": "One of the most challenging tasks when adopting Bayesian Networks (BNs) is\nthe one of learning their structure from data. This task is complicated by the\nhuge search space of possible solutions, and by the fact that the problem is\nNP-hard. Hence, full enumeration of all the possible solutions is not always\nfeasible and approximations are often required. However, to the best of our\nknowledge, a quantitative analysis of the performance and characteristics of\nthe different heuristics to solve this problem has never been done before.\n  For this reason, in this work, we provide a detailed comparison of many\ndifferent state-of-the-arts methods for structural learning on simulated data\nconsidering both BNs with discrete and continuous variables, and with different\nrates of noise in the data. In particular, we investigate the performance of\ndifferent widespread scores and algorithmic approaches proposed for the\ninference and the statistical pitfalls within them."}, "authors": ["Stefano Beretta", "Mauro Castelli", "Ivo Goncalves", "Roberto Henriques", "Daniele Ramazzotti"], "author_detail": {"name": "Daniele Ramazzotti"}, "author": "Daniele Ramazzotti", "links": [{"href": "http://arxiv.org/abs/1704.08676v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1704.08676v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.ML", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1704.08676v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1704.08676v2", "arxiv_comment": null, "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1801.00497v2", "guidislink": true, "updated": "2018-04-02T19:19:59Z", "updated_parsed": [2018, 4, 2, 19, 19, 59, 0, 92, 0], "published": "2018-01-01T19:21:44Z", "published_parsed": [2018, 1, 1, 19, 21, 44, 0, 1, 0], "title": "Implementing Bayesian Networks with Embedded Stochastic MRAM", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=500&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Implementing Bayesian Networks with Embedded Stochastic MRAM"}, "summary": "Magnetic tunnel junctions (MTJ's) with low barrier magnets have been used to\nimplement random number generators (RNG's) and it has recently been shown that\nsuch an MTJ connected to the drain of a conventional transistor provides a\nthree-terminal tunable RNG or a $p$-bit. In this letter we show how this\n$p$-bit can be used to build a $p$-circuit that emulates a Bayesian network\n(BN), such that the correlations in real world variables can be obtained from\nelectrical measurements on the corresponding circuit nodes. The $p$-circuit\ndesign proceeds in two steps: the BN is first translated into a behavioral\nmodel, called Probabilistic Spin Logic (PSL), defined by dimensionless biasing\n(h) and interconnection (J) coefficients, which are then translated into\nelectronic circuit elements. As a benchmark example, we mimic a family tree of\nthree generations and show that the genetic relatedness calculated from a\nSPICE-compatible circuit simulator matches well-known results.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=500&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Magnetic tunnel junctions (MTJ's) with low barrier magnets have been used to\nimplement random number generators (RNG's) and it has recently been shown that\nsuch an MTJ connected to the drain of a conventional transistor provides a\nthree-terminal tunable RNG or a $p$-bit. In this letter we show how this\n$p$-bit can be used to build a $p$-circuit that emulates a Bayesian network\n(BN), such that the correlations in real world variables can be obtained from\nelectrical measurements on the corresponding circuit nodes. The $p$-circuit\ndesign proceeds in two steps: the BN is first translated into a behavioral\nmodel, called Probabilistic Spin Logic (PSL), defined by dimensionless biasing\n(h) and interconnection (J) coefficients, which are then translated into\nelectronic circuit elements. As a benchmark example, we mimic a family tree of\nthree generations and show that the genetic relatedness calculated from a\nSPICE-compatible circuit simulator matches well-known results."}, "authors": ["Rafatul Faria", "Kerem Y. Camsari", "Supriyo Datta"], "author_detail": {"name": "Supriyo Datta"}, "author": "Supriyo Datta", "links": [{"title": "doi", "href": "http://dx.doi.org/10.1063/1.5021332", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/1801.00497v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1801.00497v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.ET", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.ET", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1801.00497v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1801.00497v2", "arxiv_comment": null, "journal_reference": "AIP Advances 8, 045101 (2018)", "doi": "10.1063/1.5021332"}
{"id": "http://arxiv.org/abs/1806.00882v2", "guidislink": true, "updated": "2020-02-24T19:08:21Z", "updated_parsed": [2020, 2, 24, 19, 8, 21, 0, 55, 0], "published": "2018-06-03T21:26:36Z", "published_parsed": [2018, 6, 3, 21, 26, 36, 6, 154, 0], "title": "Structural Learning of Multivariate Regression Chain Graphs via\n  Decomposition", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=510&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Structural Learning of Multivariate Regression Chain Graphs via\n  Decomposition"}, "summary": "We extend the decomposition approach for learning Bayesian networks (BNs)\nproposed by (Xie et. al.) to learning multivariate regression chain graphs (MVR\nCGs), which include BNs as a special case. The same advantages of this\ndecomposition approach hold in the more general setting: reduced complexity and\nincreased power of computational independence tests. Moreover, latent (hidden)\nvariables can be represented in MVR CGs by using bidirected edges, and our\nalgorithm correctly recovers any independence structure that is faithful to an\nMVR CG, thus greatly extending the range of applications of decomposition-based\nmodel selection techniques. Simulations under a variety of settings demonstrate\nthe competitive performance of our method in comparison with the PC-like\nalgorithm (Sonntag and Pena). In fact, the decomposition-based algorithm\nusually outperforms the PC-like algorithm except in running time. The\nperformance of both algorithms is much better when the underlying graph is\nsparse.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=510&max_results=10&sortBy=relevance&sortOrder=descending", "value": "We extend the decomposition approach for learning Bayesian networks (BNs)\nproposed by (Xie et. al.) to learning multivariate regression chain graphs (MVR\nCGs), which include BNs as a special case. The same advantages of this\ndecomposition approach hold in the more general setting: reduced complexity and\nincreased power of computational independence tests. Moreover, latent (hidden)\nvariables can be represented in MVR CGs by using bidirected edges, and our\nalgorithm correctly recovers any independence structure that is faithful to an\nMVR CG, thus greatly extending the range of applications of decomposition-based\nmodel selection techniques. Simulations under a variety of settings demonstrate\nthe competitive performance of our method in comparison with the PC-like\nalgorithm (Sonntag and Pena). In fact, the decomposition-based algorithm\nusually outperforms the PC-like algorithm except in running time. The\nperformance of both algorithms is much better when the underlying graph is\nsparse."}, "authors": ["Mohammad Ali Javidian", "Marco Valtorta"], "author_detail": {"name": "Marco Valtorta"}, "author": "Marco Valtorta", "arxiv_comment": "19 pages, 6 figures", "links": [{"href": "http://arxiv.org/abs/1806.00882v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1806.00882v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1806.00882v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1806.00882v2", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1810.00859v2", "guidislink": true, "updated": "2019-05-07T02:32:25Z", "updated_parsed": [2019, 5, 7, 2, 32, 25, 1, 127, 0], "published": "2018-10-01T17:55:43Z", "published_parsed": [2018, 10, 1, 17, 55, 43, 0, 274, 0], "title": "Dynamic Sparse Graph for Efficient Deep Learning", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=510&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Dynamic Sparse Graph for Efficient Deep Learning"}, "summary": "We propose to execute deep neural networks (DNNs) with dynamic and sparse\ngraph (DSG) structure for compressive memory and accelerative execution during\nboth training and inference. The great success of DNNs motivates the pursuing\nof lightweight models for the deployment onto embedded devices. However, most\nof the previous studies optimize for inference while neglect training or even\ncomplicate it. Training is far more intractable, since (i) the neurons dominate\nthe memory cost rather than the weights in inference; (ii) the dynamic\nactivation makes previous sparse acceleration via one-off optimization on fixed\nweight invalid; (iii) batch normalization (BN) is critical for maintaining\naccuracy while its activation reorganization damages the sparsity. To address\nthese issues, DSG activates only a small amount of neurons with high\nselectivity at each iteration via a dimension-reduction search (DRS) and\nobtains the BN compatibility via a double-mask selection (DMS). Experiments\nshow significant memory saving (1.7-4.5x) and operation reduction (2.3-4.4x)\nwith little accuracy loss on various benchmarks.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=510&max_results=10&sortBy=relevance&sortOrder=descending", "value": "We propose to execute deep neural networks (DNNs) with dynamic and sparse\ngraph (DSG) structure for compressive memory and accelerative execution during\nboth training and inference. The great success of DNNs motivates the pursuing\nof lightweight models for the deployment onto embedded devices. However, most\nof the previous studies optimize for inference while neglect training or even\ncomplicate it. Training is far more intractable, since (i) the neurons dominate\nthe memory cost rather than the weights in inference; (ii) the dynamic\nactivation makes previous sparse acceleration via one-off optimization on fixed\nweight invalid; (iii) batch normalization (BN) is critical for maintaining\naccuracy while its activation reorganization damages the sparsity. To address\nthese issues, DSG activates only a small amount of neurons with high\nselectivity at each iteration via a dimension-reduction search (DRS) and\nobtains the BN compatibility via a double-mask selection (DMS). Experiments\nshow significant memory saving (1.7-4.5x) and operation reduction (2.3-4.4x)\nwith little accuracy loss on various benchmarks."}, "authors": ["Liu Liu", "Lei Deng", "Xing Hu", "Maohua Zhu", "Guoqi Li", "Yufei Ding", "Yuan Xie"], "author_detail": {"name": "Yuan Xie"}, "author": "Yuan Xie", "arxiv_comment": "ICLR 2019", "links": [{"href": "http://arxiv.org/abs/1810.00859v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1810.00859v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.ML", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1810.00859v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1810.00859v2", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1812.03981v1", "guidislink": true, "updated": "2018-12-10T18:58:12Z", "updated_parsed": [2018, 12, 10, 18, 58, 12, 0, 344, 0], "published": "2018-12-10T18:58:12Z", "published_parsed": [2018, 12, 10, 18, 58, 12, 0, 344, 0], "title": "Theoretical Analysis of Auto Rate-Tuning by Batch Normalization", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=510&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Theoretical Analysis of Auto Rate-Tuning by Batch Normalization"}, "summary": "Batch Normalization (BN) has become a cornerstone of deep learning across\ndiverse architectures, appearing to help optimization as well as\ngeneralization. While the idea makes intuitive sense, theoretical analysis of\nits effectiveness has been lacking. Here theoretical support is provided for\none of its conjectured properties, namely, the ability to allow gradient\ndescent to succeed with less tuning of learning rates. It is shown that even if\nwe fix the learning rate of scale-invariant parameters (e.g., weights of each\nlayer with BN) to a constant (say, $0.3$), gradient descent still approaches a\nstationary point (i.e., a solution where gradient is zero) in the rate of\n$T^{-1/2}$ in $T$ iterations, asymptotically matching the best bound for\ngradient descent with well-tuned learning rates. A similar result with\nconvergence rate $T^{-1/4}$ is also shown for stochastic gradient descent.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=510&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Batch Normalization (BN) has become a cornerstone of deep learning across\ndiverse architectures, appearing to help optimization as well as\ngeneralization. While the idea makes intuitive sense, theoretical analysis of\nits effectiveness has been lacking. Here theoretical support is provided for\none of its conjectured properties, namely, the ability to allow gradient\ndescent to succeed with less tuning of learning rates. It is shown that even if\nwe fix the learning rate of scale-invariant parameters (e.g., weights of each\nlayer with BN) to a constant (say, $0.3$), gradient descent still approaches a\nstationary point (i.e., a solution where gradient is zero) in the rate of\n$T^{-1/2}$ in $T$ iterations, asymptotically matching the best bound for\ngradient descent with well-tuned learning rates. A similar result with\nconvergence rate $T^{-1/4}$ is also shown for stochastic gradient descent."}, "authors": ["Sanjeev Arora", "Zhiyuan Li", "Kaifeng Lyu"], "author_detail": {"name": "Kaifeng Lyu"}, "author": "Kaifeng Lyu", "arxiv_comment": "22 pages", "links": [{"href": "http://arxiv.org/abs/1812.03981v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1812.03981v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.ML", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1812.03981v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1812.03981v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1905.12666v3", "guidislink": true, "updated": "2020-09-02T10:01:12Z", "updated_parsed": [2020, 9, 2, 10, 1, 12, 2, 246, 0], "published": "2019-05-29T18:23:17Z", "published_parsed": [2019, 5, 29, 18, 23, 17, 2, 149, 0], "title": "Evaluating structure learning algorithms with a balanced scoring\n  function", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=510&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Evaluating structure learning algorithms with a balanced scoring\n  function"}, "summary": "Several structure learning algorithms have been proposed towards discovering\ncausal or Bayesian Network (BN) graphs. The validity of these algorithms tends\nto be evaluated by assessing the relationship between the learnt and the ground\ntruth graph. However, there is no agreed scoring metric to determine this\nrelationship. Moreover, this paper shows that some of the commonly used metrics\ntend to be biased in favour of graphs that minimise edges. While graphs that\nare less complex are desirable, some of the metrics favour underfitted graphs,\nthereby encouraging limited propagation of evidence. This paper proposes the\nBalanced Scoring Function (BSF) that eliminates this bias by adjusting the\nreward function based on the difficulty of discovering an edge, or no edge,\nproportional to their occurrence rate in the ground truth graph. The BSF score\ncan be used in conjunction with other traditional metrics to provide an\nalternative and unbiased assessment about the capability of a structure\nlearning algorithm in discovering causal or BN graphs.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=510&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Several structure learning algorithms have been proposed towards discovering\ncausal or Bayesian Network (BN) graphs. The validity of these algorithms tends\nto be evaluated by assessing the relationship between the learnt and the ground\ntruth graph. However, there is no agreed scoring metric to determine this\nrelationship. Moreover, this paper shows that some of the commonly used metrics\ntend to be biased in favour of graphs that minimise edges. While graphs that\nare less complex are desirable, some of the metrics favour underfitted graphs,\nthereby encouraging limited propagation of evidence. This paper proposes the\nBalanced Scoring Function (BSF) that eliminates this bias by adjusting the\nreward function based on the difficulty of discovering an edge, or no edge,\nproportional to their occurrence rate in the ground truth graph. The BSF score\ncan be used in conjunction with other traditional metrics to provide an\nalternative and unbiased assessment about the capability of a structure\nlearning algorithm in discovering causal or BN graphs."}, "authors": ["Anthony C. Constantinou"], "author_detail": {"name": "Anthony C. Constantinou"}, "author": "Anthony C. Constantinou", "links": [{"href": "http://arxiv.org/abs/1905.12666v3", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1905.12666v3", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.ML", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1905.12666v3", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1905.12666v3", "arxiv_comment": null, "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1907.04003v1", "guidislink": true, "updated": "2019-07-09T06:24:22Z", "updated_parsed": [2019, 7, 9, 6, 24, 22, 1, 190, 0], "published": "2019-07-09T06:24:22Z", "published_parsed": [2019, 7, 9, 6, 24, 22, 1, 190, 0], "title": "Mean Spectral Normalization of Deep Neural Networks for Embedded\n  Automation", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=510&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Mean Spectral Normalization of Deep Neural Networks for Embedded\n  Automation"}, "summary": "Deep Neural Networks (DNNs) have begun to thrive in the field of automation\nsystems, owing to the recent advancements in standardising various aspects such\nas architecture, optimization techniques, and regularization. In this paper, we\ntake a step towards a better understanding of Spectral Normalization (SN) and\nits potential for standardizing regularization of a wider range of Deep\nLearning models, following an empirical approach. We conduct several\nexperiments to study their training dynamics, in comparison with the ubiquitous\nBatch Normalization (BN) and show that SN increases the gradient sparsity and\ncontrols the gradient variance. Furthermore, we show that SN suffers from a\nphenomenon, we call the mean-drift effect, which mitigates its performance. We,\nthen, propose a weight reparameterization called as the Mean Spectral\nNormalization (MSN) to resolve the mean drift, thereby significantly improving\nthe network's performance. Our model performs ~16% faster as compared to BN in\npractice, and has fewer trainable parameters. We also show the performance of\nour MSN for small, medium, and large CNNs - 3-layer CNN, VGG7 and DenseNet-BC,\nrespectively - and unsupervised image generation tasks using Generative\nAdversarial Networks (GANs) to evaluate its applicability for a broad range of\nembedded automation tasks.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=510&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Deep Neural Networks (DNNs) have begun to thrive in the field of automation\nsystems, owing to the recent advancements in standardising various aspects such\nas architecture, optimization techniques, and regularization. In this paper, we\ntake a step towards a better understanding of Spectral Normalization (SN) and\nits potential for standardizing regularization of a wider range of Deep\nLearning models, following an empirical approach. We conduct several\nexperiments to study their training dynamics, in comparison with the ubiquitous\nBatch Normalization (BN) and show that SN increases the gradient sparsity and\ncontrols the gradient variance. Furthermore, we show that SN suffers from a\nphenomenon, we call the mean-drift effect, which mitigates its performance. We,\nthen, propose a weight reparameterization called as the Mean Spectral\nNormalization (MSN) to resolve the mean drift, thereby significantly improving\nthe network's performance. Our model performs ~16% faster as compared to BN in\npractice, and has fewer trainable parameters. We also show the performance of\nour MSN for small, medium, and large CNNs - 3-layer CNN, VGG7 and DenseNet-BC,\nrespectively - and unsupervised image generation tasks using Generative\nAdversarial Networks (GANs) to evaluate its applicability for a broad range of\nembedded automation tasks."}, "authors": ["Anand Krishnamoorthy Subramanian", "Nak Young Chong"], "author_detail": {"name": "Nak Young Chong"}, "author": "Nak Young Chong", "arxiv_comment": "8 pagesm IEEE CASE 2019", "links": [{"href": "http://arxiv.org/abs/1907.04003v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1907.04003v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.ML", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1907.04003v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1907.04003v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1910.01992v3", "guidislink": true, "updated": "2020-03-23T20:39:17Z", "updated_parsed": [2020, 3, 23, 20, 39, 17, 0, 83, 0], "published": "2019-10-04T15:31:48Z", "published_parsed": [2019, 10, 4, 15, 31, 48, 4, 277, 0], "title": "SNDCNN: Self-normalizing deep CNNs with scaled exponential linear units\n  for speech recognition", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=520&max_results=10&sortBy=relevance&sortOrder=descending", "value": "SNDCNN: Self-normalizing deep CNNs with scaled exponential linear units\n  for speech recognition"}, "summary": "Very deep CNNs achieve state-of-the-art results in both computer vision and\nspeech recognition, but are difficult to train. The most popular way to train\nvery deep CNNs is to use shortcut connections (SC) together with batch\nnormalization (BN). Inspired by Self- Normalizing Neural Networks, we propose\nthe self-normalizing deep CNN (SNDCNN) based acoustic model topology, by\nremoving the SC/BN and replacing the typical RELU activations with scaled\nexponential linear unit (SELU) in ResNet-50. SELU activations make the network\nself-normalizing and remove the need for both shortcut connections and batch\nnormalization. Compared to ResNet- 50, we can achieve the same or lower (up to\n4.5% relative) word error rate (WER) while boosting both training and inference\nspeed by 60%-80%. We also explore other model inference optimization schemes to\nfurther reduce latency for production use.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=520&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Very deep CNNs achieve state-of-the-art results in both computer vision and\nspeech recognition, but are difficult to train. The most popular way to train\nvery deep CNNs is to use shortcut connections (SC) together with batch\nnormalization (BN). Inspired by Self- Normalizing Neural Networks, we propose\nthe self-normalizing deep CNN (SNDCNN) based acoustic model topology, by\nremoving the SC/BN and replacing the typical RELU activations with scaled\nexponential linear unit (SELU) in ResNet-50. SELU activations make the network\nself-normalizing and remove the need for both shortcut connections and batch\nnormalization. Compared to ResNet- 50, we can achieve the same or lower (up to\n4.5% relative) word error rate (WER) while boosting both training and inference\nspeed by 60%-80%. We also explore other model inference optimization schemes to\nfurther reduce latency for production use."}, "authors": ["Zhen Huang", "Tim Ng", "Leo Liu", "Henry Mason", "Xiaodan Zhuang", "Daben Liu"], "author_detail": {"name": "Daben Liu"}, "author": "Daben Liu", "links": [{"href": "http://arxiv.org/abs/1910.01992v3", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1910.01992v3", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.SD", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "eess.AS", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.ML", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1910.01992v3", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1910.01992v3", "arxiv_comment": null, "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/2003.02599v2", "guidislink": true, "updated": "2020-03-06T10:33:23Z", "updated_parsed": [2020, 3, 6, 10, 33, 23, 4, 66, 0], "published": "2020-03-05T13:22:23Z", "published_parsed": [2020, 3, 5, 13, 22, 23, 3, 65, 0], "title": "An Incremental Explanation of Inference in Hybrid Bayesian Networks for\n  Increasing Model Trustworthiness and Supporting Clinical Decision Making", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=520&max_results=10&sortBy=relevance&sortOrder=descending", "value": "An Incremental Explanation of Inference in Hybrid Bayesian Networks for\n  Increasing Model Trustworthiness and Supporting Clinical Decision Making"}, "summary": "Various AI models are increasingly being considered as part of clinical\ndecision-support tools. However, the trustworthiness of such models is rarely\nconsidered. Clinicians are more likely to use a model if they can understand\nand trust its predictions. Key to this is if its underlying reasoning can be\nexplained. A Bayesian network (BN) model has the advantage that it is not a\nblack-box and its reasoning can be explained. In this paper, we propose an\nincremental explanation of inference that can be applied to hybrid BNs, i.e.\nthose that contain both discrete and continuous nodes. The key questions that\nwe answer are: (1) which important evidence supports or contradicts the\nprediction, and (2) through which intermediate variables does the information\nflow. The explanation is illustrated using a real clinical case study. A small\nevaluation study is also conducted.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=520&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Various AI models are increasingly being considered as part of clinical\ndecision-support tools. However, the trustworthiness of such models is rarely\nconsidered. Clinicians are more likely to use a model if they can understand\nand trust its predictions. Key to this is if its underlying reasoning can be\nexplained. A Bayesian network (BN) model has the advantage that it is not a\nblack-box and its reasoning can be explained. In this paper, we propose an\nincremental explanation of inference that can be applied to hybrid BNs, i.e.\nthose that contain both discrete and continuous nodes. The key questions that\nwe answer are: (1) which important evidence supports or contradicts the\nprediction, and (2) through which intermediate variables does the information\nflow. The explanation is illustrated using a real clinical case study. A small\nevaluation study is also conducted."}, "authors": ["Evangelia Kyrimi", "Somayyeh Mossadegh", "Nigel Tai", "William Marsh"], "author_detail": {"name": "William Marsh"}, "author": "William Marsh", "links": [{"href": "http://arxiv.org/abs/2003.02599v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2003.02599v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2003.02599v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2003.02599v2", "arxiv_comment": null, "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/2003.09314v1", "guidislink": true, "updated": "2020-03-20T15:01:48Z", "updated_parsed": [2020, 3, 20, 15, 1, 48, 4, 80, 0], "published": "2020-03-20T15:01:48Z", "published_parsed": [2020, 3, 20, 15, 1, 48, 4, 80, 0], "title": "New heuristics for burning graphs", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=520&max_results=10&sortBy=relevance&sortOrder=descending", "value": "New heuristics for burning graphs"}, "summary": "The concept of graph burning and burning number ($bn(G)$) of a graph G was\nintroduced recently [1]. Graph burning models the spread of contagion (fire) in\na graph in discrete time steps. $bn(G)$ is the minimum time needed to burn a\ngraph $G$.The problem is NP-complete. In this paper, we develop first\nheuristics to solve the problem in general (connected) graphs. In order to test\nthe performance of our algorithms, we applied them on some graph classes with\nknown burning number such as theta graphs, we tested our algorithms on DIMACS\nand BHOSLIB that are known benchmarks for NP-hard problems in graph theory. We\nalso improved the upper bound for burning number on general graphs in terms of\ntheir distance to cluster. Then we generated a data set of 2000 random graphs\nwith known distance to cluster and tested our heuristics on them.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=520&max_results=10&sortBy=relevance&sortOrder=descending", "value": "The concept of graph burning and burning number ($bn(G)$) of a graph G was\nintroduced recently [1]. Graph burning models the spread of contagion (fire) in\na graph in discrete time steps. $bn(G)$ is the minimum time needed to burn a\ngraph $G$.The problem is NP-complete. In this paper, we develop first\nheuristics to solve the problem in general (connected) graphs. In order to test\nthe performance of our algorithms, we applied them on some graph classes with\nknown burning number such as theta graphs, we tested our algorithms on DIMACS\nand BHOSLIB that are known benchmarks for NP-hard problems in graph theory. We\nalso improved the upper bound for burning number on general graphs in terms of\ntheir distance to cluster. Then we generated a data set of 2000 random graphs\nwith known distance to cluster and tested our heuristics on them."}, "authors": ["Zahra Rezai Farokh", "Maryam Tahmasbi", "Zahra Haj Rajab Ali Tehrani", "Yousof Buali"], "author_detail": {"name": "Yousof Buali"}, "author": "Yousof Buali", "arxiv_comment": "10 pages, 1 figure", "links": [{"href": "http://arxiv.org/abs/2003.09314v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2003.09314v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.DM", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.DM", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2003.09314v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2003.09314v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/2007.09236v1", "guidislink": true, "updated": "2020-07-17T21:11:01Z", "updated_parsed": [2020, 7, 17, 21, 11, 1, 4, 199, 0], "published": "2020-07-17T21:11:01Z", "published_parsed": [2020, 7, 17, 21, 11, 1, 4, 199, 0], "title": "User-Oriented Multi-Task Federated Deep Learning for Mobile Edge\n  Computing", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=530&max_results=10&sortBy=relevance&sortOrder=descending", "value": "User-Oriented Multi-Task Federated Deep Learning for Mobile Edge\n  Computing"}, "summary": "Federated Learning (FL) is a recent approach for collaboratively training\nMachine Learning models on mobile edge devices, without private user data\nleaving the devices. The popular FL algorithm, Federated Averaging (FedAvg),\nsuffers from poor convergence speed given non-iid user data. Furthermore, most\nexisting work on FedAvg measures central-model accuracy, but in many cases,\nsuch as user content-recommendation, improving individual User model Accuracy\n(UA) is the real objective. To address these issues, we propose a Multi-Task\nFederated Learning (MTFL) system, which converges faster than FedAvg by using\ndistributed Adam optimization (FedAdam), and benefits UA by introducing\npersonal, non-federated 'patch' Batch-Normalization (BN) layers into the model.\nTesting FedAdam on the MNIST and CIFAR10 datasets show that it converges faster\n(up to 5x) than FedAvg in non-iid scenarios, and experiments using MTFL on the\nCIFAR10 dataset show that MTFL significantly improves average UA over FedAvg,\nby up to 54%. We also analyse the affect that private BN patches have on the\nMTFL model during inference, and give evidence that MTFL strikes a better\nbalance between regularization and convergence in FL. Finally, we test the MTFL\nsystem on a mobile edge computing testbed, showing that MTFL's convergence and\nUA benefits outweigh its overhead.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=530&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Federated Learning (FL) is a recent approach for collaboratively training\nMachine Learning models on mobile edge devices, without private user data\nleaving the devices. The popular FL algorithm, Federated Averaging (FedAvg),\nsuffers from poor convergence speed given non-iid user data. Furthermore, most\nexisting work on FedAvg measures central-model accuracy, but in many cases,\nsuch as user content-recommendation, improving individual User model Accuracy\n(UA) is the real objective. To address these issues, we propose a Multi-Task\nFederated Learning (MTFL) system, which converges faster than FedAvg by using\ndistributed Adam optimization (FedAdam), and benefits UA by introducing\npersonal, non-federated 'patch' Batch-Normalization (BN) layers into the model.\nTesting FedAdam on the MNIST and CIFAR10 datasets show that it converges faster\n(up to 5x) than FedAvg in non-iid scenarios, and experiments using MTFL on the\nCIFAR10 dataset show that MTFL significantly improves average UA over FedAvg,\nby up to 54%. We also analyse the affect that private BN patches have on the\nMTFL model during inference, and give evidence that MTFL strikes a better\nbalance between regularization and convergence in FL. Finally, we test the MTFL\nsystem on a mobile edge computing testbed, showing that MTFL's convergence and\nUA benefits outweigh its overhead."}, "authors": ["Jed Mills", "Jia Hu", "Geyong Min"], "author_detail": {"name": "Geyong Min"}, "author": "Geyong Min", "links": [{"href": "http://arxiv.org/abs/2007.09236v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2007.09236v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.ML", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2007.09236v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2007.09236v1", "arxiv_comment": null, "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/2101.08430v1", "guidislink": true, "updated": "2021-01-21T04:10:04Z", "updated_parsed": [2021, 1, 21, 4, 10, 4, 3, 21, 0], "published": "2021-01-21T04:10:04Z", "published_parsed": [2021, 1, 21, 4, 10, 4, 3, 21, 0], "title": "Generative Zero-shot Network Quantization", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=530&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Generative Zero-shot Network Quantization"}, "summary": "Convolutional neural networks are able to learn realistic image priors from\nnumerous training samples in low-level image generation and restoration. We\nshow that, for high-level image recognition tasks, we can further reconstruct\n\"realistic\" images of each category by leveraging intrinsic Batch Normalization\n(BN) statistics without any training data. Inspired by the popular VAE/GAN\nmethods, we regard the zero-shot optimization process of synthetic images as\ngenerative modeling to match the distribution of BN statistics. The generated\nimages serve as a calibration set for the following zero-shot network\nquantizations. Our method meets the needs for quantizing models based on\nsensitive information, \\textit{e.g.,} due to privacy concerns, no data is\navailable. Extensive experiments on benchmark datasets show that, with the help\nof generated data, our approach consistently outperforms existing data-free\nquantization methods.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=530&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Convolutional neural networks are able to learn realistic image priors from\nnumerous training samples in low-level image generation and restoration. We\nshow that, for high-level image recognition tasks, we can further reconstruct\n\"realistic\" images of each category by leveraging intrinsic Batch Normalization\n(BN) statistics without any training data. Inspired by the popular VAE/GAN\nmethods, we regard the zero-shot optimization process of synthetic images as\ngenerative modeling to match the distribution of BN statistics. The generated\nimages serve as a calibration set for the following zero-shot network\nquantizations. Our method meets the needs for quantizing models based on\nsensitive information, \\textit{e.g.,} due to privacy concerns, no data is\navailable. Extensive experiments on benchmark datasets show that, with the help\nof generated data, our approach consistently outperforms existing data-free\nquantization methods."}, "authors": ["Xiangyu He", "Qinghao Hu", "Peisong Wang", "Jian Cheng"], "author_detail": {"name": "Jian Cheng"}, "author": "Jian Cheng", "arxiv_comment": "Technical report", "links": [{"href": "http://arxiv.org/abs/2101.08430v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2101.08430v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2101.08430v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2101.08430v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/2005.09020v2", "guidislink": true, "updated": "2020-09-11T13:12:00Z", "updated_parsed": [2020, 9, 11, 13, 12, 0, 4, 255, 0], "published": "2020-05-18T18:40:09Z", "published_parsed": [2020, 5, 18, 18, 40, 9, 0, 139, 0], "title": "Large-scale empirical validation of Bayesian Network structure learning\n  algorithms with noisy data", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=560&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Large-scale empirical validation of Bayesian Network structure learning\n  algorithms with noisy data"}, "summary": "Numerous Bayesian Network (BN) structure learning algorithms have been\nproposed in the literature over the past few decades. Each publication makes an\nempirical or theoretical case for the algorithm proposed in that publication\nand results across studies are often inconsistent in their claims about which\nalgorithm is 'best'. This is partly because there is no agreed evaluation\napproach to determine their effectiveness. Moreover, each algorithm is based on\na set of assumptions, such as complete data and causal sufficiency, and tend to\nbe evaluated with data that conforms to these assumptions, however unrealistic\nthese assumptions may be in the real world. As a result, it is widely accepted\nthat synthetic performance overestimates real performance, although to what\ndegree this may happen remains unknown. This paper investigates the performance\nof 15 structure learning algorithms. We propose a methodology that applies the\nalgorithms to data that incorporates synthetic noise, in an effort to better\nunderstand the performance of structure learning algorithms when applied to\nreal data. Each algorithm is tested over multiple case studies, sample sizes,\ntypes of noise, and assessed with multiple evaluation criteria. This work\ninvolved approximately 10,000 graphs with a total structure learning runtime of\nseven months. It provides the first large-scale empirical validation of BN\nstructure learning algorithms under different assumptions of data noise. The\nresults suggest that traditional synthetic performance may overestimate\nreal-world performance by anywhere between 10% and more than 50%. They also\nshow that while score-based learning is generally superior to constraint-based\nlearning, a higher fitting score does not necessarily imply a more accurate\ncausal graph. To facilitate comparisons with future studies, we have made all\ndata, raw results, graphs and BN models freely available online.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=560&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Numerous Bayesian Network (BN) structure learning algorithms have been\nproposed in the literature over the past few decades. Each publication makes an\nempirical or theoretical case for the algorithm proposed in that publication\nand results across studies are often inconsistent in their claims about which\nalgorithm is 'best'. This is partly because there is no agreed evaluation\napproach to determine their effectiveness. Moreover, each algorithm is based on\na set of assumptions, such as complete data and causal sufficiency, and tend to\nbe evaluated with data that conforms to these assumptions, however unrealistic\nthese assumptions may be in the real world. As a result, it is widely accepted\nthat synthetic performance overestimates real performance, although to what\ndegree this may happen remains unknown. This paper investigates the performance\nof 15 structure learning algorithms. We propose a methodology that applies the\nalgorithms to data that incorporates synthetic noise, in an effort to better\nunderstand the performance of structure learning algorithms when applied to\nreal data. Each algorithm is tested over multiple case studies, sample sizes,\ntypes of noise, and assessed with multiple evaluation criteria. This work\ninvolved approximately 10,000 graphs with a total structure learning runtime of\nseven months. It provides the first large-scale empirical validation of BN\nstructure learning algorithms under different assumptions of data noise. The\nresults suggest that traditional synthetic performance may overestimate\nreal-world performance by anywhere between 10% and more than 50%. They also\nshow that while score-based learning is generally superior to constraint-based\nlearning, a higher fitting score does not necessarily imply a more accurate\ncausal graph. To facilitate comparisons with future studies, we have made all\ndata, raw results, graphs and BN models freely available online."}, "authors": ["Anthony C. Constantinou", "Yang Liu", "Kiattikun Chobtham", "Zhigao Guo", "Neville K. Kitson"], "author_detail": {"name": "Neville K. Kitson"}, "author": "Neville K. Kitson", "links": [{"href": "http://arxiv.org/abs/2005.09020v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2005.09020v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.ML", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2005.09020v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2005.09020v2", "arxiv_comment": null, "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/2011.06319v1", "guidislink": true, "updated": "2020-11-12T11:27:40Z", "updated_parsed": [2020, 11, 12, 11, 27, 40, 3, 317, 0], "published": "2020-11-12T11:27:40Z", "published_parsed": [2020, 11, 12, 11, 27, 40, 3, 317, 0], "title": "Improving Model Accuracy for Imbalanced Image Classification Tasks by\n  Adding a Final Batch Normalization Layer: An Empirical Study", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=560&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Improving Model Accuracy for Imbalanced Image Classification Tasks by\n  Adding a Final Batch Normalization Layer: An Empirical Study"}, "summary": "Some real-world domains, such as Agriculture and Healthcare, comprise\nearly-stage disease indications whose recording constitutes a rare event, and\nyet, whose precise detection at that stage is critical. In this type of highly\nimbalanced classification problems, which encompass complex features, deep\nlearning (DL) is much needed because of its strong detection capabilities. At\nthe same time, DL is observed in practice to favor majority over minority\nclasses and consequently suffer from inaccurate detection of the targeted\nearly-stage indications. To simulate such scenarios, we artificially generate\nskewness (99% vs. 1%) for certain plant types out of the PlantVillage dataset\nas a basis for classification of scarce visual cues through transfer learning.\nBy randomly and unevenly picking healthy and unhealthy samples from certain\nplant types to form a training set, we consider a base experiment as\nfine-tuning ResNet34 and VGG19 architectures and then testing the model\nperformance on a balanced dataset of healthy and unhealthy images. We\nempirically observe that the initial F1 test score jumps from 0.29 to 0.95 for\nthe minority class upon adding a final Batch Normalization (BN) layer just\nbefore the output layer in VGG19. We demonstrate that utilizing an additional\nBN layer before the output layer in modern CNN architectures has a considerable\nimpact in terms of minimizing the training time and testing error for minority\nclasses in highly imbalanced data sets. Moreover, when the final BN is\nemployed, minimizing the loss function may not be the best way to assure a high\nF1 test score for minority classes in such problems. That is, the network might\nperform better even if it is not confident enough while making a prediction;\nleading to another discussion about why softmax output is not a good\nuncertainty measure for DL models.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=560&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Some real-world domains, such as Agriculture and Healthcare, comprise\nearly-stage disease indications whose recording constitutes a rare event, and\nyet, whose precise detection at that stage is critical. In this type of highly\nimbalanced classification problems, which encompass complex features, deep\nlearning (DL) is much needed because of its strong detection capabilities. At\nthe same time, DL is observed in practice to favor majority over minority\nclasses and consequently suffer from inaccurate detection of the targeted\nearly-stage indications. To simulate such scenarios, we artificially generate\nskewness (99% vs. 1%) for certain plant types out of the PlantVillage dataset\nas a basis for classification of scarce visual cues through transfer learning.\nBy randomly and unevenly picking healthy and unhealthy samples from certain\nplant types to form a training set, we consider a base experiment as\nfine-tuning ResNet34 and VGG19 architectures and then testing the model\nperformance on a balanced dataset of healthy and unhealthy images. We\nempirically observe that the initial F1 test score jumps from 0.29 to 0.95 for\nthe minority class upon adding a final Batch Normalization (BN) layer just\nbefore the output layer in VGG19. We demonstrate that utilizing an additional\nBN layer before the output layer in modern CNN architectures has a considerable\nimpact in terms of minimizing the training time and testing error for minority\nclasses in highly imbalanced data sets. Moreover, when the final BN is\nemployed, minimizing the loss function may not be the best way to assure a high\nF1 test score for minority classes in such problems. That is, the network might\nperform better even if it is not confident enough while making a prediction;\nleading to another discussion about why softmax output is not a good\nuncertainty measure for DL models."}, "authors": ["Veysel Kocaman", "Ofer M. Shir", "Thomas Bck"], "author_detail": {"name": "Thomas Bck"}, "author": "Thomas Bck", "arxiv_comment": "Accepted for presentation and inclusion in ICPR 2020, the 25th\n  International Conference on Pattern Recognition", "links": [{"href": "http://arxiv.org/abs/2011.06319v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2011.06319v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2011.06319v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2011.06319v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/2012.03540v1", "guidislink": true, "updated": "2020-12-07T09:11:08Z", "updated_parsed": [2020, 12, 7, 9, 11, 8, 0, 342, 0], "published": "2020-12-07T09:11:08Z", "published_parsed": [2020, 12, 7, 9, 11, 8, 0, 342, 0], "title": "Efficient and Scalable Structure Learning for Bayesian Networks:\n  Algorithms and Applications", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=560&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Efficient and Scalable Structure Learning for Bayesian Networks:\n  Algorithms and Applications"}, "summary": "Structure Learning for Bayesian network (BN) is an important problem with\nextensive research. It plays central roles in a wide variety of applications in\nAlibaba Group. However, existing structure learning algorithms suffer from\nconsiderable limitations in real world applications due to their low efficiency\nand poor scalability. To resolve this, we propose a new structure learning\nalgorithm LEAST, which comprehensively fulfills our business requirements as it\nattains high accuracy, efficiency and scalability at the same time. The core\nidea of LEAST is to formulate the structure learning into a continuous\nconstrained optimization problem, with a novel differentiable constraint\nfunction measuring the acyclicity of the resulting graph. Unlike with existing\nwork, our constraint function is built on the spectral radius of the graph and\ncould be evaluated in near linear time w.r.t. the graph node size. Based on it,\nLEAST can be efficiently implemented with low storage overhead. According to\nour benchmark evaluation, LEAST runs 1 to 2 orders of magnitude faster than\nstate of the art method with comparable accuracy, and it is able to scale on\nBNs with up to hundreds of thousands of variables. In our production\nenvironment, LEAST is deployed and serves for more than 20 applications with\nthousands of executions per day. We describe a concrete scenario in a ticket\nbooking service in Alibaba, where LEAST is applied to build a near real-time\nautomatic anomaly detection and root error cause analysis system. We also show\nthat LEAST unlocks the possibility of applying BN structure learning in new\nareas, such as large-scale gene expression data analysis and explainable\nrecommendation system.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=560&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Structure Learning for Bayesian network (BN) is an important problem with\nextensive research. It plays central roles in a wide variety of applications in\nAlibaba Group. However, existing structure learning algorithms suffer from\nconsiderable limitations in real world applications due to their low efficiency\nand poor scalability. To resolve this, we propose a new structure learning\nalgorithm LEAST, which comprehensively fulfills our business requirements as it\nattains high accuracy, efficiency and scalability at the same time. The core\nidea of LEAST is to formulate the structure learning into a continuous\nconstrained optimization problem, with a novel differentiable constraint\nfunction measuring the acyclicity of the resulting graph. Unlike with existing\nwork, our constraint function is built on the spectral radius of the graph and\ncould be evaluated in near linear time w.r.t. the graph node size. Based on it,\nLEAST can be efficiently implemented with low storage overhead. According to\nour benchmark evaluation, LEAST runs 1 to 2 orders of magnitude faster than\nstate of the art method with comparable accuracy, and it is able to scale on\nBNs with up to hundreds of thousands of variables. In our production\nenvironment, LEAST is deployed and serves for more than 20 applications with\nthousands of executions per day. We describe a concrete scenario in a ticket\nbooking service in Alibaba, where LEAST is applied to build a near real-time\nautomatic anomaly detection and root error cause analysis system. We also show\nthat LEAST unlocks the possibility of applying BN structure learning in new\nareas, such as large-scale gene expression data analysis and explainable\nrecommendation system."}, "authors": ["Rong Zhu", "Andreas Pfadler", "Ziniu Wu", "Yuxing Han", "Xiaoke Yang", "Feng Ye", "Zhenping Qian", "Jingren Zhou", "Bin Cui"], "author_detail": {"name": "Bin Cui"}, "author": "Bin Cui", "links": [{"href": "http://arxiv.org/abs/2012.03540v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2012.03540v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.DB", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2012.03540v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2012.03540v1", "arxiv_comment": null, "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1401.1472v2", "guidislink": true, "updated": "2014-10-29T05:51:17Z", "updated_parsed": [2014, 10, 29, 5, 51, 17, 2, 302, 0], "published": "2014-01-07T18:40:49Z", "published_parsed": [2014, 1, 7, 18, 40, 49, 1, 7, 0], "title": "Robust Proximity Search for Balls using Sublinear Space", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=610&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Robust Proximity Search for Balls using Sublinear Space"}, "summary": "Given a set of n disjoint balls b1, . . ., bn in IRd, we provide a data\nstructure, of near linear size, that can answer (1 \\pm \\epsilon)-approximate\nkth-nearest neighbor queries in O(log n + 1/\\epsilon^d) time, where k and\n\\epsilon are provided at query time. If k and \\epsilon are provided in advance,\nwe provide a data structure to answer such queries, that requires (roughly)\nO(n/k) space; that is, the data structure has sublinear space requirement if k\nis sufficiently large.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=610&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Given a set of n disjoint balls b1, . . ., bn in IRd, we provide a data\nstructure, of near linear size, that can answer (1 \\pm \\epsilon)-approximate\nkth-nearest neighbor queries in O(log n + 1/\\epsilon^d) time, where k and\n\\epsilon are provided at query time. If k and \\epsilon are provided in advance,\nwe provide a data structure to answer such queries, that requires (roughly)\nO(n/k) space; that is, the data structure has sublinear space requirement if k\nis sufficiently large."}, "authors": ["Sariel Har-Peled", "Nirman Kumar"], "author_detail": {"name": "Nirman Kumar"}, "author": "Nirman Kumar", "links": [{"href": "http://arxiv.org/abs/1401.1472v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1401.1472v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CG", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1401.1472v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1401.1472v2", "arxiv_comment": null, "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1411.6300v1", "guidislink": true, "updated": "2014-11-23T21:19:44Z", "updated_parsed": [2014, 11, 23, 21, 19, 44, 6, 327, 0], "published": "2014-11-23T21:19:44Z", "published_parsed": [2014, 11, 23, 21, 19, 44, 6, 327, 0], "title": "Discrete Bayesian Networks: The Exact Posterior Marginal Distributions", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=610&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Discrete Bayesian Networks: The Exact Posterior Marginal Distributions"}, "summary": "In a Bayesian network, we wish to evaluate the marginal probability of a\nquery variable, which may be conditioned on the observed values of some\nevidence variables. Here we first present our \"border algorithm,\" which\nconverts a BN into a directed chain. For the polytrees, we then present in\ndetails, with some modifications and within the border algorithm framework, the\n\"revised polytree algorithm\" by Peot & Shachter (1991). Finally, we present our\n\"parentless polytree method,\" which, coupled with the border algorithm,\nconverts any Bayesian network into a polytree, rendering the complexity of our\ninferences independent of the size of network, and linear with the number of\nits evidence and query variables. All quantities in this paper have\nprobabilistic interpretations.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=610&max_results=10&sortBy=relevance&sortOrder=descending", "value": "In a Bayesian network, we wish to evaluate the marginal probability of a\nquery variable, which may be conditioned on the observed values of some\nevidence variables. Here we first present our \"border algorithm,\" which\nconverts a BN into a directed chain. For the polytrees, we then present in\ndetails, with some modifications and within the border algorithm framework, the\n\"revised polytree algorithm\" by Peot & Shachter (1991). Finally, we present our\n\"parentless polytree method,\" which, coupled with the border algorithm,\nconverts any Bayesian network into a polytree, rendering the complexity of our\ninferences independent of the size of network, and linear with the number of\nits evidence and query variables. All quantities in this paper have\nprobabilistic interpretations."}, "authors": ["Do Le Paul Minh"], "author_detail": {"name": "Do Le Paul Minh"}, "author": "Do Le Paul Minh", "links": [{"href": "http://arxiv.org/abs/1411.6300v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1411.6300v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1411.6300v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1411.6300v1", "arxiv_comment": null, "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1602.02086v1", "guidislink": true, "updated": "2016-02-05T16:35:51Z", "updated_parsed": [2016, 2, 5, 16, 35, 51, 4, 36, 0], "published": "2016-02-05T16:35:51Z", "published_parsed": [2016, 2, 5, 16, 35, 51, 4, 36, 0], "title": "Region Based Approximation for High Dimensional Bayesian Network Models", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=610&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Region Based Approximation for High Dimensional Bayesian Network Models"}, "summary": "Performing efficient inference on Bayesian Networks (BNs), with large numbers\nof densely connected variables is challenging. With exact inference methods,\nsuch as the Junction Tree algorithm, clustering complexity can grow\nexponentially with the number of nodes and so computation becomes intractable.\nThis paper presents a general purpose approximate inference algorithm called\nTriplet Region Construction (TRC) that reduces the clustering complexity for\nfactorized models from worst case exponential to polynomial. We employ graph\nfactorization to reduce connection complexity and produce clusters of limited\nsize. Unlike MCMC algorithms TRC is guaranteed to converge and we present\nexperiments that show that TRC achieves accurate results when compared with\nexact solutions.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=610&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Performing efficient inference on Bayesian Networks (BNs), with large numbers\nof densely connected variables is challenging. With exact inference methods,\nsuch as the Junction Tree algorithm, clustering complexity can grow\nexponentially with the number of nodes and so computation becomes intractable.\nThis paper presents a general purpose approximate inference algorithm called\nTriplet Region Construction (TRC) that reduces the clustering complexity for\nfactorized models from worst case exponential to polynomial. We employ graph\nfactorization to reduce connection complexity and produce clusters of limited\nsize. Unlike MCMC algorithms TRC is guaranteed to converge and we present\nexperiments that show that TRC achieves accurate results when compared with\nexact solutions."}, "authors": ["Peng Lin", "Martin Neil", "Norman Fenton"], "author_detail": {"name": "Norman Fenton"}, "author": "Norman Fenton", "links": [{"href": "http://arxiv.org/abs/1602.02086v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1602.02086v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.IT", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "math.IT", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1602.02086v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1602.02086v1", "arxiv_comment": null, "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1207.1429v1", "guidislink": true, "updated": "2012-07-04T16:31:04Z", "updated_parsed": [2012, 7, 4, 16, 31, 4, 2, 186, 0], "published": "2012-07-04T16:31:04Z", "published_parsed": [2012, 7, 4, 16, 31, 4, 2, 186, 0], "title": "Ordering-Based Search: A Simple and Effective Algorithm for Learning\n  Bayesian Networks", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=640&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Ordering-Based Search: A Simple and Effective Algorithm for Learning\n  Bayesian Networks"}, "summary": "One of the basic tasks for Bayesian networks (BNs) is that of learning a\nnetwork structure from data. The BN-learning problem is NP-hard, so the\nstandard solution is heuristic search. Many approaches have been proposed for\nthis task, but only a very small number outperform the baseline of greedy\nhill-climbing with tabu lists; moreover, many of the proposed algorithms are\nquite complex and hard to implement. In this paper, we propose a very simple\nand easy-to-implement method for addressing this task. Our approach is based on\nthe well-known fact that the best network (of bounded in-degree) consistent\nwith a given node ordering can be found very efficiently. We therefore propose\na search not over the space of structures, but over the space of orderings,\nselecting for each ordering the best network consistent with it. This search\nspace is much smaller, makes more global search steps, has a lower branching\nfactor, and avoids costly acyclicity checks. We present results for this\nalgorithm on both synthetic and real data sets, evaluating both the score of\nthe network found and in the running time. We show that ordering-based search\noutperforms the standard baseline, and is competitive with recent algorithms\nthat are much harder to implement.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=640&max_results=10&sortBy=relevance&sortOrder=descending", "value": "One of the basic tasks for Bayesian networks (BNs) is that of learning a\nnetwork structure from data. The BN-learning problem is NP-hard, so the\nstandard solution is heuristic search. Many approaches have been proposed for\nthis task, but only a very small number outperform the baseline of greedy\nhill-climbing with tabu lists; moreover, many of the proposed algorithms are\nquite complex and hard to implement. In this paper, we propose a very simple\nand easy-to-implement method for addressing this task. Our approach is based on\nthe well-known fact that the best network (of bounded in-degree) consistent\nwith a given node ordering can be found very efficiently. We therefore propose\na search not over the space of structures, but over the space of orderings,\nselecting for each ordering the best network consistent with it. This search\nspace is much smaller, makes more global search steps, has a lower branching\nfactor, and avoids costly acyclicity checks. We present results for this\nalgorithm on both synthetic and real data sets, evaluating both the score of\nthe network found and in the running time. We show that ordering-based search\noutperforms the standard baseline, and is competitive with recent algorithms\nthat are much harder to implement."}, "authors": ["Marc Teyssier", "Daphne Koller"], "author_detail": {"name": "Daphne Koller"}, "author": "Daphne Koller", "arxiv_comment": "Appears in Proceedings of the Twenty-First Conference on Uncertainty\n  in Artificial Intelligence (UAI2005)", "links": [{"href": "http://arxiv.org/abs/1207.1429v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1207.1429v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.ML", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1207.1429v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1207.1429v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1212.2519v1", "guidislink": true, "updated": "2012-10-19T15:08:01Z", "updated_parsed": [2012, 10, 19, 15, 8, 1, 4, 293, 0], "published": "2012-10-19T15:08:01Z", "published_parsed": [2012, 10, 19, 15, 8, 1, 4, 293, 0], "title": "CLP(BN): Constraint Logic Programming for Probabilistic Knowledge", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=650&max_results=10&sortBy=relevance&sortOrder=descending", "value": "CLP(BN): Constraint Logic Programming for Probabilistic Knowledge"}, "summary": "We present CLP(BN), a novel approach that aims at expressing Bayesian\nnetworks through the constraint logic programming framework. Arguably, an\nimportant limitation of traditional Bayesian networks is that they are\npropositional, and thus cannot represent relations between multiple similar\nobjects in multiple contexts. Several researchers have thus proposed\nfirst-order languages to describe such networks. Namely, one very successful\nexample of this approach are the Probabilistic Relational Models (PRMs), that\ncombine Bayesian networks with relational database technology. The key\ndifficulty that we had to address when designing CLP(cal{BN}) is that logic\nbased representations use ground terms to denote objects. With probabilitic\ndata, we need to be able to uniquely represent an object whose value we are not\nsure about. We use {sl Skolem functions} as unique new symbols that uniquely\nrepresent objects with unknown value. The semantics of CLP(cal{BN}) programs\nthen naturally follow from the general framework of constraint logic\nprogramming, as applied to a specific domain where we have probabilistic data.\nThis paper introduces and defines CLP(cal{BN}), and it describes an\nimplementation and initial experiments. The paper also shows how CLP(cal{BN})\nrelates to Probabilistic Relational Models (PRMs), Ngo and Haddawys\nProbabilistic Logic Programs, AND Kersting AND De Raedts Bayesian Logic\nPrograms.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=650&max_results=10&sortBy=relevance&sortOrder=descending", "value": "We present CLP(BN), a novel approach that aims at expressing Bayesian\nnetworks through the constraint logic programming framework. Arguably, an\nimportant limitation of traditional Bayesian networks is that they are\npropositional, and thus cannot represent relations between multiple similar\nobjects in multiple contexts. Several researchers have thus proposed\nfirst-order languages to describe such networks. Namely, one very successful\nexample of this approach are the Probabilistic Relational Models (PRMs), that\ncombine Bayesian networks with relational database technology. The key\ndifficulty that we had to address when designing CLP(cal{BN}) is that logic\nbased representations use ground terms to denote objects. With probabilitic\ndata, we need to be able to uniquely represent an object whose value we are not\nsure about. We use {sl Skolem functions} as unique new symbols that uniquely\nrepresent objects with unknown value. The semantics of CLP(cal{BN}) programs\nthen naturally follow from the general framework of constraint logic\nprogramming, as applied to a specific domain where we have probabilistic data.\nThis paper introduces and defines CLP(cal{BN}), and it describes an\nimplementation and initial experiments. The paper also shows how CLP(cal{BN})\nrelates to Probabilistic Relational Models (PRMs), Ngo and Haddawys\nProbabilistic Logic Programs, AND Kersting AND De Raedts Bayesian Logic\nPrograms."}, "authors": ["Vitor Santos Costa", "David Page", "Maleeha Qazi", "James Cussens"], "author_detail": {"name": "James Cussens"}, "author": "James Cussens", "arxiv_comment": "Appears in Proceedings of the Nineteenth Conference on Uncertainty in\n  Artificial Intelligence (UAI2003)", "links": [{"href": "http://arxiv.org/abs/1212.2519v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1212.2519v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1212.2519v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1212.2519v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1301.2295v1", "guidislink": true, "updated": "2013-01-10T16:25:25Z", "updated_parsed": [2013, 1, 10, 16, 25, 25, 3, 10, 0], "published": "2013-01-10T16:25:25Z", "published_parsed": [2013, 1, 10, 16, 25, 25, 3, 10, 0], "title": "Recognition Networks for Approximate Inference in BN20 Networks", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=650&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Recognition Networks for Approximate Inference in BN20 Networks"}, "summary": "We propose using recognition networks for approximate inference inBayesian\nnetworks (BNs). A recognition network is a multilayerperception (MLP) trained\nto predict posterior marginals given observedevidence in a particular BN. The\ninput to the MLP is a vector of thestates of the evidential nodes. The activity\nof an output unit isinterpreted as a prediction of the posterior marginal of\nthecorresponding variable. The MLP is trained using samples generated fromthe\ncorresponding BN.We evaluate a recognition network that was trained to do\ninference ina large Bayesian network, similar in structure and complexity to\ntheQuick Medical Reference, Decision Theoretic (QMR-DT). Our networkis a\nbinary, two-layer, noisy-OR network containing over 4000 potentially observable\nnodes and over 600 unobservable, hidden nodes. Inreal medical diagnosis, most\nobservables are unavailable, and there isa complex and unknown bias that\nselects which ones are provided. Weincorporate a very basic type of selection\nbias in our network: a knownpreference that available observables are positive\nrather than negative.Even this simple bias has a significant effect on the\nposterior. We compare the performance of our recognition network\ntostate-of-the-art approximate inference algorithms on a large set oftest\ncases. In order to evaluate the effect of our simplistic modelof the selection\nbias, we evaluate algorithms using a variety ofincorrectly modeled observation\nbiases. Recognition networks performwell using both correct and incorrect\nobservation biases.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=650&max_results=10&sortBy=relevance&sortOrder=descending", "value": "We propose using recognition networks for approximate inference inBayesian\nnetworks (BNs). A recognition network is a multilayerperception (MLP) trained\nto predict posterior marginals given observedevidence in a particular BN. The\ninput to the MLP is a vector of thestates of the evidential nodes. The activity\nof an output unit isinterpreted as a prediction of the posterior marginal of\nthecorresponding variable. The MLP is trained using samples generated fromthe\ncorresponding BN.We evaluate a recognition network that was trained to do\ninference ina large Bayesian network, similar in structure and complexity to\ntheQuick Medical Reference, Decision Theoretic (QMR-DT). Our networkis a\nbinary, two-layer, noisy-OR network containing over 4000 potentially observable\nnodes and over 600 unobservable, hidden nodes. Inreal medical diagnosis, most\nobservables are unavailable, and there isa complex and unknown bias that\nselects which ones are provided. Weincorporate a very basic type of selection\nbias in our network: a knownpreference that available observables are positive\nrather than negative.Even this simple bias has a significant effect on the\nposterior. We compare the performance of our recognition network\ntostate-of-the-art approximate inference algorithms on a large set oftest\ncases. In order to evaluate the effect of our simplistic modelof the selection\nbias, we evaluate algorithms using a variety ofincorrectly modeled observation\nbiases. Recognition networks performwell using both correct and incorrect\nobservation biases."}, "authors": ["Quaid Morris"], "author_detail": {"name": "Quaid Morris"}, "author": "Quaid Morris", "arxiv_comment": "Appears in Proceedings of the Seventeenth Conference on Uncertainty\n  in Artificial Intelligence (UAI2001)", "links": [{"href": "http://arxiv.org/abs/1301.2295v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1301.2295v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1301.2295v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1301.2295v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1703.03074v4", "guidislink": true, "updated": "2018-10-23T17:43:54Z", "updated_parsed": [2018, 10, 23, 17, 43, 54, 1, 296, 0], "published": "2017-03-08T23:50:19Z", "published_parsed": [2017, 3, 8, 23, 50, 19, 2, 67, 0], "title": "Efficient computational strategies to learn the structure of\n  probabilistic graphical models of cumulative phenomena", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=660&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Efficient computational strategies to learn the structure of\n  probabilistic graphical models of cumulative phenomena"}, "summary": "Structural learning of Bayesian Networks (BNs) is a NP-hard problem, which is\nfurther complicated by many theoretical issues, such as the I-equivalence among\ndifferent structures. In this work, we focus on a specific subclass of BNs,\nnamed Suppes-Bayes Causal Networks (SBCNs), which include specific structural\nconstraints based on Suppes' probabilistic causation to efficiently model\ncumulative phenomena. Here we compare the performance, via extensive\nsimulations, of various state-of-the-art search strategies, such as local\nsearch techniques and Genetic Algorithms, as well as of distinct regularization\nmethods. The assessment is performed on a large number of simulated datasets\nfrom topologies with distinct levels of complexity, various sample size and\ndifferent rates of errors in the data. Among the main results, we show that the\nintroduction of Suppes' constraints dramatically improve the inference\naccuracy, by reducing the solution space and providing a temporal ordering on\nthe variables. We also report on trade-offs among different search techniques\nthat can be efficiently employed in distinct experimental settings. This\nmanuscript is an extended version of the paper \"Structural Learning of\nProbabilistic Graphical Models of Cumulative Phenomena\" presented at the 2018\nInternational Conference on Computational Science.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=660&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Structural learning of Bayesian Networks (BNs) is a NP-hard problem, which is\nfurther complicated by many theoretical issues, such as the I-equivalence among\ndifferent structures. In this work, we focus on a specific subclass of BNs,\nnamed Suppes-Bayes Causal Networks (SBCNs), which include specific structural\nconstraints based on Suppes' probabilistic causation to efficiently model\ncumulative phenomena. Here we compare the performance, via extensive\nsimulations, of various state-of-the-art search strategies, such as local\nsearch techniques and Genetic Algorithms, as well as of distinct regularization\nmethods. The assessment is performed on a large number of simulated datasets\nfrom topologies with distinct levels of complexity, various sample size and\ndifferent rates of errors in the data. Among the main results, we show that the\nintroduction of Suppes' constraints dramatically improve the inference\naccuracy, by reducing the solution space and providing a temporal ordering on\nthe variables. We also report on trade-offs among different search techniques\nthat can be efficiently employed in distinct experimental settings. This\nmanuscript is an extended version of the paper \"Structural Learning of\nProbabilistic Graphical Models of Cumulative Phenomena\" presented at the 2018\nInternational Conference on Computational Science."}, "authors": ["Daniele Ramazzotti", "Marco S. Nobile", "Marco Antoniotti", "Alex Graudenzi"], "author_detail": {"name": "Alex Graudenzi"}, "author": "Alex Graudenzi", "links": [{"href": "http://arxiv.org/abs/1703.03074v4", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1703.03074v4", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1703.03074v4", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1703.03074v4", "arxiv_comment": null, "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1706.03397v1", "guidislink": true, "updated": "2017-06-11T19:42:31Z", "updated_parsed": [2017, 6, 11, 19, 42, 31, 6, 162, 0], "published": "2017-06-11T19:42:31Z", "published_parsed": [2017, 6, 11, 19, 42, 31, 6, 162, 0], "title": "Adversarial Network Bottleneck Features for Noise Robust Speaker\n  Verification", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=670&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Adversarial Network Bottleneck Features for Noise Robust Speaker\n  Verification"}, "summary": "In this paper, we propose a noise robust bottleneck feature representation\nwhich is generated by an adversarial network (AN). The AN includes two cascade\nconnected networks, an encoding network (EN) and a discriminative network (DN).\nMel-frequency cepstral coefficients (MFCCs) of clean and noisy speech are used\nas input to the EN and the output of the EN is used as the noise robust\nfeature. The EN and DN are trained in turn, namely, when training the DN, noise\ntypes are selected as the training labels and when training the EN, all labels\nare set as the same, i.e., the clean speech label, which aims to make the AN\nfeatures invariant to noise and thus achieve noise robustness. We evaluate the\nperformance of the proposed feature on a Gaussian Mixture Model-Universal\nBackground Model based speaker verification system, and make comparison to MFCC\nfeatures of speech enhanced by short-time spectral amplitude minimum mean\nsquare error (STSA-MMSE) and deep neural network-based speech enhancement\n(DNN-SE) methods. Experimental results on the RSR2015 database show that the\nproposed AN bottleneck feature (AN-BN) dramatically outperforms the STSA-MMSE\nand DNN-SE based MFCCs for different noise types and signal-to-noise ratios.\nFurthermore, the AN-BN feature is able to improve the speaker verification\nperformance under the clean condition.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=670&max_results=10&sortBy=relevance&sortOrder=descending", "value": "In this paper, we propose a noise robust bottleneck feature representation\nwhich is generated by an adversarial network (AN). The AN includes two cascade\nconnected networks, an encoding network (EN) and a discriminative network (DN).\nMel-frequency cepstral coefficients (MFCCs) of clean and noisy speech are used\nas input to the EN and the output of the EN is used as the noise robust\nfeature. The EN and DN are trained in turn, namely, when training the DN, noise\ntypes are selected as the training labels and when training the EN, all labels\nare set as the same, i.e., the clean speech label, which aims to make the AN\nfeatures invariant to noise and thus achieve noise robustness. We evaluate the\nperformance of the proposed feature on a Gaussian Mixture Model-Universal\nBackground Model based speaker verification system, and make comparison to MFCC\nfeatures of speech enhanced by short-time spectral amplitude minimum mean\nsquare error (STSA-MMSE) and deep neural network-based speech enhancement\n(DNN-SE) methods. Experimental results on the RSR2015 database show that the\nproposed AN bottleneck feature (AN-BN) dramatically outperforms the STSA-MMSE\nand DNN-SE based MFCCs for different noise types and signal-to-noise ratios.\nFurthermore, the AN-BN feature is able to improve the speaker verification\nperformance under the clean condition."}, "authors": ["Hong Yu", "Zheng-Hua Tan", "Zhanyu Ma", "Jun Guo"], "author_detail": {"name": "Jun Guo"}, "author": "Jun Guo", "links": [{"href": "http://arxiv.org/abs/1706.03397v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1706.03397v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.SD", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.SD", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1706.03397v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1706.03397v1", "arxiv_comment": null, "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1805.11046v3", "guidislink": true, "updated": "2018-06-17T18:37:14Z", "updated_parsed": [2018, 6, 17, 18, 37, 14, 6, 168, 0], "published": "2018-05-25T15:20:37Z", "published_parsed": [2018, 5, 25, 15, 20, 37, 4, 145, 0], "title": "Scalable Methods for 8-bit Training of Neural Networks", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=680&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Scalable Methods for 8-bit Training of Neural Networks"}, "summary": "Quantized Neural Networks (QNNs) are often used to improve network efficiency\nduring the inference phase, i.e. after the network has been trained. Extensive\nresearch in the field suggests many different quantization schemes. Still, the\nnumber of bits required, as well as the best quantization scheme, are yet\nunknown. Our theoretical analysis suggests that most of the training process is\nrobust to substantial precision reduction, and points to only a few specific\noperations that require higher precision. Armed with this knowledge, we\nquantize the model parameters, activations and layer gradients to 8-bit,\nleaving at a higher precision only the final step in the computation of the\nweight gradients. Additionally, as QNNs require batch-normalization to be\ntrained at high precision, we introduce Range Batch-Normalization (BN) which\nhas significantly higher tolerance to quantization noise and improved\ncomputational complexity. Our simulations show that Range BN is equivalent to\nthe traditional batch norm if a precise scale adjustment, which can be\napproximated analytically, is applied. To the best of the authors' knowledge,\nthis work is the first to quantize the weights, activations, as well as a\nsubstantial volume of the gradients stream, in all layers (including batch\nnormalization) to 8-bit while showing state-of-the-art results over the\nImageNet-1K dataset.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=680&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Quantized Neural Networks (QNNs) are often used to improve network efficiency\nduring the inference phase, i.e. after the network has been trained. Extensive\nresearch in the field suggests many different quantization schemes. Still, the\nnumber of bits required, as well as the best quantization scheme, are yet\nunknown. Our theoretical analysis suggests that most of the training process is\nrobust to substantial precision reduction, and points to only a few specific\noperations that require higher precision. Armed with this knowledge, we\nquantize the model parameters, activations and layer gradients to 8-bit,\nleaving at a higher precision only the final step in the computation of the\nweight gradients. Additionally, as QNNs require batch-normalization to be\ntrained at high precision, we introduce Range Batch-Normalization (BN) which\nhas significantly higher tolerance to quantization noise and improved\ncomputational complexity. Our simulations show that Range BN is equivalent to\nthe traditional batch norm if a precise scale adjustment, which can be\napproximated analytically, is applied. To the best of the authors' knowledge,\nthis work is the first to quantize the weights, activations, as well as a\nsubstantial volume of the gradients stream, in all layers (including batch\nnormalization) to 8-bit while showing state-of-the-art results over the\nImageNet-1K dataset."}, "authors": ["Ron Banner", "Itay Hubara", "Elad Hoffer", "Daniel Soudry"], "author_detail": {"name": "Daniel Soudry"}, "author": "Daniel Soudry", "links": [{"href": "http://arxiv.org/abs/1805.11046v3", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1805.11046v3", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.ML", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1805.11046v3", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1805.11046v3", "arxiv_comment": null, "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1806.02457v2", "guidislink": true, "updated": "2018-06-08T00:37:20Z", "updated_parsed": [2018, 6, 8, 0, 37, 20, 4, 159, 0], "published": "2018-06-06T23:17:12Z", "published_parsed": [2018, 6, 6, 23, 17, 12, 2, 157, 0], "title": "Reference Model of Multi-Entity Bayesian Networks for Predictive\n  Situation Awareness", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=680&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Reference Model of Multi-Entity Bayesian Networks for Predictive\n  Situation Awareness"}, "summary": "During the past quarter-century, situation awareness (SAW) has become a\ncritical research theme, because of its importance. Since the concept of SAW\nwas first introduced during World War I, various versions of SAW have been\nresearched and introduced. Predictive Situation Awareness (PSAW) focuses on the\nability to predict aspects of a temporally evolving situation over time. PSAW\nrequires a formal representation and a reasoning method using such a\nrepresentation. A Multi-Entity Bayesian Network (MEBN) is a knowledge\nrepresentation formalism combining Bayesian Networks (BN) with First-Order\nLogic (FOL). MEBN can be used to represent uncertain situations (supported by\nBN) as well as complex situations (supported by FOL). Also, efficient reasoning\nalgorithms for MEBN have been developed. MEBN can be a formal representation to\nsupport PSAW and has been used for several PSAW systems. Although several MEBN\napplications for PSAW exist, very little work can be found in the literature\nthat attempts to generalize a MEBN model to support PSAW. In this research, we\ndefine a reference model for MEBN in PSAW, called a PSAW-MEBN reference model.\nThe PSAW-MEBN reference model enables us to easily develop a MEBN model for\nPSAW by supporting the design of a MEBN model for PSAW. In this research, we\nintroduce two example use cases using the PSAW-MEBN reference model to develop\nMEBN models to support PSAW: a Smart Manufacturing System and a Maritime Domain\nAwareness System.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=680&max_results=10&sortBy=relevance&sortOrder=descending", "value": "During the past quarter-century, situation awareness (SAW) has become a\ncritical research theme, because of its importance. Since the concept of SAW\nwas first introduced during World War I, various versions of SAW have been\nresearched and introduced. Predictive Situation Awareness (PSAW) focuses on the\nability to predict aspects of a temporally evolving situation over time. PSAW\nrequires a formal representation and a reasoning method using such a\nrepresentation. A Multi-Entity Bayesian Network (MEBN) is a knowledge\nrepresentation formalism combining Bayesian Networks (BN) with First-Order\nLogic (FOL). MEBN can be used to represent uncertain situations (supported by\nBN) as well as complex situations (supported by FOL). Also, efficient reasoning\nalgorithms for MEBN have been developed. MEBN can be a formal representation to\nsupport PSAW and has been used for several PSAW systems. Although several MEBN\napplications for PSAW exist, very little work can be found in the literature\nthat attempts to generalize a MEBN model to support PSAW. In this research, we\ndefine a reference model for MEBN in PSAW, called a PSAW-MEBN reference model.\nThe PSAW-MEBN reference model enables us to easily develop a MEBN model for\nPSAW by supporting the design of a MEBN model for PSAW. In this research, we\nintroduce two example use cases using the PSAW-MEBN reference model to develop\nMEBN models to support PSAW: a Smart Manufacturing System and a Maritime Domain\nAwareness System."}, "authors": ["Cheol Young Park", "Kathryn Blackmond Laskey"], "author_detail": {"name": "Kathryn Blackmond Laskey"}, "author": "Kathryn Blackmond Laskey", "links": [{"href": "http://arxiv.org/abs/1806.02457v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1806.02457v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1806.02457v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1806.02457v2", "arxiv_comment": null, "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1904.13258v1", "guidislink": true, "updated": "2019-04-30T13:59:18Z", "updated_parsed": [2019, 4, 30, 13, 59, 18, 1, 120, 0], "published": "2019-04-30T13:59:18Z", "published_parsed": [2019, 4, 30, 13, 59, 18, 1, 120, 0], "title": "English Broadcast News Speech Recognition by Humans and Machines", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=680&max_results=10&sortBy=relevance&sortOrder=descending", "value": "English Broadcast News Speech Recognition by Humans and Machines"}, "summary": "With recent advances in deep learning, considerable attention has been given\nto achieving automatic speech recognition performance close to human\nperformance on tasks like conversational telephone speech (CTS) recognition. In\nthis paper we evaluate the usefulness of these proposed techniques on broadcast\nnews (BN), a similar challenging task. We also perform a set of recognition\nmeasurements to understand how close the achieved automatic speech recognition\nresults are to human performance on this task. On two publicly available BN\ntest sets, DEV04F and RT04, our speech recognition system using LSTM and\nresidual network based acoustic models with a combination of n-gram and neural\nnetwork language models performs at 6.5% and 5.9% word error rate. By achieving\nnew performance milestones on these test sets, our experiments show that\ntechniques developed on other related tasks, like CTS, can be transferred to\nachieve similar performance. In contrast, the best measured human recognition\nperformance on these test sets is much lower, at 3.6% and 2.8% respectively,\nindicating that there is still room for new techniques and improvements in this\nspace, to reach human performance levels.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=680&max_results=10&sortBy=relevance&sortOrder=descending", "value": "With recent advances in deep learning, considerable attention has been given\nto achieving automatic speech recognition performance close to human\nperformance on tasks like conversational telephone speech (CTS) recognition. In\nthis paper we evaluate the usefulness of these proposed techniques on broadcast\nnews (BN), a similar challenging task. We also perform a set of recognition\nmeasurements to understand how close the achieved automatic speech recognition\nresults are to human performance on this task. On two publicly available BN\ntest sets, DEV04F and RT04, our speech recognition system using LSTM and\nresidual network based acoustic models with a combination of n-gram and neural\nnetwork language models performs at 6.5% and 5.9% word error rate. By achieving\nnew performance milestones on these test sets, our experiments show that\ntechniques developed on other related tasks, like CTS, can be transferred to\nachieve similar performance. In contrast, the best measured human recognition\nperformance on these test sets is much lower, at 3.6% and 2.8% respectively,\nindicating that there is still room for new techniques and improvements in this\nspace, to reach human performance levels."}, "authors": ["Samuel Thomas", "Masayuki Suzuki", "Yinghui Huang", "Gakuto Kurata", "Zoltan Tuske", "George Saon", "Brian Kingsbury", "Michael Picheny", "Tom Dibert", "Alice Kaiser-Schatzlein", "Bern Samko"], "author_detail": {"name": "Bern Samko"}, "author": "Bern Samko", "links": [{"title": "doi", "href": "http://dx.doi.org/10.1109/ICASSP.2019.8683211", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/1904.13258v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1904.13258v1", "rel": "related", "type": "application/pdf"}], "arxiv_comment": "\\copyright 2019 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.SD", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "eess.AS", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1904.13258v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1904.13258v1", "journal_reference": null, "doi": "10.1109/ICASSP.2019.8683211"}
{"id": "http://arxiv.org/abs/1907.05715v2", "guidislink": true, "updated": "2020-06-22T16:39:39Z", "updated_parsed": [2020, 6, 22, 16, 39, 39, 0, 174, 0], "published": "2019-07-11T10:55:39Z", "published_parsed": [2019, 7, 11, 10, 55, 39, 3, 192, 0], "title": "Order and Chaos: NTK views on DNN Normalization, Checkerboard and\n  Boundary Artifacts", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=690&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Order and Chaos: NTK views on DNN Normalization, Checkerboard and\n  Boundary Artifacts"}, "summary": "We analyze architectural features of Deep Neural Networks (DNNs) using the\nso-called Neural Tangent Kernel (NTK), which describes the training and\ngeneralization of DNNs in the infinite-width setting. In this setting, we show\nthat for fully-connected DNNs, as the depth grows, two regimes appear: \"order\",\nwhere the (scaled) NTK converges to a constant, and \"chaos\", where it converges\nto a Kronecker delta. Extreme order slows down training while extreme chaos\nhinders generalization. Using the scaled ReLU as a nonlinearity, we end up in\nthe ordered regime. In contrast, Layer Normalization brings the network into\nthe chaotic regime. We observe a similar effect for Batch Normalization (BN)\napplied after the last nonlinearity. We uncover the same order and chaos modes\nin Deep Deconvolutional Networks (DC-NNs). Our analysis explains the appearance\nof so-called checkerboard patterns and border artifacts. Moving the network\ninto the chaotic regime prevents checkerboard patterns; we propose a\ngraph-based parametrization which eliminates border artifacts; finally, we\nintroduce a new layer-dependent learning rate to improve the convergence of\nDC-NNs. We illustrate our findings on DCGANs: the ordered regime leads to a\ncollapse of the generator to a checkerboard mode, which can be avoided by\ntuning the nonlinearity to reach the chaotic regime. As a result, we are able\nto obtain good quality samples for DCGANs without BN.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=690&max_results=10&sortBy=relevance&sortOrder=descending", "value": "We analyze architectural features of Deep Neural Networks (DNNs) using the\nso-called Neural Tangent Kernel (NTK), which describes the training and\ngeneralization of DNNs in the infinite-width setting. In this setting, we show\nthat for fully-connected DNNs, as the depth grows, two regimes appear: \"order\",\nwhere the (scaled) NTK converges to a constant, and \"chaos\", where it converges\nto a Kronecker delta. Extreme order slows down training while extreme chaos\nhinders generalization. Using the scaled ReLU as a nonlinearity, we end up in\nthe ordered regime. In contrast, Layer Normalization brings the network into\nthe chaotic regime. We observe a similar effect for Batch Normalization (BN)\napplied after the last nonlinearity. We uncover the same order and chaos modes\nin Deep Deconvolutional Networks (DC-NNs). Our analysis explains the appearance\nof so-called checkerboard patterns and border artifacts. Moving the network\ninto the chaotic regime prevents checkerboard patterns; we propose a\ngraph-based parametrization which eliminates border artifacts; finally, we\nintroduce a new layer-dependent learning rate to improve the convergence of\nDC-NNs. We illustrate our findings on DCGANs: the ordered regime leads to a\ncollapse of the generator to a checkerboard mode, which can be avoided by\ntuning the nonlinearity to reach the chaotic regime. As a result, we are able\nto obtain good quality samples for DCGANs without BN."}, "authors": ["Arthur Jacot", "Franck Gabriel", "Franois Ged", "Clment Hongler"], "author_detail": {"name": "Clment Hongler"}, "author": "Clment Hongler", "links": [{"href": "http://arxiv.org/abs/1907.05715v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1907.05715v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.ML", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1907.05715v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1907.05715v2", "arxiv_comment": null, "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1909.02384v2", "guidislink": true, "updated": "2019-12-31T14:31:20Z", "updated_parsed": [2019, 12, 31, 14, 31, 20, 1, 365, 0], "published": "2019-09-05T13:17:38Z", "published_parsed": [2019, 9, 5, 13, 17, 38, 3, 248, 0], "title": "Training High-Performance and Large-Scale Deep Neural Networks with Full\n  8-bit Integers", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=690&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Training High-Performance and Large-Scale Deep Neural Networks with Full\n  8-bit Integers"}, "summary": "Deep neural network (DNN) quantization converting floating-point (FP) data in\nthe network to integers (INT) is an effective way to shrink the model size for\nmemory saving and simplify the operations for compute acceleration. Recently,\nresearches on DNN quantization develop from inference to training, laying a\nfoundation for the online training on accelerators. However, existing schemes\nleaving batch normalization (BN) untouched during training are mostly\nincomplete quantization that still adopts high precision FP in some parts of\nthe data paths. Currently, there is no solution that can use only low bit-width\nINT data during the whole training process of large-scale DNNs with acceptable\naccuracy. In this work, through decomposing all the computation steps in DNNs\nand fusing three special quantization functions to satisfy the different\nprecision requirements, we propose a unified complete quantization framework\ntermed as ``WAGEUBN'' to quantize DNNs involving all data paths including W\n(Weights), A (Activation), G (Gradient), E (Error), U (Update), and BN.\nMoreover, the Momentum optimizer is also quantized to realize a completely\nquantized framework. Experiments on ResNet18/34/50 models demonstrate that\nWAGEUBN can achieve competitive accuracy on the ImageNet dataset. For the first\ntime, the study of quantization in large-scale DNNs is advanced to the full\n8-bit INT level. In this way, all the operations in the training and inference\ncan be bit-wise operations, pushing towards faster processing speed, decreased\nmemory cost, and higher energy efficiency. Our throughout quantization\nframework has great potential for future efficient portable devices with online\nlearning ability.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=690&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Deep neural network (DNN) quantization converting floating-point (FP) data in\nthe network to integers (INT) is an effective way to shrink the model size for\nmemory saving and simplify the operations for compute acceleration. Recently,\nresearches on DNN quantization develop from inference to training, laying a\nfoundation for the online training on accelerators. However, existing schemes\nleaving batch normalization (BN) untouched during training are mostly\nincomplete quantization that still adopts high precision FP in some parts of\nthe data paths. Currently, there is no solution that can use only low bit-width\nINT data during the whole training process of large-scale DNNs with acceptable\naccuracy. In this work, through decomposing all the computation steps in DNNs\nand fusing three special quantization functions to satisfy the different\nprecision requirements, we propose a unified complete quantization framework\ntermed as ``WAGEUBN'' to quantize DNNs involving all data paths including W\n(Weights), A (Activation), G (Gradient), E (Error), U (Update), and BN.\nMoreover, the Momentum optimizer is also quantized to realize a completely\nquantized framework. Experiments on ResNet18/34/50 models demonstrate that\nWAGEUBN can achieve competitive accuracy on the ImageNet dataset. For the first\ntime, the study of quantization in large-scale DNNs is advanced to the full\n8-bit INT level. In this way, all the operations in the training and inference\ncan be bit-wise operations, pushing towards faster processing speed, decreased\nmemory cost, and higher energy efficiency. Our throughout quantization\nframework has great potential for future efficient portable devices with online\nlearning ability."}, "authors": ["Yukuan Yang", "Shuang Wu", "Lei Deng", "Tianyi Yan", "Yuan Xie", "Guoqi Li"], "author_detail": {"name": "Guoqi Li"}, "author": "Guoqi Li", "links": [{"href": "http://arxiv.org/abs/1909.02384v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1909.02384v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.ML", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1909.02384v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1909.02384v2", "arxiv_comment": null, "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1912.09356v1", "guidislink": true, "updated": "2019-12-19T16:39:45Z", "updated_parsed": [2019, 12, 19, 16, 39, 45, 3, 353, 0], "published": "2019-12-19T16:39:45Z", "published_parsed": [2019, 12, 19, 16, 39, 45, 3, 353, 0], "title": "FQ-Conv: Fully Quantized Convolution for Efficient and Accurate\n  Inference", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=690&max_results=10&sortBy=relevance&sortOrder=descending", "value": "FQ-Conv: Fully Quantized Convolution for Efficient and Accurate\n  Inference"}, "summary": "Deep neural networks (DNNs) can be made hardware-efficient by reducing the\nnumerical precision of the weights and activations of the network and by\nimproving the network's resilience to noise. However, this gain in efficiency\noften comes at the cost of significantly reduced accuracy. In this paper, we\npresent a novel approach to quantizing convolutional neural network. The\nresulting networks perform all computations in low-precision, without requiring\nhigher-precision BN and nonlinearities, while still being highly accurate. To\nachieve this result, we employ a novel quantization technique that learns to\noptimally quantize the weights and activations of the network during training.\nAdditionally, to enhance training convergence we use a new training technique,\ncalled gradual quantization. We leverage the nonlinear and normalizing behavior\nof our quantization function to effectively remove the higher-precision\nnonlinearities and BN from the network. The resulting convolutional layers are\nfully quantized to low precision, from input to output, ideal for neural\nnetwork accelerators on the edge. We demonstrate the potential of this approach\non different datasets and networks, showing that ternary-weight CNNs with\nlow-precision in- and outputs perform virtually on par with their\nfull-precision equivalents. Finally, we analyze the influence of noise on the\nweights, activations and convolution outputs (multiply-accumulate, MAC) and\npropose a strategy to improve network performance under noisy conditions.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=690&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Deep neural networks (DNNs) can be made hardware-efficient by reducing the\nnumerical precision of the weights and activations of the network and by\nimproving the network's resilience to noise. However, this gain in efficiency\noften comes at the cost of significantly reduced accuracy. In this paper, we\npresent a novel approach to quantizing convolutional neural network. The\nresulting networks perform all computations in low-precision, without requiring\nhigher-precision BN and nonlinearities, while still being highly accurate. To\nachieve this result, we employ a novel quantization technique that learns to\noptimally quantize the weights and activations of the network during training.\nAdditionally, to enhance training convergence we use a new training technique,\ncalled gradual quantization. We leverage the nonlinear and normalizing behavior\nof our quantization function to effectively remove the higher-precision\nnonlinearities and BN from the network. The resulting convolutional layers are\nfully quantized to low precision, from input to output, ideal for neural\nnetwork accelerators on the edge. We demonstrate the potential of this approach\non different datasets and networks, showing that ternary-weight CNNs with\nlow-precision in- and outputs perform virtually on par with their\nfull-precision equivalents. Finally, we analyze the influence of noise on the\nweights, activations and convolution outputs (multiply-accumulate, MAC) and\npropose a strategy to improve network performance under noisy conditions."}, "authors": ["Bram-Ernst Verhoef", "Nathan Laubeuf", "Stefan Cosemans", "Peter Debacker", "Ioannis Papistas", "Arindam Mallik", "Diederik Verkest"], "author_detail": {"name": "Diederik Verkest"}, "author": "Diederik Verkest", "arxiv_comment": "12 pages, 4 Figures, 7 Tables", "links": [{"href": "http://arxiv.org/abs/1912.09356v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1912.09356v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.ML", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1912.09356v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1912.09356v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/2003.00547v1", "guidislink": true, "updated": "2020-03-01T18:38:11Z", "updated_parsed": [2020, 3, 1, 18, 38, 11, 6, 61, 0], "published": "2020-03-01T18:38:11Z", "published_parsed": [2020, 3, 1, 18, 38, 11, 6, 61, 0], "title": "Soft-Root-Sign Activation Function", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=700&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Soft-Root-Sign Activation Function"}, "summary": "The choice of activation function in deep networks has a significant effect\non the training dynamics and task performance. At present, the most effective\nand widely-used activation function is ReLU. However, because of the non-zero\nmean, negative missing and unbounded output, ReLU is at a potential\ndisadvantage during optimization. To this end, we introduce a novel activation\nfunction to manage to overcome the above three challenges. The proposed\nnonlinearity, namely \"Soft-Root-Sign\" (SRS), is smooth, non-monotonic, and\nbounded. Notably, the bounded property of SRS distinguishes itself from most\nstate-of-the-art activation functions. In contrast to ReLU, SRS can adaptively\nadjust the output by a pair of independent trainable parameters to capture\nnegative information and provide zero-mean property, which leading not only to\nbetter generalization performance, but also to faster learning speed. It also\navoids and rectifies the output distribution to be scattered in the\nnon-negative real number space, making it more compatible with batch\nnormalization (BN) and less sensitive to initialization. In experiments, we\nevaluated SRS on deep networks applied to a variety of tasks, including image\nclassification, machine translation and generative modelling. Our SRS matches\nor exceeds models with ReLU and other state-of-the-art nonlinearities, showing\nthat the proposed activation function is generalized and can achieve high\nperformance across tasks. Ablation study further verified the compatibility\nwith BN and self-adaptability for different initialization.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=700&max_results=10&sortBy=relevance&sortOrder=descending", "value": "The choice of activation function in deep networks has a significant effect\non the training dynamics and task performance. At present, the most effective\nand widely-used activation function is ReLU. However, because of the non-zero\nmean, negative missing and unbounded output, ReLU is at a potential\ndisadvantage during optimization. To this end, we introduce a novel activation\nfunction to manage to overcome the above three challenges. The proposed\nnonlinearity, namely \"Soft-Root-Sign\" (SRS), is smooth, non-monotonic, and\nbounded. Notably, the bounded property of SRS distinguishes itself from most\nstate-of-the-art activation functions. In contrast to ReLU, SRS can adaptively\nadjust the output by a pair of independent trainable parameters to capture\nnegative information and provide zero-mean property, which leading not only to\nbetter generalization performance, but also to faster learning speed. It also\navoids and rectifies the output distribution to be scattered in the\nnon-negative real number space, making it more compatible with batch\nnormalization (BN) and less sensitive to initialization. In experiments, we\nevaluated SRS on deep networks applied to a variety of tasks, including image\nclassification, machine translation and generative modelling. Our SRS matches\nor exceeds models with ReLU and other state-of-the-art nonlinearities, showing\nthat the proposed activation function is generalized and can achieve high\nperformance across tasks. Ablation study further verified the compatibility\nwith BN and self-adaptability for different initialization."}, "authors": ["Yuan Zhou", "Dandan Li", "Shuwei Huo", "Sun-Yuan Kung"], "author_detail": {"name": "Sun-Yuan Kung"}, "author": "Sun-Yuan Kung", "links": [{"href": "http://arxiv.org/abs/2003.00547v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2003.00547v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2003.00547v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2003.00547v1", "arxiv_comment": null, "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/2004.05461v2", "guidislink": true, "updated": "2020-04-19T13:44:20Z", "updated_parsed": [2020, 4, 19, 13, 44, 20, 6, 110, 0], "published": "2020-04-11T18:54:07Z", "published_parsed": [2020, 4, 11, 18, 54, 7, 5, 102, 0], "title": "Deep learning-based topological optimization for representing a\n  user-specified design area", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=700&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Deep learning-based topological optimization for representing a\n  user-specified design area"}, "summary": "Presently, topology optimization requires multiple iterations to create an\noptimized structure for given conditions. Among the conditions for topology\noptimization,the design area is one of the most important for structural\ndesign. In this study, we propose a new deep learning model to generate an\noptimized structure for a given design domain and other boundary conditions\nwithout iteration. For this purpose, we used open-source topology optimization\nMATLAB code to generate a pair of optimized structures under various design\nconditions. The resolution of the optimized structure is 32 * 32 pixels, and\nthe design conditions are design area, volume fraction, distribution of\nexternal forces, and load value. Our deep learning model is primarily composed\nof a convolutional neural network (CNN)-based encoder and decoder, trained with\ndatasets generated with MATLAB code. In the encoder, we use batch normalization\n(BN) to increase the stability of the CNN model. In the decoder, we use SPADE\n(spatially adaptive denormalization) to reinforce the design area information.\nComparing the performance of our proposed model with a CNN model that does not\nuse BN and SPADE, values for mean absolute error (MAE), mean compliance error,\nand volume error with the optimized topology structure generated in MAT-LAB\ncode were smaller, and the proposed model was able to represent the design area\nmore precisely. The proposed method generates near-optimal structures\nreflecting the design area in less computational time, compared with the\nopen-source topology optimization MATLAB code.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=700&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Presently, topology optimization requires multiple iterations to create an\noptimized structure for given conditions. Among the conditions for topology\noptimization,the design area is one of the most important for structural\ndesign. In this study, we propose a new deep learning model to generate an\noptimized structure for a given design domain and other boundary conditions\nwithout iteration. For this purpose, we used open-source topology optimization\nMATLAB code to generate a pair of optimized structures under various design\nconditions. The resolution of the optimized structure is 32 * 32 pixels, and\nthe design conditions are design area, volume fraction, distribution of\nexternal forces, and load value. Our deep learning model is primarily composed\nof a convolutional neural network (CNN)-based encoder and decoder, trained with\ndatasets generated with MATLAB code. In the encoder, we use batch normalization\n(BN) to increase the stability of the CNN model. In the decoder, we use SPADE\n(spatially adaptive denormalization) to reinforce the design area information.\nComparing the performance of our proposed model with a CNN model that does not\nuse BN and SPADE, values for mean absolute error (MAE), mean compliance error,\nand volume error with the optimized topology structure generated in MAT-LAB\ncode were smaller, and the proposed model was able to represent the design area\nmore precisely. The proposed method generates near-optimal structures\nreflecting the design area in less computational time, compared with the\nopen-source topology optimization MATLAB code."}, "authors": ["Keigo Nakamura", "Yoshiro Suzuki"], "author_detail": {"name": "Yoshiro Suzuki"}, "author": "Yoshiro Suzuki", "arxiv_comment": "12 pages, 16 figures", "links": [{"href": "http://arxiv.org/abs/2004.05461v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2004.05461v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CE", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CE", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2004.05461v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2004.05461v2", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/2004.12585v2", "guidislink": true, "updated": "2020-06-01T01:17:18Z", "updated_parsed": [2020, 6, 1, 1, 17, 18, 0, 153, 0], "published": "2020-04-27T05:20:01Z", "published_parsed": [2020, 4, 27, 5, 20, 1, 0, 118, 0], "title": "A Batch Normalized Inference Network Keeps the KL Vanishing Away", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=700&max_results=10&sortBy=relevance&sortOrder=descending", "value": "A Batch Normalized Inference Network Keeps the KL Vanishing Away"}, "summary": "Variational Autoencoder (VAE) is widely used as a generative model to\napproximate a model's posterior on latent variables by combining the amortized\nvariational inference and deep neural networks. However, when paired with\nstrong autoregressive decoders, VAE often converges to a degenerated local\noptimum known as \"posterior collapse\". Previous approaches consider the\nKullback Leibler divergence (KL) individual for each datapoint. We propose to\nlet the KL follow a distribution across the whole dataset, and analyze that it\nis sufficient to prevent posterior collapse by keeping the expectation of the\nKL's distribution positive. Then we propose Batch Normalized-VAE (BN-VAE), a\nsimple but effective approach to set a lower bound of the expectation by\nregularizing the distribution of the approximate posterior's parameters.\nWithout introducing any new model component or modifying the objective, our\napproach can avoid the posterior collapse effectively and efficiently. We\nfurther show that the proposed BN-VAE can be extended to conditional VAE\n(CVAE). Empirically, our approach surpasses strong autoregressive baselines on\nlanguage modeling, text classification and dialogue generation, and rivals more\ncomplex approaches while keeping almost the same training time as VAE.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=700&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Variational Autoencoder (VAE) is widely used as a generative model to\napproximate a model's posterior on latent variables by combining the amortized\nvariational inference and deep neural networks. However, when paired with\nstrong autoregressive decoders, VAE often converges to a degenerated local\noptimum known as \"posterior collapse\". Previous approaches consider the\nKullback Leibler divergence (KL) individual for each datapoint. We propose to\nlet the KL follow a distribution across the whole dataset, and analyze that it\nis sufficient to prevent posterior collapse by keeping the expectation of the\nKL's distribution positive. Then we propose Batch Normalized-VAE (BN-VAE), a\nsimple but effective approach to set a lower bound of the expectation by\nregularizing the distribution of the approximate posterior's parameters.\nWithout introducing any new model component or modifying the objective, our\napproach can avoid the posterior collapse effectively and efficiently. We\nfurther show that the proposed BN-VAE can be extended to conditional VAE\n(CVAE). Empirically, our approach surpasses strong autoregressive baselines on\nlanguage modeling, text classification and dialogue generation, and rivals more\ncomplex approaches while keeping almost the same training time as VAE."}, "authors": ["Qile Zhu", "Jianlin Su", "Wei Bi", "Xiaojiang Liu", "Xiyao Ma", "Xiaolin Li", "Dapeng Wu"], "author_detail": {"name": "Dapeng Wu"}, "author": "Dapeng Wu", "arxiv_comment": "An extension for the original ACL 2020 paper", "links": [{"href": "http://arxiv.org/abs/2004.12585v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2004.12585v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2004.12585v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2004.12585v2", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/2009.13333v3", "guidislink": true, "updated": "2020-11-24T09:46:16Z", "updated_parsed": [2020, 11, 24, 9, 46, 16, 1, 329, 0], "published": "2020-09-28T14:00:07Z", "published_parsed": [2020, 9, 28, 14, 0, 7, 0, 272, 0], "title": "Group Whitening: Balancing Learning Efficiency and Representational\n  Capacity", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=710&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Group Whitening: Balancing Learning Efficiency and Representational\n  Capacity"}, "summary": "Batch normalization (BN) is an important technique commonly incorporated into\ndeep learning models to perform standardization within mini-batches. The merits\nof BN in improving a model's learning efficiency can be further amplified by\napplying whitening, while its drawbacks in estimating population statistics for\ninference can be avoided through group normalization (GN). This paper proposes\ngroup whitening (GW), which exploits the advantages of the whitening operation\nand avoids the disadvantages of normalization within mini-batches. In addition,\nwe analyze the constraints imposed on features by normalization, and show how\nthe batch size (group number) affects the performance of batch (group)\nnormalized networks, from the perspective of model's representational capacity.\nThis analysis provides theoretical guidance for applying GW in practice.\nFinally, we apply the proposed GW to ResNet and ResNeXt architectures and\nconduct experiments on the ImageNet and COCO benchmarks. Results show that GW\nconsistently improves the performance of different architectures, with absolute\ngains of $1.02\\%$ $\\sim$ $1.49\\%$ in top-1 accuracy on ImageNet and $1.82\\%$\n$\\sim$ $3.21\\%$ in bounding box AP on COCO.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=710&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Batch normalization (BN) is an important technique commonly incorporated into\ndeep learning models to perform standardization within mini-batches. The merits\nof BN in improving a model's learning efficiency can be further amplified by\napplying whitening, while its drawbacks in estimating population statistics for\ninference can be avoided through group normalization (GN). This paper proposes\ngroup whitening (GW), which exploits the advantages of the whitening operation\nand avoids the disadvantages of normalization within mini-batches. In addition,\nwe analyze the constraints imposed on features by normalization, and show how\nthe batch size (group number) affects the performance of batch (group)\nnormalized networks, from the perspective of model's representational capacity.\nThis analysis provides theoretical guidance for applying GW in practice.\nFinally, we apply the proposed GW to ResNet and ResNeXt architectures and\nconduct experiments on the ImageNet and COCO benchmarks. Results show that GW\nconsistently improves the performance of different architectures, with absolute\ngains of $1.02\\%$ $\\sim$ $1.49\\%$ in top-1 accuracy on ImageNet and $1.82\\%$\n$\\sim$ $3.21\\%$ in bounding box AP on COCO."}, "authors": ["Lei Huang", "Yi Zhou", "Li Liu", "Fan Zhu", "Ling Shao"], "author_detail": {"name": "Ling Shao"}, "author": "Ling Shao", "arxiv_comment": "V2&V3 with main revisions: rewrite Section 4 of the NeurIPS\n  submission, and provide more rigorous arguments and experiments. Code\n  available at: https://github.com/huangleiBuaa/GroupWhitening", "links": [{"href": "http://arxiv.org/abs/2009.13333v3", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2009.13333v3", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.ML", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2009.13333v3", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2009.13333v3", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/2010.03658v2", "guidislink": true, "updated": "2021-01-14T04:44:11Z", "updated_parsed": [2021, 1, 14, 4, 44, 11, 3, 14, 0], "published": "2020-10-07T21:18:46Z", "published_parsed": [2020, 10, 7, 21, 18, 46, 2, 281, 0], "title": "Robust Semi-Supervised Learning with Out of Distribution Data", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=710&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Robust Semi-Supervised Learning with Out of Distribution Data"}, "summary": "Recent Semi-supervised learning (SSL) works show significant improvement in\nSSL algorithms' performance using better-unlabeled data representations.\nHowever, recent work [Oliver et al., 2018] shows that the SSL algorithm's\nperformance could degrade when the unlabeled set has out-of-distribution\nexamples (OODs). In this work, we first study the critical causes of OOD's\nnegative impact on SSL algorithms. We found that (1) the OOD's effect on the\nSSL algorithm's performance increases as its distance to the decision boundary\ndecreases, and (2) Batch Normalization (BN), a popular module, could degrade\nthe performance instead of improving the performance when the unlabeled set\ncontains OODs. To address the above causes, we proposed a novel unified-robust\nSSL approach that can be easily extended to many existing SSL algorithms, and\nimprove their robustness against OODs. In particular, we propose a simple\nmodification of batch normalization, called weighted batch normalization, that\nimproves BN's robustness against OODs. We also developed two efficient\nhyper-parameter optimization algorithms that have different tradeoffs in\ncomputational efficiency and accuracy. Extensive experiments on synthetic and\nreal-world datasets prove that our proposed approaches significantly improves\nthe robustness of four representative SSL algorithms against OODs compared with\nfour state-of-the-art robust SSL approaches.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=710&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Recent Semi-supervised learning (SSL) works show significant improvement in\nSSL algorithms' performance using better-unlabeled data representations.\nHowever, recent work [Oliver et al., 2018] shows that the SSL algorithm's\nperformance could degrade when the unlabeled set has out-of-distribution\nexamples (OODs). In this work, we first study the critical causes of OOD's\nnegative impact on SSL algorithms. We found that (1) the OOD's effect on the\nSSL algorithm's performance increases as its distance to the decision boundary\ndecreases, and (2) Batch Normalization (BN), a popular module, could degrade\nthe performance instead of improving the performance when the unlabeled set\ncontains OODs. To address the above causes, we proposed a novel unified-robust\nSSL approach that can be easily extended to many existing SSL algorithms, and\nimprove their robustness against OODs. In particular, we propose a simple\nmodification of batch normalization, called weighted batch normalization, that\nimproves BN's robustness against OODs. We also developed two efficient\nhyper-parameter optimization algorithms that have different tradeoffs in\ncomputational efficiency and accuracy. Extensive experiments on synthetic and\nreal-world datasets prove that our proposed approaches significantly improves\nthe robustness of four representative SSL algorithms against OODs compared with\nfour state-of-the-art robust SSL approaches."}, "authors": ["Xujiang Zhao", "Killamsetty Krishnateja", "Rishabh Iyer", "Feng Chen"], "author_detail": {"name": "Feng Chen"}, "author": "Feng Chen", "arxiv_comment": "Preprint", "links": [{"href": "http://arxiv.org/abs/2010.03658v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2010.03658v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2010.03658v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2010.03658v2", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/2011.05005v2", "guidislink": true, "updated": "2020-12-05T05:42:46Z", "updated_parsed": [2020, 12, 5, 5, 42, 46, 5, 340, 0], "published": "2020-11-10T09:53:20Z", "published_parsed": [2020, 11, 10, 9, 53, 20, 1, 315, 0], "title": "Deep Multimodal Fusion by Channel Exchanging", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=710&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Deep Multimodal Fusion by Channel Exchanging"}, "summary": "Deep multimodal fusion by using multiple sources of data for classification\nor regression has exhibited a clear advantage over the unimodal counterpart on\nvarious applications. Yet, current methods including aggregation-based and\nalignment-based fusion are still inadequate in balancing the trade-off between\ninter-modal fusion and intra-modal processing, incurring a bottleneck of\nperformance improvement. To this end, this paper proposes\nChannel-Exchanging-Network (CEN), a parameter-free multimodal fusion framework\nthat dynamically exchanges channels between sub-networks of different\nmodalities. Specifically, the channel exchanging process is self-guided by\nindividual channel importance that is measured by the magnitude of\nBatch-Normalization (BN) scaling factor during training. The validity of such\nexchanging process is also guaranteed by sharing convolutional filters yet\nkeeping separate BN layers across modalities, which, as an add-on benefit,\nallows our multimodal architecture to be almost as compact as a unimodal\nnetwork. Extensive experiments on semantic segmentation via RGB-D data and\nimage translation through multi-domain input verify the effectiveness of our\nCEN compared to current state-of-the-art methods. Detailed ablation studies\nhave also been carried out, which provably affirm the advantage of each\ncomponent we propose. Our code is available at https://github.com/yikaiw/CEN.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=710&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Deep multimodal fusion by using multiple sources of data for classification\nor regression has exhibited a clear advantage over the unimodal counterpart on\nvarious applications. Yet, current methods including aggregation-based and\nalignment-based fusion are still inadequate in balancing the trade-off between\ninter-modal fusion and intra-modal processing, incurring a bottleneck of\nperformance improvement. To this end, this paper proposes\nChannel-Exchanging-Network (CEN), a parameter-free multimodal fusion framework\nthat dynamically exchanges channels between sub-networks of different\nmodalities. Specifically, the channel exchanging process is self-guided by\nindividual channel importance that is measured by the magnitude of\nBatch-Normalization (BN) scaling factor during training. The validity of such\nexchanging process is also guaranteed by sharing convolutional filters yet\nkeeping separate BN layers across modalities, which, as an add-on benefit,\nallows our multimodal architecture to be almost as compact as a unimodal\nnetwork. Extensive experiments on semantic segmentation via RGB-D data and\nimage translation through multi-domain input verify the effectiveness of our\nCEN compared to current state-of-the-art methods. Detailed ablation studies\nhave also been carried out, which provably affirm the advantage of each\ncomponent we propose. Our code is available at https://github.com/yikaiw/CEN."}, "authors": ["Yikai Wang", "Wenbing Huang", "Fuchun Sun", "Tingyang Xu", "Yu Rong", "Junzhou Huang"], "author_detail": {"name": "Junzhou Huang"}, "author": "Junzhou Huang", "arxiv_comment": "NeurIPS 2020. Code and models: https://github.com/yikaiw/CEN", "links": [{"href": "http://arxiv.org/abs/2011.05005v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2011.05005v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2011.05005v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2011.05005v2", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/2012.01654v1", "guidislink": true, "updated": "2020-12-03T02:26:01Z", "updated_parsed": [2020, 12, 3, 2, 26, 1, 3, 338, 0], "published": "2020-12-03T02:26:01Z", "published_parsed": [2020, 12, 3, 2, 26, 1, 3, 338, 0], "title": "Towards Defending Multiple Adversarial Perturbations via Gated Batch\n  Normalization", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=720&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Towards Defending Multiple Adversarial Perturbations via Gated Batch\n  Normalization"}, "summary": "There is now extensive evidence demonstrating that deep neural networks are\nvulnerable to adversarial examples, motivating the development of defenses\nagainst adversarial attacks. However, existing adversarial defenses typically\nimprove model robustness against individual specific perturbation types. Some\nrecent methods improve model robustness against adversarial attacks in multiple\n$\\ell_p$ balls, but their performance against each perturbation type is still\nfar from satisfactory. To better understand this phenomenon, we propose the\n\\emph{multi-domain} hypothesis, stating that different types of adversarial\nperturbations are drawn from different domains. Guided by the multi-domain\nhypothesis, we propose \\emph{Gated Batch Normalization (GBN)}, a novel building\nblock for deep neural networks that improves robustness against multiple\nperturbation types. GBN consists of a gated sub-network and a multi-branch\nbatch normalization (BN) layer, where the gated sub-network separates different\nperturbation types, and each BN branch is in charge of a single perturbation\ntype and learns domain-specific statistics for input transformation. Then,\nfeatures from different branches are aligned as domain-invariant\nrepresentations for the subsequent layers. We perform extensive evaluations of\nour approach on MNIST, CIFAR-10, and Tiny-ImageNet, and demonstrate that GBN\noutperforms previous defense proposals against multiple perturbation types,\ni.e, $\\ell_1$, $\\ell_2$, and $\\ell_{\\infty}$ perturbations, by large margins of\n10-20\\%.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=720&max_results=10&sortBy=relevance&sortOrder=descending", "value": "There is now extensive evidence demonstrating that deep neural networks are\nvulnerable to adversarial examples, motivating the development of defenses\nagainst adversarial attacks. However, existing adversarial defenses typically\nimprove model robustness against individual specific perturbation types. Some\nrecent methods improve model robustness against adversarial attacks in multiple\n$\\ell_p$ balls, but their performance against each perturbation type is still\nfar from satisfactory. To better understand this phenomenon, we propose the\n\\emph{multi-domain} hypothesis, stating that different types of adversarial\nperturbations are drawn from different domains. Guided by the multi-domain\nhypothesis, we propose \\emph{Gated Batch Normalization (GBN)}, a novel building\nblock for deep neural networks that improves robustness against multiple\nperturbation types. GBN consists of a gated sub-network and a multi-branch\nbatch normalization (BN) layer, where the gated sub-network separates different\nperturbation types, and each BN branch is in charge of a single perturbation\ntype and learns domain-specific statistics for input transformation. Then,\nfeatures from different branches are aligned as domain-invariant\nrepresentations for the subsequent layers. We perform extensive evaluations of\nour approach on MNIST, CIFAR-10, and Tiny-ImageNet, and demonstrate that GBN\noutperforms previous defense proposals against multiple perturbation types,\ni.e, $\\ell_1$, $\\ell_2$, and $\\ell_{\\infty}$ perturbations, by large margins of\n10-20\\%."}, "authors": ["Aishan Liu", "Shiyu Tang", "Xianglong Liu", "Xinyun Chen", "Lei Huang", "Zhuozhuo Tu", "Dawn Song", "Dacheng Tao"], "author_detail": {"name": "Dacheng Tao"}, "author": "Dacheng Tao", "arxiv_comment": "28 pages", "links": [{"href": "http://arxiv.org/abs/2012.01654v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2012.01654v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2012.01654v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2012.01654v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1904.09432v1", "guidislink": true, "updated": "2019-04-20T10:59:45Z", "updated_parsed": [2019, 4, 20, 10, 59, 45, 5, 110, 0], "published": "2019-04-20T10:59:45Z", "published_parsed": [2019, 4, 20, 10, 59, 45, 5, 110, 0], "title": "Qualitative and Quantitative Risk Analysis and Safety Assessment of\n  Unmanned Aerial Vehicles Missions over the Internet", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=750&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Qualitative and Quantitative Risk Analysis and Safety Assessment of\n  Unmanned Aerial Vehicles Missions over the Internet"}, "summary": "In the last few years, Unmanned Aerial Vehicles (UAVs) are making a\nrevolution as an emerging technology with many different applications in the\nmilitary, civilian, and commercial fields. The advent of autonomous drones has\ninitiated serious challenges, including how to maintain their safe operation\nduring their missions. The safe operation of UAVs remains an open and sensitive\nissue since any unexpected behavior of the drone or any hazard would lead to\npotential risks that might be very severe. The motivation behind this work is\nto propose a methodology for the safety assurance of drones over the Internet\n{(Internet of drones (IoD))}. Two approaches will be used in performing the\nsafety analysis: (1) a qualitative safety analysis approach, and (2) a\nquantitative safety analysis approach. The first approach uses the\ninternational safety standards, namely ISO 12100 and ISO 13849 to assess the\nsafety of drone's missions by focusing on qualitative assessment techniques.\nThe methodology starts with hazard identification, risk assessment, risk\nmitigation, and finally, draws the safety recommendations associated with a\ndrone delivery use case. The second approach presents a method for the\nquantitative safety assessment using Bayesian Networks (BN) for probabilistic\nmodeling. BN utilizes the information provided by the first approach to model\nthe safety risks related to UAVs' flights. An illustrative UAV crash scenario\nis presented as a case study, followed by scenario analysis, to demonstrate the\napplicability of the proposed approach. These two analyses, qualitative and\nquantitative, enable { all involved stakeholders} to detect, explore and\naddress the risks of UAV flights, which will help the industry to better manage\nthe safety concerns of UAVs.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=750&max_results=10&sortBy=relevance&sortOrder=descending", "value": "In the last few years, Unmanned Aerial Vehicles (UAVs) are making a\nrevolution as an emerging technology with many different applications in the\nmilitary, civilian, and commercial fields. The advent of autonomous drones has\ninitiated serious challenges, including how to maintain their safe operation\nduring their missions. The safe operation of UAVs remains an open and sensitive\nissue since any unexpected behavior of the drone or any hazard would lead to\npotential risks that might be very severe. The motivation behind this work is\nto propose a methodology for the safety assurance of drones over the Internet\n{(Internet of drones (IoD))}. Two approaches will be used in performing the\nsafety analysis: (1) a qualitative safety analysis approach, and (2) a\nquantitative safety analysis approach. The first approach uses the\ninternational safety standards, namely ISO 12100 and ISO 13849 to assess the\nsafety of drone's missions by focusing on qualitative assessment techniques.\nThe methodology starts with hazard identification, risk assessment, risk\nmitigation, and finally, draws the safety recommendations associated with a\ndrone delivery use case. The second approach presents a method for the\nquantitative safety assessment using Bayesian Networks (BN) for probabilistic\nmodeling. BN utilizes the information provided by the first approach to model\nthe safety risks related to UAVs' flights. An illustrative UAV crash scenario\nis presented as a case study, followed by scenario analysis, to demonstrate the\napplicability of the proposed approach. These two analyses, qualitative and\nquantitative, enable { all involved stakeholders} to detect, explore and\naddress the risks of UAV flights, which will help the industry to better manage\nthe safety concerns of UAVs."}, "authors": ["Azza Allouch", "Anis Koubaa", "Mohamed Khalgui", "Tarek Abbes"], "author_detail": {"name": "Tarek Abbes"}, "author": "Tarek Abbes", "arxiv_comment": "Accepted in IEEE Access, April 2019", "links": [{"href": "http://arxiv.org/abs/1904.09432v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1904.09432v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.RO", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.RO", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.CY", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1904.09432v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1904.09432v1", "journal_reference": "IEEE Access, April 2019", "doi": null}
{"id": "http://arxiv.org/abs/2002.10870v2", "guidislink": true, "updated": "2020-08-04T20:38:42Z", "updated_parsed": [2020, 8, 4, 20, 38, 42, 1, 217, 0], "published": "2020-02-24T18:14:14Z", "published_parsed": [2020, 2, 24, 18, 14, 14, 0, 55, 0], "title": "AMP Chain Graphs: Minimal Separators and Structure Learning Algorithms", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=750&max_results=10&sortBy=relevance&sortOrder=descending", "value": "AMP Chain Graphs: Minimal Separators and Structure Learning Algorithms"}, "summary": "We address the problem of finding a minimal separator in an\nAndersson-Madigan-Perlman chain graph (AMP CG), namely, finding a set Z of\nnodes that separates a given nonadjacent pair of nodes such that no proper\nsubset of Z separates that pair. We analyze several versions of this problem\nand offer polynomial-time algorithms for each. These include finding a minimal\nseparator from a restricted set of nodes, finding a minimal separator for two\ngiven disjoint sets, and testing whether a given separator is minimal. To\naddress the problem of learning the structure of AMP CGs from data, we show\nthat the PC-like algorithm (Pena, 2012) is order-dependent, in the sense that\nthe output can depend on the order in which the variables are given. We propose\nseveral modifications of the PC-like algorithm that remove part or all of this\norder-dependence. We also extend the decomposition-based approach for learning\nBayesian networks (BNs) proposed by (Xie et al., 2006) to learn AMP CGs, which\ninclude BNs as a special case, under the faithfulness assumption. We prove the\ncorrectness of our extension using the minimal separator results. Using\nstandard benchmarks and synthetically generated models and data in our\nexperiments demonstrate the competitive performance of our decomposition-based\nmethod, called LCD-AMP, in comparison with the (modified versions of) PC-like\nalgorithm. The LCD-AMP algorithm usually outperforms the PC-like algorithm, and\nour modifications of the PC-like algorithm learn structures that are more\nsimilar to the underlying ground truth graphs than the original PC-like\nalgorithm, especially in high-dimensional settings. In particular, we\nempirically show that the results of both algorithms are more accurate and\nstabler when the sample size is reasonably large and the underlying graph is\nsparse.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=750&max_results=10&sortBy=relevance&sortOrder=descending", "value": "We address the problem of finding a minimal separator in an\nAndersson-Madigan-Perlman chain graph (AMP CG), namely, finding a set Z of\nnodes that separates a given nonadjacent pair of nodes such that no proper\nsubset of Z separates that pair. We analyze several versions of this problem\nand offer polynomial-time algorithms for each. These include finding a minimal\nseparator from a restricted set of nodes, finding a minimal separator for two\ngiven disjoint sets, and testing whether a given separator is minimal. To\naddress the problem of learning the structure of AMP CGs from data, we show\nthat the PC-like algorithm (Pena, 2012) is order-dependent, in the sense that\nthe output can depend on the order in which the variables are given. We propose\nseveral modifications of the PC-like algorithm that remove part or all of this\norder-dependence. We also extend the decomposition-based approach for learning\nBayesian networks (BNs) proposed by (Xie et al., 2006) to learn AMP CGs, which\ninclude BNs as a special case, under the faithfulness assumption. We prove the\ncorrectness of our extension using the minimal separator results. Using\nstandard benchmarks and synthetically generated models and data in our\nexperiments demonstrate the competitive performance of our decomposition-based\nmethod, called LCD-AMP, in comparison with the (modified versions of) PC-like\nalgorithm. The LCD-AMP algorithm usually outperforms the PC-like algorithm, and\nour modifications of the PC-like algorithm learn structures that are more\nsimilar to the underlying ground truth graphs than the original PC-like\nalgorithm, especially in high-dimensional settings. In particular, we\nempirically show that the results of both algorithms are more accurate and\nstabler when the sample size is reasonably large and the underlying graph is\nsparse."}, "authors": ["Mohammad Ali Javidian", "Marco Valtorta", "Pooyan Jamshidi"], "author_detail": {"name": "Pooyan Jamshidi"}, "author": "Pooyan Jamshidi", "arxiv_comment": "This is an arXiv version of the paper that has been accepted for\n  publication in the Journal of Artificial Intelligence Research (JAIR). arXiv\n  admin note: text overlap with arXiv:1211.3295 by other authors", "links": [{"href": "http://arxiv.org/abs/2002.10870v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2002.10870v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.DS", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.ML", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2002.10870v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2002.10870v2", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/2004.03874v1", "guidislink": true, "updated": "2020-04-08T08:27:13Z", "updated_parsed": [2020, 4, 8, 8, 27, 13, 2, 99, 0], "published": "2020-04-08T08:27:13Z", "published_parsed": [2020, 4, 8, 8, 27, 13, 2, 99, 0], "title": "Full-Duplex Radios for Edge Caching", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=760&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Full-Duplex Radios for Edge Caching"}, "summary": "This chapter focuses on the performance enhancement brought by the addition\nof caching capabilities to full-duplex (FD) radios in the context of\nultra-dense networks (UDNs). More specifically, we aim at showing that the\ninterference footprint of such networks, i.e., the major bottleneck to overcome\nto observe the theoretical FD throughput doubling at the network level, can be\nsignificantly reduced thanks to edge caching. Fundamental results show that\nmost of the gain, as compared to their half-duplex (HD) counterparts, can be\nachieved by such networks only if costly modifications to their infrastructure\nare performed and/or if high-rate signaling is exchanged between user\nequipments (UEs) over suitable control links. Therefore, we aim at proposing a\nviable and cost-effective alternative to these solutions based on pre-fetching\nlocally popular contents at the network edge. We start by considering an\ninterference-rich scenario such as an ultra-dense FD small-cell network, in\nwhich several non-cooperative FD base stations (BSs) serve their associated UEs\nwhile communicating with a wireless backhaul node (BN) to retrieve the content\nto deliver. We then describe a geographical caching policy aiming at capturing\nlocal files popularity and compute the corresponding cache-hit probability.\nThereupon, we calculate the probability of successful transmission of a file\nrequested by a UE, either directly by its serving small-cell base station (SBS)\nor by the corresponding BN: this quantity is then used to lower-bound the\nthroughput of the considered network. Our approach leverages tools from\nstochastic geometry in order to guarantee both analytical tractability of the\nproblem and generality of the results. Our numerical simulations show that\nshifting from cache-free to cache-aided FD small-cell networks yields a\nremarkable performance improvement.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=760&max_results=10&sortBy=relevance&sortOrder=descending", "value": "This chapter focuses on the performance enhancement brought by the addition\nof caching capabilities to full-duplex (FD) radios in the context of\nultra-dense networks (UDNs). More specifically, we aim at showing that the\ninterference footprint of such networks, i.e., the major bottleneck to overcome\nto observe the theoretical FD throughput doubling at the network level, can be\nsignificantly reduced thanks to edge caching. Fundamental results show that\nmost of the gain, as compared to their half-duplex (HD) counterparts, can be\nachieved by such networks only if costly modifications to their infrastructure\nare performed and/or if high-rate signaling is exchanged between user\nequipments (UEs) over suitable control links. Therefore, we aim at proposing a\nviable and cost-effective alternative to these solutions based on pre-fetching\nlocally popular contents at the network edge. We start by considering an\ninterference-rich scenario such as an ultra-dense FD small-cell network, in\nwhich several non-cooperative FD base stations (BSs) serve their associated UEs\nwhile communicating with a wireless backhaul node (BN) to retrieve the content\nto deliver. We then describe a geographical caching policy aiming at capturing\nlocal files popularity and compute the corresponding cache-hit probability.\nThereupon, we calculate the probability of successful transmission of a file\nrequested by a UE, either directly by its serving small-cell base station (SBS)\nor by the corresponding BN: this quantity is then used to lower-bound the\nthroughput of the considered network. Our approach leverages tools from\nstochastic geometry in order to guarantee both analytical tractability of the\nproblem and generality of the results. Our numerical simulations show that\nshifting from cache-free to cache-aided FD small-cell networks yields a\nremarkable performance improvement."}, "authors": ["Italo Atzeni", "Marco Maso"], "author_detail": {"name": "Marco Maso"}, "author": "Marco Maso", "arxiv_comment": "To appear in \"Wireless Edge Caching: Modelling, Analysis and\n  Optimization\", Cambridge University Press", "links": [{"href": "http://arxiv.org/abs/2004.03874v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2004.03874v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.IT", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.IT", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "math.IT", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2004.03874v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2004.03874v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/2005.09217v1", "guidislink": true, "updated": "2020-05-19T05:24:54Z", "updated_parsed": [2020, 5, 19, 5, 24, 54, 1, 140, 0], "published": "2020-05-19T05:24:54Z", "published_parsed": [2020, 5, 19, 5, 24, 54, 1, 140, 0], "title": "Do Code Review Measures Explain the Incidence of Post-Release Defects?", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=760&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Do Code Review Measures Explain the Incidence of Post-Release Defects?"}, "summary": "Aim: In contrast to studies of defects found during code review, we aim to\nclarify whether code reviews measures can explain the prevalence of\npost-release defects. Method: We replicate a study by McIntoshet. al that uses\nadditive regression to model the relationship between defects and code reviews.\nTo increase external validity, we apply the same methodology on a new software\nproject. We discuss our findings with the first author of the original study,\nMcIntosh. We then investigate how to reduce the impact of correlated predictors\nin the variable selection process and how to increase understanding of the\ninter-relationships among the predictors by employing Bayesian Network (BN)\nmodels. Context: As in the original study, we use the same measures authors\nobtained for Qt project in the original study. We mine data from version\ncontrol and issue tracker of Google Chrome and operationalize measures that are\nclose analogs to the large collection of code, process, and code review\nmeasures used in the replicated the study. Results: Both the data from the\noriginal study and the Chrome data showed high instability of the influence of\ncode review measures on defects with the results being highly sensitive to\nvariable selection procedure. Models without code review predictors had as good\nor better fit than those with review predictors. Replication, however, confirms\nwith the bulk of prior work showing that prior defects, module size, and\nauthorship have the strongest relationship to post-release defects. The\napplication of BN models helped explain the observed instability by\ndemonstrating that the review-related predictors do not affect post-release\ndefects directly and showed indirect effects. For example, changes that have no\nreview discussion tend to be associated with files that have had many prior\ndefects which in turn increase the number of post-release defects.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=760&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Aim: In contrast to studies of defects found during code review, we aim to\nclarify whether code reviews measures can explain the prevalence of\npost-release defects. Method: We replicate a study by McIntoshet. al that uses\nadditive regression to model the relationship between defects and code reviews.\nTo increase external validity, we apply the same methodology on a new software\nproject. We discuss our findings with the first author of the original study,\nMcIntosh. We then investigate how to reduce the impact of correlated predictors\nin the variable selection process and how to increase understanding of the\ninter-relationships among the predictors by employing Bayesian Network (BN)\nmodels. Context: As in the original study, we use the same measures authors\nobtained for Qt project in the original study. We mine data from version\ncontrol and issue tracker of Google Chrome and operationalize measures that are\nclose analogs to the large collection of code, process, and code review\nmeasures used in the replicated the study. Results: Both the data from the\noriginal study and the Chrome data showed high instability of the influence of\ncode review measures on defects with the results being highly sensitive to\nvariable selection procedure. Models without code review predictors had as good\nor better fit than those with review predictors. Replication, however, confirms\nwith the bulk of prior work showing that prior defects, module size, and\nauthorship have the strongest relationship to post-release defects. The\napplication of BN models helped explain the observed instability by\ndemonstrating that the review-related predictors do not affect post-release\ndefects directly and showed indirect effects. For example, changes that have no\nreview discussion tend to be associated with files that have had many prior\ndefects which in turn increase the number of post-release defects."}, "authors": ["Andrey Krutauz", "Tapajit Dey", "Peter C. Rigby", "Audris Mockus"], "author_detail": {"name": "Audris Mockus"}, "author": "Audris Mockus", "links": [{"title": "doi", "href": "http://dx.doi.org/10.1007/s10664-020-09837-4", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/2005.09217v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2005.09217v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.SE", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.SE", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2005.09217v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2005.09217v1", "arxiv_comment": null, "journal_reference": null, "doi": "10.1007/s10664-020-09837-4"}
{"id": "http://arxiv.org/abs/2010.12861v1", "guidislink": true, "updated": "2020-10-24T10:31:49Z", "updated_parsed": [2020, 10, 24, 10, 31, 49, 5, 298, 0], "published": "2020-10-24T10:31:49Z", "published_parsed": [2020, 10, 24, 10, 31, 49, 5, 298, 0], "title": "MARS: Multi-macro Architecture SRAM CIM-Based Accelerator with\n  Co-designed Compressed Neural Networks", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=760&max_results=10&sortBy=relevance&sortOrder=descending", "value": "MARS: Multi-macro Architecture SRAM CIM-Based Accelerator with\n  Co-designed Compressed Neural Networks"}, "summary": "Convolutional neural networks (CNNs) play a key role in deep learning\napplications. However, the large storage overheads and the substantial\ncomputation cost of CNNs are problematic in hardware accelerators.\nComputing-in-memory (CIM) architecture has demonstrated great potential to\neffectively compute large-scale matrix-vector multiplication. However, the\nintensive multiply and accumulation (MAC) operations executed at the crossbar\narray and the limited capacity of CIM macros remain bottlenecks for further\nimprovement of energy efficiency and throughput. To reduce computation costs,\nnetwork pruning and quantization are two widely studied compression methods to\nshrink the model size. However, most of the model compression algorithms can\nonly be implemented in digital-based CNN accelerators. For implementation in a\nstatic random access memory (SRAM) CIM-based accelerator, the model compression\nalgorithm must consider the hardware limitations of CIM macros, such as the\nnumber of word lines and bit lines that can be turned on at the same time, as\nwell as how to map the weight to the SRAM CIM macro. In this study, a software\nand hardware co-design approach is proposed to design an SRAM CIM-based CNN\naccelerator and an SRAM CIM-aware model compression algorithm. To lessen the\nhigh-precision MAC required by batch normalization (BN), a quantization\nalgorithm that can fuse BN into the weights is proposed. Furthermore, to reduce\nthe number of network parameters, a sparsity algorithm that considers a CIM\narchitecture is proposed. Last, MARS, a CIM-based CNN accelerator that can\nutilize multiple SRAM CIM macros as processing units and support a sparsity\nneural network, is proposed.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=760&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Convolutional neural networks (CNNs) play a key role in deep learning\napplications. However, the large storage overheads and the substantial\ncomputation cost of CNNs are problematic in hardware accelerators.\nComputing-in-memory (CIM) architecture has demonstrated great potential to\neffectively compute large-scale matrix-vector multiplication. However, the\nintensive multiply and accumulation (MAC) operations executed at the crossbar\narray and the limited capacity of CIM macros remain bottlenecks for further\nimprovement of energy efficiency and throughput. To reduce computation costs,\nnetwork pruning and quantization are two widely studied compression methods to\nshrink the model size. However, most of the model compression algorithms can\nonly be implemented in digital-based CNN accelerators. For implementation in a\nstatic random access memory (SRAM) CIM-based accelerator, the model compression\nalgorithm must consider the hardware limitations of CIM macros, such as the\nnumber of word lines and bit lines that can be turned on at the same time, as\nwell as how to map the weight to the SRAM CIM macro. In this study, a software\nand hardware co-design approach is proposed to design an SRAM CIM-based CNN\naccelerator and an SRAM CIM-aware model compression algorithm. To lessen the\nhigh-precision MAC required by batch normalization (BN), a quantization\nalgorithm that can fuse BN into the weights is proposed. Furthermore, to reduce\nthe number of network parameters, a sparsity algorithm that considers a CIM\narchitecture is proposed. Last, MARS, a CIM-based CNN accelerator that can\nutilize multiple SRAM CIM macros as processing units and support a sparsity\nneural network, is proposed."}, "authors": ["Syuan-Hao Sie", "Jye-Luen Lee", "Yi-Ren Chen", "Chih-Cheng Lu", "Chih-Cheng Hsieh", "Meng-Fan Chang", "Kea-Tiong Tang"], "author_detail": {"name": "Kea-Tiong Tang"}, "author": "Kea-Tiong Tang", "links": [{"href": "http://arxiv.org/abs/2010.12861v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2010.12861v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.AR", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.AR", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.ET", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2010.12861v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2010.12861v1", "arxiv_comment": null, "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/0802.3429v1", "guidislink": true, "updated": "2008-02-23T06:45:28Z", "updated_parsed": [2008, 2, 23, 6, 45, 28, 5, 54, 0], "published": "2008-02-23T06:45:28Z", "published_parsed": [2008, 2, 23, 6, 45, 28, 5, 54, 0], "title": "Quasi-Large Sparse-Sequence CDMA: Approach to Single-User Bound by\n  Linearly-Complex LAS Detectors", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=800&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Quasi-Large Sparse-Sequence CDMA: Approach to Single-User Bound by\n  Linearly-Complex LAS Detectors"}, "summary": "We have proposed a quasi-large random-sequence (QLRS) CDMA where K users\naccess a point through a common channel with spectral spreading factor N. Each\nbit is extended by a temporal spreading factor B and hopped on a BN-chip random\nsequence that is spread in time and frequency. Each user multiplexes and\ntransmits B extended bits and the total channel load is alpha = K/N bits/s/Hz.\nThe linearly-complex LAS detectors detect the transmitted bits. We have\nobtained that as B tends to infinity, if alpha < 1/2 - 1/(4ln2), each\ntransmitted bit achieves the single-bit bound in BER in high SNR regime as if\nthere was no interference bit. In simulation, when bit number BK >= 500, each\nbit can approach the single-bit bound for alpha as high as 1 bit/s/Hz. In this\npaper, we further propose the quasi-large sparse-sequence (QLSS) CDMA by\nreplacing the dense sequence in QLRS-CDMA with sparse sequence. Simulation\nresults show that when the nonzero chips are as few as 16, the BER is already\nnear that of QLRS-CDMA while the complexity is significantly reduced due to\nsequence sparsity.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=800&max_results=10&sortBy=relevance&sortOrder=descending", "value": "We have proposed a quasi-large random-sequence (QLRS) CDMA where K users\naccess a point through a common channel with spectral spreading factor N. Each\nbit is extended by a temporal spreading factor B and hopped on a BN-chip random\nsequence that is spread in time and frequency. Each user multiplexes and\ntransmits B extended bits and the total channel load is alpha = K/N bits/s/Hz.\nThe linearly-complex LAS detectors detect the transmitted bits. We have\nobtained that as B tends to infinity, if alpha < 1/2 - 1/(4ln2), each\ntransmitted bit achieves the single-bit bound in BER in high SNR regime as if\nthere was no interference bit. In simulation, when bit number BK >= 500, each\nbit can approach the single-bit bound for alpha as high as 1 bit/s/Hz. In this\npaper, we further propose the quasi-large sparse-sequence (QLSS) CDMA by\nreplacing the dense sequence in QLRS-CDMA with sparse sequence. Simulation\nresults show that when the nonzero chips are as few as 16, the BER is already\nnear that of QLRS-CDMA while the complexity is significantly reduced due to\nsequence sparsity."}, "authors": ["Yi Sun"], "author_detail": {"name": "Yi Sun"}, "author": "Yi Sun", "arxiv_comment": "CISS 2008", "links": [{"href": "http://arxiv.org/abs/0802.3429v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/0802.3429v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.IT", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.IT", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "math.IT", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/0802.3429v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/0802.3429v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1204.1681v1", "guidislink": true, "updated": "2012-04-07T21:09:48Z", "updated_parsed": [2012, 4, 7, 21, 9, 48, 5, 98, 0], "published": "2012-04-07T21:09:48Z", "published_parsed": [2012, 4, 7, 21, 9, 48, 5, 98, 0], "title": "The threshold EM algorithm for parameter learning in bayesian network\n  with incomplete data", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=840&max_results=10&sortBy=relevance&sortOrder=descending", "value": "The threshold EM algorithm for parameter learning in bayesian network\n  with incomplete data"}, "summary": "Bayesian networks (BN) are used in a big range of applications but they have\none issue concerning parameter learning. In real application, training data are\nalways incomplete or some nodes are hidden. To deal with this problem many\nlearning parameter algorithms are suggested foreground EM, Gibbs sampling and\nRBE algorithms. In order to limit the search space and escape from local maxima\nproduced by executing EM algorithm, this paper presents a learning parameter\nalgorithm that is a fusion of EM and RBE algorithms. This algorithm\nincorporates the range of a parameter into the EM algorithm. This range is\ncalculated by the first step of RBE algorithm allowing a regularization of each\nparameter in bayesian network after the maximization step of the EM algorithm.\nThe threshold EM algorithm is applied in brain tumor diagnosis and show some\nadvantages and disadvantages over the EM algorithm.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=840&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Bayesian networks (BN) are used in a big range of applications but they have\none issue concerning parameter learning. In real application, training data are\nalways incomplete or some nodes are hidden. To deal with this problem many\nlearning parameter algorithms are suggested foreground EM, Gibbs sampling and\nRBE algorithms. In order to limit the search space and escape from local maxima\nproduced by executing EM algorithm, this paper presents a learning parameter\nalgorithm that is a fusion of EM and RBE algorithms. This algorithm\nincorporates the range of a parameter into the EM algorithm. This range is\ncalculated by the first step of RBE algorithm allowing a regularization of each\nparameter in bayesian network after the maximization step of the EM algorithm.\nThe threshold EM algorithm is applied in brain tumor diagnosis and show some\nadvantages and disadvantages over the EM algorithm."}, "authors": ["Fradj Ben Lamine", "Karim Kalti", "Mohamed Ali Mahjoub"], "author_detail": {"name": "Mohamed Ali Mahjoub"}, "author": "Mohamed Ali Mahjoub", "arxiv_comment": "6 pages", "links": [{"href": "http://arxiv.org/abs/1204.1681v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1204.1681v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.ML", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1204.1681v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1204.1681v1", "journal_reference": "(IJACSA) International Journal of Advanced Computer Science and\n  Applications, Vol. 2, No. 7, pp 86-90, July 2011", "doi": null}
{"id": "http://arxiv.org/abs/1301.2313v1", "guidislink": true, "updated": "2013-01-10T16:26:44Z", "updated_parsed": [2013, 1, 10, 16, 26, 44, 3, 10, 0], "published": "2013-01-10T16:26:44Z", "published_parsed": [2013, 1, 10, 16, 26, 44, 3, 10, 0], "title": "Bayesian Error-Bars for Belief Net Inference", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=850&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Bayesian Error-Bars for Belief Net Inference"}, "summary": "A Bayesian Belief Network (BN) is a model of a joint distribution over a\nsetof n variables, with a DAG structure to represent the immediate\ndependenciesbetween the variables, and a set of parameters (aka CPTables) to\nrepresent thelocal conditional probabilities of a node, given each assignment\nto itsparents. In many situations, these parameters are themselves random\nvariables - this may reflect the uncertainty of the domain expert, or may come\nfrom atraining sample used to estimate the parameter values. The distribution\noverthese \"CPtable variables\" induces a distribution over the response the\nBNwill return to any \"What is Pr(H | E)?\" query. This paper investigates\nthevariance of this response, showing first that it is asymptotically\nnormal,then providing its mean and asymptotical variance. We then present\naneffective general algorithm for computing this variance, which has the\nsamecomplexity as simply computing the (mean value of) the response itself -\nie,O(n 2^w), where n is the number of variables and w is the effective\ntreewidth. Finally, we provide empirical evidence that this algorithm,\nwhichincorporates assumptions and approximations, works effectively in\npractice,given only small samples.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=850&max_results=10&sortBy=relevance&sortOrder=descending", "value": "A Bayesian Belief Network (BN) is a model of a joint distribution over a\nsetof n variables, with a DAG structure to represent the immediate\ndependenciesbetween the variables, and a set of parameters (aka CPTables) to\nrepresent thelocal conditional probabilities of a node, given each assignment\nto itsparents. In many situations, these parameters are themselves random\nvariables - this may reflect the uncertainty of the domain expert, or may come\nfrom atraining sample used to estimate the parameter values. The distribution\noverthese \"CPtable variables\" induces a distribution over the response the\nBNwill return to any \"What is Pr(H | E)?\" query. This paper investigates\nthevariance of this response, showing first that it is asymptotically\nnormal,then providing its mean and asymptotical variance. We then present\naneffective general algorithm for computing this variance, which has the\nsamecomplexity as simply computing the (mean value of) the response itself -\nie,O(n 2^w), where n is the number of variables and w is the effective\ntreewidth. Finally, we provide empirical evidence that this algorithm,\nwhichincorporates assumptions and approximations, works effectively in\npractice,given only small samples."}, "authors": ["Tim Van Allen", "Russell Greiner", "Peter Hooper"], "author_detail": {"name": "Peter Hooper"}, "author": "Peter Hooper", "arxiv_comment": "Appears in Proceedings of the Seventeenth Conference on Uncertainty\n  in Artificial Intelligence (UAI2001)", "links": [{"href": "http://arxiv.org/abs/1301.2313v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1301.2313v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1301.2313v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1301.2313v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1301.6727v1", "guidislink": true, "updated": "2013-01-23T16:00:10Z", "updated_parsed": [2013, 1, 23, 16, 0, 10, 2, 23, 0], "published": "2013-01-23T16:00:10Z", "published_parsed": [2013, 1, 23, 16, 0, 10, 2, 23, 0], "title": "Learning Bayesian Networks with Restricted Causal Interactions", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=850&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Learning Bayesian Networks with Restricted Causal Interactions"}, "summary": "A major problem for the learning of Bayesian networks (BNs) is the\nexponential number of parameters needed for conditional probability tables.\nRecent research reduces this complexity by modeling local structure in the\nprobability tables. We examine the use of log-linear local models. While\nlog-linear models in this context are not new (Whittaker, 1990; Buntine, 1991;\nNeal, 1992; Heckerman and Meek, 1997), for structure learning they are\ngenerally subsumed under a naive Bayes model. We describe an alternative\ninterpretation, and use a Minimum Message Length (MML) (Wallace, 1987) metric\nfor structure learning of networks exhibiting causal independence, which we\nterm first-order networks (FONs). We also investigate local model selection on\na node-by-node basis.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=850&max_results=10&sortBy=relevance&sortOrder=descending", "value": "A major problem for the learning of Bayesian networks (BNs) is the\nexponential number of parameters needed for conditional probability tables.\nRecent research reduces this complexity by modeling local structure in the\nprobability tables. We examine the use of log-linear local models. While\nlog-linear models in this context are not new (Whittaker, 1990; Buntine, 1991;\nNeal, 1992; Heckerman and Meek, 1997), for structure learning they are\ngenerally subsumed under a naive Bayes model. We describe an alternative\ninterpretation, and use a Minimum Message Length (MML) (Wallace, 1987) metric\nfor structure learning of networks exhibiting causal independence, which we\nterm first-order networks (FONs). We also investigate local model selection on\na node-by-node basis."}, "authors": ["Julian R. Neil", "Chris S. Wallace", "Kevin B. Korb"], "author_detail": {"name": "Kevin B. Korb"}, "author": "Kevin B. Korb", "arxiv_comment": "Appears in Proceedings of the Fifteenth Conference on Uncertainty in\n  Artificial Intelligence (UAI1999)", "links": [{"href": "http://arxiv.org/abs/1301.6727v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1301.6727v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.ML", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1301.6727v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1301.6727v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1303.5721v1", "guidislink": true, "updated": "2013-03-20T15:30:56Z", "updated_parsed": [2013, 3, 20, 15, 30, 56, 2, 79, 0], "published": "2013-03-20T15:30:56Z", "published_parsed": [2013, 3, 20, 15, 30, 56, 2, 79, 0], "title": "Search-based Methods to Bound Diagnostic Probabilities in Very Large\n  Belief Nets", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=850&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Search-based Methods to Bound Diagnostic Probabilities in Very Large\n  Belief Nets"}, "summary": "Since exact probabilistic inference is intractable in general for large\nmultiply connected belief nets, approximate methods are required. A promising\napproach is to use heuristic search among hypotheses (instantiations of the\nnetwork) to find the most probable ones, as in the TopN algorithm. Search is\nbased on the relative probabilities of hypotheses which are efficient to\ncompute. Given upper and lower bounds on the relative probability of partial\nhypotheses, it is possible to obtain bounds on the absolute probabilities of\nhypotheses. Best-first search aimed at reducing the maximum error progressively\nnarrows the bounds as more hypotheses are examined. Here, qualitative\nprobabilistic analysis is employed to obtain bounds on the relative probability\nof partial hypotheses for the BN20 class of networks networks and a\ngeneralization replacing the noisy OR assumption by negative synergy. The\napproach is illustrated by application to a very large belief network, QMR-BN,\nwhich is a reformulation of the Internist-1 system for diagnosis in internal\nmedicine.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=850&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Since exact probabilistic inference is intractable in general for large\nmultiply connected belief nets, approximate methods are required. A promising\napproach is to use heuristic search among hypotheses (instantiations of the\nnetwork) to find the most probable ones, as in the TopN algorithm. Search is\nbased on the relative probabilities of hypotheses which are efficient to\ncompute. Given upper and lower bounds on the relative probability of partial\nhypotheses, it is possible to obtain bounds on the absolute probabilities of\nhypotheses. Best-first search aimed at reducing the maximum error progressively\nnarrows the bounds as more hypotheses are examined. Here, qualitative\nprobabilistic analysis is employed to obtain bounds on the relative probability\nof partial hypotheses for the BN20 class of networks networks and a\ngeneralization replacing the noisy OR assumption by negative synergy. The\napproach is illustrated by application to a very large belief network, QMR-BN,\nwhich is a reformulation of the Internist-1 system for diagnosis in internal\nmedicine."}, "authors": ["Max Henrion"], "author_detail": {"name": "Max Henrion"}, "author": "Max Henrion", "arxiv_comment": "Appears in Proceedings of the Seventh Conference on Uncertainty in\n  Artificial Intelligence (UAI1991)", "links": [{"href": "http://arxiv.org/abs/1303.5721v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1303.5721v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1303.5721v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1303.5721v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1510.05067v4", "guidislink": true, "updated": "2016-02-04T08:35:58Z", "updated_parsed": [2016, 2, 4, 8, 35, 58, 3, 35, 0], "published": "2015-10-17T03:49:05Z", "published_parsed": [2015, 10, 17, 3, 49, 5, 5, 290, 0], "title": "How Important is Weight Symmetry in Backpropagation?", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=880&max_results=10&sortBy=relevance&sortOrder=descending", "value": "How Important is Weight Symmetry in Backpropagation?"}, "summary": "Gradient backpropagation (BP) requires symmetric feedforward and feedback\nconnections -- the same weights must be used for forward and backward passes.\nThis \"weight transport problem\" (Grossberg 1987) is thought to be one of the\nmain reasons to doubt BP's biologically plausibility. Using 15 different\nclassification datasets, we systematically investigate to what extent BP really\ndepends on weight symmetry. In a study that turned out to be surprisingly\nsimilar in spirit to Lillicrap et al.'s demonstration (Lillicrap et al. 2014)\nbut orthogonal in its results, our experiments indicate that: (1) the\nmagnitudes of feedback weights do not matter to performance (2) the signs of\nfeedback weights do matter -- the more concordant signs between feedforward and\ntheir corresponding feedback connections, the better (3) with feedback weights\nhaving random magnitudes and 100% concordant signs, we were able to achieve the\nsame or even better performance than SGD. (4) some\nnormalizations/stabilizations are indispensable for such asymmetric BP to work,\nnamely Batch Normalization (BN) (Ioffe and Szegedy 2015) and/or a \"Batch\nManhattan\" (BM) update rule.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=880&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Gradient backpropagation (BP) requires symmetric feedforward and feedback\nconnections -- the same weights must be used for forward and backward passes.\nThis \"weight transport problem\" (Grossberg 1987) is thought to be one of the\nmain reasons to doubt BP's biologically plausibility. Using 15 different\nclassification datasets, we systematically investigate to what extent BP really\ndepends on weight symmetry. In a study that turned out to be surprisingly\nsimilar in spirit to Lillicrap et al.'s demonstration (Lillicrap et al. 2014)\nbut orthogonal in its results, our experiments indicate that: (1) the\nmagnitudes of feedback weights do not matter to performance (2) the signs of\nfeedback weights do matter -- the more concordant signs between feedforward and\ntheir corresponding feedback connections, the better (3) with feedback weights\nhaving random magnitudes and 100% concordant signs, we were able to achieve the\nsame or even better performance than SGD. (4) some\nnormalizations/stabilizations are indispensable for such asymmetric BP to work,\nnamely Batch Normalization (BN) (Ioffe and Szegedy 2015) and/or a \"Batch\nManhattan\" (BM) update rule."}, "authors": ["Qianli Liao", "Joel Z. Leibo", "Tomaso Poggio"], "author_detail": {"name": "Tomaso Poggio"}, "author": "Tomaso Poggio", "links": [{"href": "http://arxiv.org/abs/1510.05067v4", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1510.05067v4", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1510.05067v4", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1510.05067v4", "arxiv_comment": null, "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1603.00709v1", "guidislink": true, "updated": "2016-03-02T13:46:31Z", "updated_parsed": [2016, 3, 2, 13, 46, 31, 2, 62, 0], "published": "2016-03-02T13:46:31Z", "published_parsed": [2016, 3, 2, 13, 46, 31, 2, 62, 0], "title": "Probabilistic Relational Model Benchmark Generation", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=880&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Probabilistic Relational Model Benchmark Generation"}, "summary": "The validation of any database mining methodology goes through an evaluation\nprocess where benchmarks availability is essential. In this paper, we aim to\nrandomly generate relational database benchmarks that allow to check\nprobabilistic dependencies among the attributes. We are particularly interested\nin Probabilistic Relational Models (PRMs), which extend Bayesian Networks (BNs)\nto a relational data mining context and enable effective and robust reasoning\nover relational data. Even though a panoply of works have focused, separately ,\non the generation of random Bayesian networks and relational databases, no work\nhas been identified for PRMs on that track. This paper provides an algorithmic\napproach for generating random PRMs from scratch to fill this gap. The proposed\nmethod allows to generate PRMs as well as synthetic relational data from a\nrandomly generated relational schema and a random set of probabilistic\ndependencies. This can be of interest not only for machine learning researchers\nto evaluate their proposals in a common framework, but also for databases\ndesigners to evaluate the effectiveness of the components of a database\nmanagement system.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=880&max_results=10&sortBy=relevance&sortOrder=descending", "value": "The validation of any database mining methodology goes through an evaluation\nprocess where benchmarks availability is essential. In this paper, we aim to\nrandomly generate relational database benchmarks that allow to check\nprobabilistic dependencies among the attributes. We are particularly interested\nin Probabilistic Relational Models (PRMs), which extend Bayesian Networks (BNs)\nto a relational data mining context and enable effective and robust reasoning\nover relational data. Even though a panoply of works have focused, separately ,\non the generation of random Bayesian networks and relational databases, no work\nhas been identified for PRMs on that track. This paper provides an algorithmic\napproach for generating random PRMs from scratch to fill this gap. The proposed\nmethod allows to generate PRMs as well as synthetic relational data from a\nrandomly generated relational schema and a random set of probabilistic\ndependencies. This can be of interest not only for machine learning researchers\nto evaluate their proposals in a common framework, but also for databases\ndesigners to evaluate the effectiveness of the components of a database\nmanagement system."}, "authors": ["Mouna Ben Ishak", "Rajani Chulyadyo", "Philippe Leray"], "author_detail": {"name": "Philippe Leray"}, "author": "Philippe Leray", "links": [{"href": "http://arxiv.org/abs/1603.00709v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1603.00709v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1603.00709v1", "affiliation": "LINA", "arxiv_url": "http://arxiv.org/abs/1603.00709v1", "arxiv_comment": null, "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1702.01360v1", "guidislink": true, "updated": "2017-02-05T02:22:31Z", "updated_parsed": [2017, 2, 5, 2, 22, 31, 6, 36, 0], "published": "2017-02-05T02:22:31Z", "published_parsed": [2017, 2, 5, 2, 22, 31, 6, 36, 0], "title": "An Empirical Evaluation of Zero Resource Acoustic Unit Discovery", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=890&max_results=10&sortBy=relevance&sortOrder=descending", "value": "An Empirical Evaluation of Zero Resource Acoustic Unit Discovery"}, "summary": "Acoustic unit discovery (AUD) is a process of automatically identifying a\ncategorical acoustic unit inventory from speech and producing corresponding\nacoustic unit tokenizations. AUD provides an important avenue for unsupervised\nacoustic model training in a zero resource setting where expert-provided\nlinguistic knowledge and transcribed speech are unavailable. Therefore, to\nfurther facilitate zero-resource AUD process, in this paper, we demonstrate\nacoustic feature representations can be significantly improved by (i)\nperforming linear discriminant analysis (LDA) in an unsupervised self-trained\nfashion, and (ii) leveraging resources of other languages through building a\nmultilingual bottleneck (BN) feature extractor to give effective cross-lingual\ngeneralization. Moreover, we perform comprehensive evaluations of AUD efficacy\non multiple downstream speech applications, and their correlated performance\nsuggests that AUD evaluations are feasible using different alternative language\nresources when only a subset of these evaluation resources can be available in\ntypical zero resource applications.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=890&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Acoustic unit discovery (AUD) is a process of automatically identifying a\ncategorical acoustic unit inventory from speech and producing corresponding\nacoustic unit tokenizations. AUD provides an important avenue for unsupervised\nacoustic model training in a zero resource setting where expert-provided\nlinguistic knowledge and transcribed speech are unavailable. Therefore, to\nfurther facilitate zero-resource AUD process, in this paper, we demonstrate\nacoustic feature representations can be significantly improved by (i)\nperforming linear discriminant analysis (LDA) in an unsupervised self-trained\nfashion, and (ii) leveraging resources of other languages through building a\nmultilingual bottleneck (BN) feature extractor to give effective cross-lingual\ngeneralization. Moreover, we perform comprehensive evaluations of AUD efficacy\non multiple downstream speech applications, and their correlated performance\nsuggests that AUD evaluations are feasible using different alternative language\nresources when only a subset of these evaluation resources can be available in\ntypical zero resource applications."}, "authors": ["Chunxi Liu", "Jinyi Yang", "Ming Sun", "Santosh Kesiraju", "Alena Rott", "Lucas Ondel", "Pegah Ghahremani", "Najim Dehak", "Lukas Burget", "Sanjeev Khudanpur"], "author_detail": {"name": "Sanjeev Khudanpur"}, "author": "Sanjeev Khudanpur", "arxiv_comment": "5 pages, 1 figure; Accepted for publication at ICASSP 2017", "links": [{"href": "http://arxiv.org/abs/1702.01360v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1702.01360v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1702.01360v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1702.01360v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1710.10328v1", "guidislink": true, "updated": "2017-10-27T20:48:57Z", "updated_parsed": [2017, 10, 27, 20, 48, 57, 4, 300, 0], "published": "2017-10-27T20:48:57Z", "published_parsed": [2017, 10, 27, 20, 48, 57, 4, 300, 0], "title": "Revisit Fuzzy Neural Network: Demystifying Batch Normalization and ReLU\n  with Generalized Hamming Network", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=900&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Revisit Fuzzy Neural Network: Demystifying Batch Normalization and ReLU\n  with Generalized Hamming Network"}, "summary": "We revisit fuzzy neural network with a cornerstone notion of generalized\nhamming distance, which provides a novel and theoretically justified framework\nto re-interpret many useful neural network techniques in terms of fuzzy logic.\nIn particular, we conjecture and empirically illustrate that, the celebrated\nbatch normalization (BN) technique actually adapts the normalized bias such\nthat it approximates the rightful bias induced by the generalized hamming\ndistance. Once the due bias is enforced analytically, neither the optimization\nof bias terms nor the sophisticated batch normalization is needed. Also in the\nlight of generalized hamming distance, the popular rectified linear units\n(ReLU) can be treated as setting a minimal hamming distance threshold between\nnetwork inputs and weights. This thresholding scheme, on the one hand, can be\nimproved by introducing double thresholding on both extremes of neuron outputs.\nOn the other hand, ReLUs turn out to be non-essential and can be removed from\nnetworks trained for simple tasks like MNIST classification. The proposed\ngeneralized hamming network (GHN) as such not only lends itself to rigorous\nanalysis and interpretation within the fuzzy logic theory but also demonstrates\nfast learning speed, well-controlled behaviour and state-of-the-art\nperformances on a variety of learning tasks.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=900&max_results=10&sortBy=relevance&sortOrder=descending", "value": "We revisit fuzzy neural network with a cornerstone notion of generalized\nhamming distance, which provides a novel and theoretically justified framework\nto re-interpret many useful neural network techniques in terms of fuzzy logic.\nIn particular, we conjecture and empirically illustrate that, the celebrated\nbatch normalization (BN) technique actually adapts the normalized bias such\nthat it approximates the rightful bias induced by the generalized hamming\ndistance. Once the due bias is enforced analytically, neither the optimization\nof bias terms nor the sophisticated batch normalization is needed. Also in the\nlight of generalized hamming distance, the popular rectified linear units\n(ReLU) can be treated as setting a minimal hamming distance threshold between\nnetwork inputs and weights. This thresholding scheme, on the one hand, can be\nimproved by introducing double thresholding on both extremes of neuron outputs.\nOn the other hand, ReLUs turn out to be non-essential and can be removed from\nnetworks trained for simple tasks like MNIST classification. The proposed\ngeneralized hamming network (GHN) as such not only lends itself to rigorous\nanalysis and interpretation within the fuzzy logic theory but also demonstrates\nfast learning speed, well-controlled behaviour and state-of-the-art\nperformances on a variety of learning tasks."}, "authors": ["Lixin Fan"], "author_detail": {"name": "Lixin Fan"}, "author": "Lixin Fan", "arxiv_comment": "10 pages, 5 figures, NIPS 2017", "links": [{"href": "http://arxiv.org/abs/1710.10328v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1710.10328v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.ML", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1710.10328v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1710.10328v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1804.07543v1", "guidislink": true, "updated": "2018-04-20T10:46:33Z", "updated_parsed": [2018, 4, 20, 10, 46, 33, 4, 110, 0], "published": "2018-04-20T10:46:33Z", "published_parsed": [2018, 4, 20, 10, 46, 33, 4, 110, 0], "title": "A Heuristic for Reachability Problem in Asynchronous Binary Automata\n  Networks", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=900&max_results=10&sortBy=relevance&sortOrder=descending", "value": "A Heuristic for Reachability Problem in Asynchronous Binary Automata\n  Networks"}, "summary": "On demand of efficient reachability analysis due to the inevitable complexity\nof large-scale biological models, this paper is dedicated to a novel approach:\nPermReach, for reachability problem of our new framework, Asynchronous Binary\nAutomata Networks (ABAN). ABAN is an expressive modeling framework which\ncontains all the dynamics behaviors performed by Asynchronous Boolean Networks.\nCompared to Boolean Networks (BN), ABAN has a finer description of state\ntransitions (from a local state to another, instead of symmetric Boolean\nfunctions). To analyze the reachability properties on large-scale models (like\nthe ones from systems biology), previous works exhibited an efficient\nabstraction technique called Local Causality Graph (LCG). However, this\ntechnique may be not conclusive. Our contribution here is to extend these\nresults by tackling those complex intractable cases via a heuristic technique.\nTo validate our method, tests were conducted in large biological networks,\nshowing that our method is more conclusive than existing ones.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=900&max_results=10&sortBy=relevance&sortOrder=descending", "value": "On demand of efficient reachability analysis due to the inevitable complexity\nof large-scale biological models, this paper is dedicated to a novel approach:\nPermReach, for reachability problem of our new framework, Asynchronous Binary\nAutomata Networks (ABAN). ABAN is an expressive modeling framework which\ncontains all the dynamics behaviors performed by Asynchronous Boolean Networks.\nCompared to Boolean Networks (BN), ABAN has a finer description of state\ntransitions (from a local state to another, instead of symmetric Boolean\nfunctions). To analyze the reachability properties on large-scale models (like\nthe ones from systems biology), previous works exhibited an efficient\nabstraction technique called Local Causality Graph (LCG). However, this\ntechnique may be not conclusive. Our contribution here is to extend these\nresults by tackling those complex intractable cases via a heuristic technique.\nTo validate our method, tests were conducted in large biological networks,\nshowing that our method is more conclusive than existing ones."}, "authors": ["Xinwei Chai", "Morgan Magnin", "Olivier Roux"], "author_detail": {"name": "Olivier Roux"}, "author": "Olivier Roux", "arxiv_comment": "10 pages including appendix and references", "links": [{"href": "http://arxiv.org/abs/1804.07543v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1804.07543v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.FL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.FL", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1804.07543v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1804.07543v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1809.11068v1", "guidislink": true, "updated": "2018-09-28T14:49:27Z", "updated_parsed": [2018, 9, 28, 14, 49, 27, 4, 271, 0], "published": "2018-09-28T14:49:27Z", "published_parsed": [2018, 9, 28, 14, 49, 27, 4, 271, 0], "title": "Spoken Pass-Phrase Verification in the i-vector Space", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=900&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Spoken Pass-Phrase Verification in the i-vector Space"}, "summary": "The task of spoken pass-phrase verification is to decide whether a test\nutterance contains the same phrase as given enrollment utterances. Beside other\napplications, pass-phrase verification can complement an independent speaker\nverification subsystem in text-dependent speaker verification. It can also be\nused for liveness detection by verifying that the user is able to correctly\nrespond to a randomly prompted phrase. In this paper, we build on our previous\nwork on i-vector based text-dependent speaker verification, where we have shown\nthat i-vectors extracted using phrase specific Hidden Markov Models (HMMs) or\nusing Deep Neural Network (DNN) based bottle-neck (BN) features help to reject\nutterances with wrong pass-phrases. We apply the same i-vector extraction\ntechniques to the stand-alone task of speaker-independent spoken pass-phrase\nclassification and verification. The experiments on RSR2015 and RedDots\ndatabases show that very simple scoring techniques (e.g. cosine distance\nscoring) applied to such i-vectors can provide results superior to those\npreviously published on the same data.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=900&max_results=10&sortBy=relevance&sortOrder=descending", "value": "The task of spoken pass-phrase verification is to decide whether a test\nutterance contains the same phrase as given enrollment utterances. Beside other\napplications, pass-phrase verification can complement an independent speaker\nverification subsystem in text-dependent speaker verification. It can also be\nused for liveness detection by verifying that the user is able to correctly\nrespond to a randomly prompted phrase. In this paper, we build on our previous\nwork on i-vector based text-dependent speaker verification, where we have shown\nthat i-vectors extracted using phrase specific Hidden Markov Models (HMMs) or\nusing Deep Neural Network (DNN) based bottle-neck (BN) features help to reject\nutterances with wrong pass-phrases. We apply the same i-vector extraction\ntechniques to the stand-alone task of speaker-independent spoken pass-phrase\nclassification and verification. The experiments on RSR2015 and RedDots\ndatabases show that very simple scoring techniques (e.g. cosine distance\nscoring) applied to such i-vectors can provide results superior to those\npreviously published on the same data."}, "authors": ["Hossein Zeinali", "Lukas Burget", "Hossein Sameti", "Jan Cernocky"], "author_detail": {"name": "Jan Cernocky"}, "author": "Jan Cernocky", "links": [{"href": "http://arxiv.org/abs/1809.11068v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1809.11068v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.SD", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.SD", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "eess.AS", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1809.11068v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1809.11068v1", "arxiv_comment": null, "journal_reference": "Proc. Odyssey 2018 The Speaker and Language Recognition Workshop", "doi": null}
{"id": "http://arxiv.org/abs/1606.01116v1", "guidislink": true, "updated": "2016-06-03T14:47:12Z", "updated_parsed": [2016, 6, 3, 14, 47, 12, 4, 155, 0], "published": "2016-06-03T14:47:12Z", "published_parsed": [2016, 6, 3, 14, 47, 12, 4, 155, 0], "title": "The belief noisy-or model applied to network reliability analysis", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=920&max_results=10&sortBy=relevance&sortOrder=descending", "value": "The belief noisy-or model applied to network reliability analysis"}, "summary": "One difficulty faced in knowledge engineering for Bayesian Network (BN) is\nthe quan-tification step where the Conditional Probability Tables (CPTs) are\ndetermined. The number of parameters included in CPTs increases exponentially\nwith the number of parent variables. The most common solution is the\napplication of the so-called canonical gates. The Noisy-OR (NOR) gate, which\ntakes advantage of the independence of causal interactions, provides a\nlogarithmic reduction of the number of parameters required to specify a CPT. In\nthis paper, an extension of NOR model based on the theory of belief functions,\nnamed Belief Noisy-OR (BNOR), is proposed. BNOR is capable of dealing with both\naleatory and epistemic uncertainty of the network. Compared with NOR, more rich\ninformation which is of great value for making decisions can be got when the\navailable knowledge is uncertain. Specially, when there is no epistemic\nuncertainty, BNOR degrades into NOR. Additionally, different structures of BNOR\nare presented in this paper in order to meet various needs of engineers. The\napplication of BNOR model on the reliability evaluation problem of networked\nsystems demonstrates its effectiveness.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=920&max_results=10&sortBy=relevance&sortOrder=descending", "value": "One difficulty faced in knowledge engineering for Bayesian Network (BN) is\nthe quan-tification step where the Conditional Probability Tables (CPTs) are\ndetermined. The number of parameters included in CPTs increases exponentially\nwith the number of parent variables. The most common solution is the\napplication of the so-called canonical gates. The Noisy-OR (NOR) gate, which\ntakes advantage of the independence of causal interactions, provides a\nlogarithmic reduction of the number of parameters required to specify a CPT. In\nthis paper, an extension of NOR model based on the theory of belief functions,\nnamed Belief Noisy-OR (BNOR), is proposed. BNOR is capable of dealing with both\naleatory and epistemic uncertainty of the network. Compared with NOR, more rich\ninformation which is of great value for making decisions can be got when the\navailable knowledge is uncertain. Specially, when there is no epistemic\nuncertainty, BNOR degrades into NOR. Additionally, different structures of BNOR\nare presented in this paper in order to meet various needs of engineers. The\napplication of BNOR model on the reliability evaluation problem of networked\nsystems demonstrates its effectiveness."}, "authors": ["Kuang Zhou", "Arnaud Martin", "Quan Pan"], "author_detail": {"name": "Quan Pan"}, "author": "Quan Pan", "links": [{"href": "http://arxiv.org/abs/1606.01116v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1606.01116v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1606.01116v1", "affiliation": "DRUID", "arxiv_url": "http://arxiv.org/abs/1606.01116v1", "arxiv_comment": null, "journal_reference": "International Journal of Uncertainty, Fuzziness and\n  Knowledge-Based Systems, World Scientific Publishing, 2016", "doi": null}
{"id": "http://arxiv.org/abs/1611.04989v2", "guidislink": true, "updated": "2016-11-16T04:28:06Z", "updated_parsed": [2016, 11, 16, 4, 28, 6, 2, 321, 0], "published": "2016-11-15T19:02:35Z", "published_parsed": [2016, 11, 15, 19, 2, 35, 1, 320, 0], "title": "Recurrent Neural Network based Part-of-Speech Tagger for Code-Mixed\n  Social Media Text", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=920&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Recurrent Neural Network based Part-of-Speech Tagger for Code-Mixed\n  Social Media Text"}, "summary": "This paper describes Centre for Development of Advanced Computing's (CDACM)\nsubmission to the shared task-'Tool Contest on POS tagging for Code-Mixed\nIndian Social Media (Facebook, Twitter, and Whatsapp) Text', collocated with\nICON-2016. The shared task was to predict Part of Speech (POS) tag at word\nlevel for a given text. The code-mixed text is generated mostly on social media\nby multilingual users. The presence of the multilingual words,\ntransliterations, and spelling variations make such content linguistically\ncomplex. In this paper, we propose an approach to POS tag code-mixed social\nmedia text using Recurrent Neural Network Language Model (RNN-LM) architecture.\nWe submitted the results for Hindi-English (hi-en), Bengali-English (bn-en),\nand Telugu-English (te-en) code-mixed data.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=920&max_results=10&sortBy=relevance&sortOrder=descending", "value": "This paper describes Centre for Development of Advanced Computing's (CDACM)\nsubmission to the shared task-'Tool Contest on POS tagging for Code-Mixed\nIndian Social Media (Facebook, Twitter, and Whatsapp) Text', collocated with\nICON-2016. The shared task was to predict Part of Speech (POS) tag at word\nlevel for a given text. The code-mixed text is generated mostly on social media\nby multilingual users. The presence of the multilingual words,\ntransliterations, and spelling variations make such content linguistically\ncomplex. In this paper, we propose an approach to POS tag code-mixed social\nmedia text using Recurrent Neural Network Language Model (RNN-LM) architecture.\nWe submitted the results for Hindi-English (hi-en), Bengali-English (bn-en),\nand Telugu-English (te-en) code-mixed data."}, "authors": ["Raj Nath Patel", "Prakash B. Pimpale", "M Sasikumar"], "author_detail": {"name": "M Sasikumar"}, "author": "M Sasikumar", "arxiv_comment": "7 pages, Published at the Tool Contest on POS tagging for Indian\n  Social Media Text, ICON 2016", "links": [{"href": "http://arxiv.org/abs/1611.04989v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1611.04989v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1611.04989v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1611.04989v2", "journal_reference": "In Proceedings of the Tool Contest on POS tagging for Indian\n  Social Media Text, ICON 2016", "doi": null}
{"id": "http://arxiv.org/abs/1709.09623v1", "guidislink": true, "updated": "2017-09-27T16:55:25Z", "updated_parsed": [2017, 9, 27, 16, 55, 25, 2, 270, 0], "published": "2017-09-27T16:55:25Z", "published_parsed": [2017, 9, 27, 16, 55, 25, 2, 270, 0], "title": "A Permission-Dependent Type System for Secure Information Flow Analysis", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=930&max_results=10&sortBy=relevance&sortOrder=descending", "value": "A Permission-Dependent Type System for Secure Information Flow Analysis"}, "summary": "We introduce a novel type system for enforcing secure information flow in an\nimperative language. Our work is motivated by the problem of statically\nchecking potential information leakage in Android applications. To this end, we\ndesign a lightweight type system featuring Android permission model, where the\npermissions are statically assigned to applications and are used to enforce\naccess control in the applications. We take inspiration from a type system by\nBanerjee and Naumann (BN) to allow security types to be dependent on the\npermissions of the applications. A novel feature of our type system is a typing\nrule for conditional branching induced by permission testing, which introduces\na merging operator on security types, allowing more precise security policies\nto be enforced. The soundness of our type system is proved with respect to a\nnotion of noninterference. In addition, a type inference algorithm is presented\nfor the underlying security type system, by reducing the inference problem to a\nconstraint solving problem in the lattice of security types.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=930&max_results=10&sortBy=relevance&sortOrder=descending", "value": "We introduce a novel type system for enforcing secure information flow in an\nimperative language. Our work is motivated by the problem of statically\nchecking potential information leakage in Android applications. To this end, we\ndesign a lightweight type system featuring Android permission model, where the\npermissions are statically assigned to applications and are used to enforce\naccess control in the applications. We take inspiration from a type system by\nBanerjee and Naumann (BN) to allow security types to be dependent on the\npermissions of the applications. A novel feature of our type system is a typing\nrule for conditional branching induced by permission testing, which introduces\na merging operator on security types, allowing more precise security policies\nto be enforced. The soundness of our type system is proved with respect to a\nnotion of noninterference. In addition, a type inference algorithm is presented\nfor the underlying security type system, by reducing the inference problem to a\nconstraint solving problem in the lattice of security types."}, "authors": ["Hongxu Chen", "Alwen Tiu", "Zhiwu Xu", "Yang Liu"], "author_detail": {"name": "Yang Liu"}, "author": "Yang Liu", "arxiv_comment": "48 pages", "links": [{"href": "http://arxiv.org/abs/1709.09623v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1709.09623v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.PL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.PL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.CR", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1709.09623v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1709.09623v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1803.10560v1", "guidislink": true, "updated": "2018-03-28T12:37:27Z", "updated_parsed": [2018, 3, 28, 12, 37, 27, 2, 87, 0], "published": "2018-03-28T12:37:27Z", "published_parsed": [2018, 3, 28, 12, 37, 27, 2, 87, 0], "title": "Normalization of Neural Networks using Analytic Variance Propagation", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=930&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Normalization of Neural Networks using Analytic Variance Propagation"}, "summary": "We address the problem of estimating statistics of hidden units in a neural\nnetwork using a method of analytic moment propagation. These statistics are\nuseful for approximate whitening of the inputs in front of saturating\nnon-linearities such as a sigmoid function. This is important for\ninitialization of training and for reducing the accumulated scale and bias\ndependencies (compensating covariate shift), which presumably eases the\nlearning. In batch normalization, which is currently a very widely applied\ntechnique, sample estimates of statistics of hidden units over a batch are\nused. The proposed estimation uses an analytic propagation of mean and variance\nof the training set through the network. The result depends on the network\nstructure and its current weights but not on the specific batch input. The\nestimates are suitable for initialization and normalization, efficient to\ncompute and independent of the batch size. The experimental verification well\nsupports these claims. However, the method does not share the generalization\nproperties of BN, to which our experiments give some additional insight.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=930&max_results=10&sortBy=relevance&sortOrder=descending", "value": "We address the problem of estimating statistics of hidden units in a neural\nnetwork using a method of analytic moment propagation. These statistics are\nuseful for approximate whitening of the inputs in front of saturating\nnon-linearities such as a sigmoid function. This is important for\ninitialization of training and for reducing the accumulated scale and bias\ndependencies (compensating covariate shift), which presumably eases the\nlearning. In batch normalization, which is currently a very widely applied\ntechnique, sample estimates of statistics of hidden units over a batch are\nused. The proposed estimation uses an analytic propagation of mean and variance\nof the training set through the network. The result depends on the network\nstructure and its current weights but not on the specific batch input. The\nestimates are suitable for initialization and normalization, efficient to\ncompute and independent of the batch size. The experimental verification well\nsupports these claims. However, the method does not share the generalization\nproperties of BN, to which our experiments give some additional insight."}, "authors": ["Alexander Shekhovtsov", "Boris Flach"], "author_detail": {"name": "Boris Flach"}, "author": "Boris Flach", "links": [{"href": "http://arxiv.org/abs/1803.10560v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1803.10560v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.NE", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.ML", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1803.10560v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1803.10560v1", "arxiv_comment": null, "journal_reference": "In Proceedings of Computer Vision Winter Workshop 2018", "doi": null}
{"id": "http://arxiv.org/abs/1810.00846v2", "guidislink": true, "updated": "2019-07-13T12:16:18Z", "updated_parsed": [2019, 7, 13, 12, 16, 18, 5, 194, 0], "published": "2018-10-01T17:38:58Z", "published_parsed": [2018, 10, 1, 17, 38, 58, 0, 274, 0], "title": "Classification from Positive, Unlabeled and Biased Negative Data", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=940&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Classification from Positive, Unlabeled and Biased Negative Data"}, "summary": "In binary classification, there are situations where negative (N) data are\ntoo diverse to be fully labeled and we often resort to positive-unlabeled (PU)\nlearning in these scenarios. However, collecting a non-representative N set\nthat contains only a small portion of all possible N data can often be much\neasier in practice. This paper studies a novel classification framework which\nincorporates such biased N (bN) data in PU learning. We provide a method based\non empirical risk minimization to address this PUbN classification problem. Our\napproach can be regarded as a novel example-weighting algorithm, with the\nweight of each example computed through a preliminary step that draws\ninspiration from PU learning. We also derive an estimation error bound for the\nproposed method. Experimental results demonstrate the effectiveness of our\nalgorithm in not only PUbN learning scenarios but also ordinary PU learning\nscenarios on several benchmark datasets.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=940&max_results=10&sortBy=relevance&sortOrder=descending", "value": "In binary classification, there are situations where negative (N) data are\ntoo diverse to be fully labeled and we often resort to positive-unlabeled (PU)\nlearning in these scenarios. However, collecting a non-representative N set\nthat contains only a small portion of all possible N data can often be much\neasier in practice. This paper studies a novel classification framework which\nincorporates such biased N (bN) data in PU learning. We provide a method based\non empirical risk minimization to address this PUbN classification problem. Our\napproach can be regarded as a novel example-weighting algorithm, with the\nweight of each example computed through a preliminary step that draws\ninspiration from PU learning. We also derive an estimation error bound for the\nproposed method. Experimental results demonstrate the effectiveness of our\nalgorithm in not only PUbN learning scenarios but also ordinary PU learning\nscenarios on several benchmark datasets."}, "authors": ["Yu-Guan Hsieh", "Gang Niu", "Masashi Sugiyama"], "author_detail": {"name": "Masashi Sugiyama"}, "author": "Masashi Sugiyama", "arxiv_comment": "In Proceedings of the 36th International Conference on Machine\n  Learning (ICML 2019)", "links": [{"href": "http://arxiv.org/abs/1810.00846v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1810.00846v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.ML", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1810.00846v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1810.00846v2", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1811.10169v1", "guidislink": true, "updated": "2018-11-26T04:00:03Z", "updated_parsed": [2018, 11, 26, 4, 0, 3, 0, 330, 0], "published": "2018-11-26T04:00:03Z", "published_parsed": [2018, 11, 26, 4, 0, 3, 0, 330, 0], "title": "Improving Gated Recurrent Unit Based Acoustic Modeling with Batch\n  Normalization and Enlarged Context", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=940&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Improving Gated Recurrent Unit Based Acoustic Modeling with Batch\n  Normalization and Enlarged Context"}, "summary": "The use of future contextual information is typically shown to be helpful for\nacoustic modeling. Recently, we proposed a RNN model called minimal gated\nrecurrent unit with input projection (mGRUIP), in which a context module namely\ntemporal convolution, is specifically designed to model the future context.\nThis model, mGRUIP with context module (mGRUIP-Ctx), has been shown to be able\nof utilizing the future context effectively, meanwhile with quite low model\nlatency and computation cost. In this paper, we continue to improve mGRUIP-Ctx\nwith two revisions: applying BN methods and enlarging model context.\nExperimental results on two Mandarin ASR tasks (8400 hours and 60K hours) show\nthat, the revised mGRUIP-Ctx outperform LSTM with a large margin (11% to 38%).\nIt even performs slightly better than a superior BLSTM on the 8400h task, with\n33M less parameters and just 290ms model latency.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=940&max_results=10&sortBy=relevance&sortOrder=descending", "value": "The use of future contextual information is typically shown to be helpful for\nacoustic modeling. Recently, we proposed a RNN model called minimal gated\nrecurrent unit with input projection (mGRUIP), in which a context module namely\ntemporal convolution, is specifically designed to model the future context.\nThis model, mGRUIP with context module (mGRUIP-Ctx), has been shown to be able\nof utilizing the future context effectively, meanwhile with quite low model\nlatency and computation cost. In this paper, we continue to improve mGRUIP-Ctx\nwith two revisions: applying BN methods and enlarging model context.\nExperimental results on two Mandarin ASR tasks (8400 hours and 60K hours) show\nthat, the revised mGRUIP-Ctx outperform LSTM with a large margin (11% to 38%).\nIt even performs slightly better than a superior BLSTM on the 8400h task, with\n33M less parameters and just 290ms model latency."}, "authors": ["Jie Li", "Yahui Shan", "Xiaorui Wang", "Yan Li"], "author_detail": {"name": "Yan Li"}, "author": "Yan Li", "arxiv_comment": "ISCSLP 2018", "links": [{"href": "http://arxiv.org/abs/1811.10169v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1811.10169v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "eess.AS", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1811.10169v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1811.10169v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1902.00744v2", "guidislink": true, "updated": "2019-04-07T01:58:12Z", "updated_parsed": [2019, 4, 7, 1, 58, 12, 6, 97, 0], "published": "2019-02-02T16:14:52Z", "published_parsed": [2019, 2, 2, 16, 14, 52, 5, 33, 0], "title": "Asymmetric Valleys: Beyond Sharp and Flat Local Minima", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=950&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Asymmetric Valleys: Beyond Sharp and Flat Local Minima"}, "summary": "Despite the non-convex nature of their loss functions, deep neural networks\nare known to generalize well when optimized with stochastic gradient descent\n(SGD). Recent work conjectures that SGD with proper configuration is able to\nfind wide and flat local minima, which have been proposed to be associated with\ngood generalization performance. In this paper, we observe that local minima of\nmodern deep networks are more than being flat or sharp. Specifically, at a\nlocal minimum there exist many asymmetric directions such that the loss\nincreases abruptly along one side, and slowly along the opposite side--we\nformally define such minima as asymmetric valleys. Under mild assumptions, we\nprove that for asymmetric valleys, a solution biased towards the flat side\ngeneralizes better than the exact minimizer. Further, we show that simply\naveraging the weights along the SGD trajectory gives rise to such biased\nsolutions implicitly. This provides a theoretical explanation for the\nintriguing phenomenon observed by Izmailov et al. (2018). In addition, we\nempirically find that batch normalization (BN) appears to be a major cause for\nasymmetric valleys.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=950&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Despite the non-convex nature of their loss functions, deep neural networks\nare known to generalize well when optimized with stochastic gradient descent\n(SGD). Recent work conjectures that SGD with proper configuration is able to\nfind wide and flat local minima, which have been proposed to be associated with\ngood generalization performance. In this paper, we observe that local minima of\nmodern deep networks are more than being flat or sharp. Specifically, at a\nlocal minimum there exist many asymmetric directions such that the loss\nincreases abruptly along one side, and slowly along the opposite side--we\nformally define such minima as asymmetric valleys. Under mild assumptions, we\nprove that for asymmetric valleys, a solution biased towards the flat side\ngeneralizes better than the exact minimizer. Further, we show that simply\naveraging the weights along the SGD trajectory gives rise to such biased\nsolutions implicitly. This provides a theoretical explanation for the\nintriguing phenomenon observed by Izmailov et al. (2018). In addition, we\nempirically find that batch normalization (BN) appears to be a major cause for\nasymmetric valleys."}, "authors": ["Haowei He", "Gao Huang", "Yang Yuan"], "author_detail": {"name": "Yang Yuan"}, "author": "Yang Yuan", "arxiv_comment": "submitted to ICML 2019", "links": [{"href": "http://arxiv.org/abs/1902.00744v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1902.00744v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.ML", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1902.00744v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1902.00744v2", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1902.03495v2", "guidislink": true, "updated": "2020-03-24T02:53:35Z", "updated_parsed": [2020, 3, 24, 2, 53, 35, 1, 84, 0], "published": "2019-02-09T21:34:15Z", "published_parsed": [2019, 2, 9, 21, 34, 15, 5, 40, 0], "title": "Evaluating reliability of complex systems for Predictive maintenance", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=950&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Evaluating reliability of complex systems for Predictive maintenance"}, "summary": "Predictive Maintenance (PdM) can only be implemented when the online\nknowledge of system condition is available, and this has become available with\ndeployment of on-equipment sensors. To date, most studies on predicting the\nremaining useful lifetime of a system have been focusing on either\nsingle-component systems or systems with deterministic reliability structures.\nThis assumption is not applicable on some realistic problems, where there exist\nuncertainties in reliability structures of complex systems. In this paper, a\nPdM scheme is developed by employing a Discrete Time Markov Chain (DTMC) for\nforecasting the health of monitored components and a Bayesian Network (BN) for\nmodeling the multi-component system reliability. Therefore, probabilistic\ninferences on both the system and its components status can be made and PdM can\nbe scheduled on both levels.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=950&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Predictive Maintenance (PdM) can only be implemented when the online\nknowledge of system condition is available, and this has become available with\ndeployment of on-equipment sensors. To date, most studies on predicting the\nremaining useful lifetime of a system have been focusing on either\nsingle-component systems or systems with deterministic reliability structures.\nThis assumption is not applicable on some realistic problems, where there exist\nuncertainties in reliability structures of complex systems. In this paper, a\nPdM scheme is developed by employing a Discrete Time Markov Chain (DTMC) for\nforecasting the health of monitored components and a Bayesian Network (BN) for\nmodeling the multi-component system reliability. Therefore, probabilistic\ninferences on both the system and its components status can be made and PdM can\nbe scheduled on both levels."}, "authors": ["Dongjin Lee", "Rong Pan"], "author_detail": {"name": "Rong Pan"}, "author": "Rong Pan", "arxiv_comment": "7 pages, This is a Conference paper submitted to Industrial and\n  Systems Engineering Research Conference 2016 (ISERC)", "links": [{"href": "http://arxiv.org/abs/1902.03495v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1902.03495v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.SY", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.SY", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1902.03495v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1902.03495v2", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1902.09809v1", "guidislink": true, "updated": "2019-02-26T09:09:24Z", "updated_parsed": [2019, 2, 26, 9, 9, 24, 1, 57, 0], "published": "2019-02-26T09:09:24Z", "published_parsed": [2019, 2, 26, 9, 9, 24, 1, 57, 0], "title": "Recurrent Convolution for Compact and Cost-Adjustable Neural Networks:\n  An Empirical Study", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=950&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Recurrent Convolution for Compact and Cost-Adjustable Neural Networks:\n  An Empirical Study"}, "summary": "Recurrent convolution (RC) shares the same convolutional kernels and unrolls\nthem multiple steps, which is originally proposed to model time-space signals.\nWe argue that RC can be viewed as a model compression strategy for deep\nconvolutional neural networks. RC reduces the redundancy across layers.\nHowever, the performance of an RC network is not satisfactory if we directly\nunroll the same kernels multiple steps. We propose a simple yet effective\nvariant which improves the RC networks: the batch normalization layers of an RC\nmodule are learned independently (not shared) for different unrolling steps.\nMoreover, we verify that RC can perform cost-adjustable inference which is\nachieved by varying its unrolling steps. We learn double independent BN layers\nfor cost-adjustable RC networks, i.e. independent w.r.t both the unrolling\nsteps of current cell and upstream cell. We provide insights on why the\nproposed method works successfully. Experiments on both image classification\nand image denoise demonstrate the effectiveness of our method.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=950&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Recurrent convolution (RC) shares the same convolutional kernels and unrolls\nthem multiple steps, which is originally proposed to model time-space signals.\nWe argue that RC can be viewed as a model compression strategy for deep\nconvolutional neural networks. RC reduces the redundancy across layers.\nHowever, the performance of an RC network is not satisfactory if we directly\nunroll the same kernels multiple steps. We propose a simple yet effective\nvariant which improves the RC networks: the batch normalization layers of an RC\nmodule are learned independently (not shared) for different unrolling steps.\nMoreover, we verify that RC can perform cost-adjustable inference which is\nachieved by varying its unrolling steps. We learn double independent BN layers\nfor cost-adjustable RC networks, i.e. independent w.r.t both the unrolling\nsteps of current cell and upstream cell. We provide insights on why the\nproposed method works successfully. Experiments on both image classification\nand image denoise demonstrate the effectiveness of our method."}, "authors": ["Zhendong Zhang", "Cheolkon Jung"], "author_detail": {"name": "Cheolkon Jung"}, "author": "Cheolkon Jung", "arxiv_comment": "8 pages; preprint; work in progress", "links": [{"href": "http://arxiv.org/abs/1902.09809v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1902.09809v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.ML", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1902.09809v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1902.09809v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1903.02034v1", "guidislink": true, "updated": "2019-03-05T20:10:11Z", "updated_parsed": [2019, 3, 5, 20, 10, 11, 1, 64, 0], "published": "2019-03-05T20:10:11Z", "published_parsed": [2019, 3, 5, 20, 10, 11, 1, 64, 0], "title": "Risk Assessment of Autonomous Vehicles Using Bayesian Defense Graphs", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=960&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Risk Assessment of Autonomous Vehicles Using Bayesian Defense Graphs"}, "summary": "Recent developments have made autonomous vehicles (AVs) closer to hitting our\nroads. However, their security is still a major concern among drivers as well\nas manufacturers. Although some work has been done to identify threats and\npossible solutions, a theoretical framework is needed to measure the security\nof AVs. In this paper, a simple security model based on defense graphs is\nproposed to quantitatively assess the likelihood of threats on components of an\nAV in the presence of available countermeasures. A Bayesian network (BN)\nanalysis is then applied to obtain the associated security risk. In a case\nstudy, the model and the analysis are studied for GPS spoofing attacks to\ndemonstrate the effectiveness of the proposed approach for a highly vulnerable\ncomponent.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=960&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Recent developments have made autonomous vehicles (AVs) closer to hitting our\nroads. However, their security is still a major concern among drivers as well\nas manufacturers. Although some work has been done to identify threats and\npossible solutions, a theoretical framework is needed to measure the security\nof AVs. In this paper, a simple security model based on defense graphs is\nproposed to quantitatively assess the likelihood of threats on components of an\nAV in the presence of available countermeasures. A Bayesian network (BN)\nanalysis is then applied to obtain the associated security risk. In a case\nstudy, the model and the analysis are studied for GPS spoofing attacks to\ndemonstrate the effectiveness of the proposed approach for a highly vulnerable\ncomponent."}, "authors": ["Ali Behfarnia", "Ali Eslami"], "author_detail": {"name": "Ali Eslami"}, "author": "Ali Eslami", "arxiv_comment": "IEEE 88th Vehicular Technology Conference: VTC2018-Fall", "links": [{"href": "http://arxiv.org/abs/1903.02034v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1903.02034v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CR", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CR", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1903.02034v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1903.02034v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1903.09330v1", "guidislink": true, "updated": "2019-03-22T02:55:31Z", "updated_parsed": [2019, 3, 22, 2, 55, 31, 4, 81, 0], "published": "2019-03-22T02:55:31Z", "published_parsed": [2019, 3, 22, 2, 55, 31, 4, 81, 0], "title": "A resnet-based universal method for speckle reduction in optical\n  coherence tomography images", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=960&max_results=10&sortBy=relevance&sortOrder=descending", "value": "A resnet-based universal method for speckle reduction in optical\n  coherence tomography images"}, "summary": "In this work we propose a ResNet-based universal method for speckle reduction\nin optical coherence tomography (OCT) images. The proposed model contains 3\nmain modules: Convolution-BN-ReLU, Branch and Residual module. Unlike\ntraditional algorithms, the model can learn from training data instead of\nselecting parameters manually such as noise level. Application of this proposed\nmethod to the OCT images shows a more than 22 dB signal-to-noise ratio\nimprovement in speckle noise reduction with minimal structure blurring. The\nproposed method provides strong generalization ability and can process noisy\nother types of OCT images without retraining. It outperforms other filtering\nmethods in suppressing speckle noises and revealing subtle features.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=960&max_results=10&sortBy=relevance&sortOrder=descending", "value": "In this work we propose a ResNet-based universal method for speckle reduction\nin optical coherence tomography (OCT) images. The proposed model contains 3\nmain modules: Convolution-BN-ReLU, Branch and Residual module. Unlike\ntraditional algorithms, the model can learn from training data instead of\nselecting parameters manually such as noise level. Application of this proposed\nmethod to the OCT images shows a more than 22 dB signal-to-noise ratio\nimprovement in speckle noise reduction with minimal structure blurring. The\nproposed method provides strong generalization ability and can process noisy\nother types of OCT images without retraining. It outperforms other filtering\nmethods in suppressing speckle noises and revealing subtle features."}, "authors": ["Cai Ning", "Shi Fei", "Hu Dianlin", "Chen Yang"], "author_detail": {"name": "Chen Yang"}, "author": "Chen Yang", "links": [{"href": "http://arxiv.org/abs/1903.09330v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1903.09330v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "eess.IV", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1903.09330v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1903.09330v1", "arxiv_comment": null, "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1904.03515v1", "guidislink": true, "updated": "2019-04-06T19:10:05Z", "updated_parsed": [2019, 4, 6, 19, 10, 5, 5, 96, 0], "published": "2019-04-06T19:10:05Z", "published_parsed": [2019, 4, 6, 19, 10, 5, 5, 96, 0], "title": "Split Batch Normalization: Improving Semi-Supervised Learning under\n  Domain Shift", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=960&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Split Batch Normalization: Improving Semi-Supervised Learning under\n  Domain Shift"}, "summary": "Recent work has shown that using unlabeled data in semi-supervised learning\nis not always beneficial and can even hurt generalization, especially when\nthere is a class mismatch between the unlabeled and labeled examples. We\ninvestigate this phenomenon for image classification on the CIFAR-10 and the\nImageNet datasets, and with many other forms of domain shifts applied (e.g.\nsalt-and-pepper noise). Our main contribution is Split Batch Normalization\n(Split-BN), a technique to improve SSL when the additional unlabeled data comes\nfrom a shifted distribution. We achieve it by using separate batch\nnormalization statistics for unlabeled examples. Due to its simplicity, we\nrecommend it as a standard practice. Finally, we analyse how domain shift\naffects the SSL training process. In particular, we find that during training\nthe statistics of hidden activations in late layers become markedly different\nbetween the unlabeled and the labeled examples.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=960&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Recent work has shown that using unlabeled data in semi-supervised learning\nis not always beneficial and can even hurt generalization, especially when\nthere is a class mismatch between the unlabeled and labeled examples. We\ninvestigate this phenomenon for image classification on the CIFAR-10 and the\nImageNet datasets, and with many other forms of domain shifts applied (e.g.\nsalt-and-pepper noise). Our main contribution is Split Batch Normalization\n(Split-BN), a technique to improve SSL when the additional unlabeled data comes\nfrom a shifted distribution. We achieve it by using separate batch\nnormalization statistics for unlabeled examples. Due to its simplicity, we\nrecommend it as a standard practice. Finally, we analyse how domain shift\naffects the SSL training process. In particular, we find that during training\nthe statistics of hidden activations in late layers become markedly different\nbetween the unlabeled and the labeled examples."}, "authors": ["Micha Zajc", "Konrad Zolna", "Stanisaw Jastrzbski"], "author_detail": {"name": "Stanisaw Jastrzbski"}, "author": "Stanisaw Jastrzbski", "arxiv_comment": "Under review for ECML PKDD 2019", "links": [{"href": "http://arxiv.org/abs/1904.03515v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1904.03515v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.ML", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1904.03515v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1904.03515v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1907.08009v1", "guidislink": true, "updated": "2019-07-18T12:03:12Z", "updated_parsed": [2019, 7, 18, 12, 3, 12, 3, 199, 0], "published": "2019-07-18T12:03:12Z", "published_parsed": [2019, 7, 18, 12, 3, 12, 3, 199, 0], "title": "Real-Time Driver State Monitoring Using a CNN Based Spatio-Temporal\n  Approach", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=960&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Real-Time Driver State Monitoring Using a CNN Based Spatio-Temporal\n  Approach"}, "summary": "Many road accidents occur due to distracted drivers. Today, driver monitoring\nis essential even for the latest autonomous vehicles to alert distracted\ndrivers in order to take over control of the vehicle in case of emergency. In\nthis paper, a spatio-temporal approach is applied to classify drivers'\ndistraction level and movement decisions using convolutional neural networks\n(CNNs). We approach this problem as action recognition to benefit from temporal\ninformation in addition to spatial information. Our approach relies on features\nextracted from sparsely selected frames of an action using a pre-trained\nBN-Inception network. Experiments show that our approach outperforms the\nstate-of-the art results on the Distracted Driver Dataset (96.31%), with an\naccuracy of 99.10% for 10-class classification while providing real-time\nperformance. We also analyzed the impact of fusion using RGB and optical flow\nmodalities with a very recent data level fusion strategy. The results on the\nDistracted Driver and Brain4Cars datasets show that fusion of these modalities\nfurther increases the accuracy.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=960&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Many road accidents occur due to distracted drivers. Today, driver monitoring\nis essential even for the latest autonomous vehicles to alert distracted\ndrivers in order to take over control of the vehicle in case of emergency. In\nthis paper, a spatio-temporal approach is applied to classify drivers'\ndistraction level and movement decisions using convolutional neural networks\n(CNNs). We approach this problem as action recognition to benefit from temporal\ninformation in addition to spatial information. Our approach relies on features\nextracted from sparsely selected frames of an action using a pre-trained\nBN-Inception network. Experiments show that our approach outperforms the\nstate-of-the art results on the Distracted Driver Dataset (96.31%), with an\naccuracy of 99.10% for 10-class classification while providing real-time\nperformance. We also analyzed the impact of fusion using RGB and optical flow\nmodalities with a very recent data level fusion strategy. The results on the\nDistracted Driver and Brain4Cars datasets show that fusion of these modalities\nfurther increases the accuracy."}, "authors": ["Neslihan Kose", "Okan Kopuklu", "Alexander Unnervik", "Gerhard Rigoll"], "author_detail": {"name": "Gerhard Rigoll"}, "author": "Gerhard Rigoll", "arxiv_comment": "Accepted for publication by the IEEE Intelligent Transportation\n  Systems Conference (ITSC 2019)", "links": [{"href": "http://arxiv.org/abs/1907.08009v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1907.08009v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "eess.IV", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1907.08009v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1907.08009v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/2001.05887v3", "guidislink": true, "updated": "2020-03-10T10:47:27Z", "updated_parsed": [2020, 3, 10, 10, 47, 27, 1, 70, 0], "published": "2020-01-16T15:24:26Z", "published_parsed": [2020, 1, 16, 15, 24, 26, 3, 16, 0], "title": "MixPath: A Unified Approach for One-shot Neural Architecture Search", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=970&max_results=10&sortBy=relevance&sortOrder=descending", "value": "MixPath: A Unified Approach for One-shot Neural Architecture Search"}, "summary": "Blending multiple convolutional kernels is proved advantageous in neural\narchitectural design. However, current neural architecture search approaches\nare mainly limited to stacked single-path search space. How can the one-shot\ndoctrine search for multi-path models remains unresolved. Specifically, we are\nmotivated to train a multi-path supernet to accurately evaluate the candidate\narchitectures. In this paper, we discover that in the studied search space,\nfeature vectors summed from multiple paths are nearly multiples of those from a\nsingle path, which perturbs supernet training and its ranking ability. In this\nregard, we propose a novel mechanism called Shadow Batch Normalization(SBN) to\nregularize the disparate feature statistics. Extensive experiments prove that\nSBN is capable of stabilizing the training and improving the ranking\nperformance (e.g. Kendall Tau 0.597 tested on NAS-Bench-101). We call our\nunified multi-path one-shot approach as MixPath, which generates a series of\nmodels that achieve state-of-the-art results on ImageNet.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=970&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Blending multiple convolutional kernels is proved advantageous in neural\narchitectural design. However, current neural architecture search approaches\nare mainly limited to stacked single-path search space. How can the one-shot\ndoctrine search for multi-path models remains unresolved. Specifically, we are\nmotivated to train a multi-path supernet to accurately evaluate the candidate\narchitectures. In this paper, we discover that in the studied search space,\nfeature vectors summed from multiple paths are nearly multiples of those from a\nsingle path, which perturbs supernet training and its ranking ability. In this\nregard, we propose a novel mechanism called Shadow Batch Normalization(SBN) to\nregularize the disparate feature statistics. Extensive experiments prove that\nSBN is capable of stabilizing the training and improving the ranking\nperformance (e.g. Kendall Tau 0.597 tested on NAS-Bench-101). We call our\nunified multi-path one-shot approach as MixPath, which generates a series of\nmodels that achieve state-of-the-art results on ImageNet."}, "authors": ["Xiangxiang Chu", "Xudong Li", "Shun Lu", "Bo Zhang", "Jixiang Li"], "author_detail": {"name": "Jixiang Li"}, "author": "Jixiang Li", "arxiv_comment": "Bridge the gap between one shot NAS and multi branch using shadow BN\n  with good ranking", "links": [{"href": "http://arxiv.org/abs/2001.05887v3", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2001.05887v3", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.ML", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2001.05887v3", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2001.05887v3", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/2003.00214v1", "guidislink": true, "updated": "2020-02-29T09:02:31Z", "updated_parsed": [2020, 2, 29, 9, 2, 31, 5, 60, 0], "published": "2020-02-29T09:02:31Z", "published_parsed": [2020, 2, 29, 9, 2, 31, 5, 60, 0], "title": "Channel Equilibrium Networks for Learning Deep Representation", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=980&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Channel Equilibrium Networks for Learning Deep Representation"}, "summary": "Convolutional Neural Networks (CNNs) are typically constructed by stacking\nmultiple building blocks, each of which contains a normalization layer such as\nbatch normalization (BN) and a rectified linear function such as ReLU. However,\nthis work shows that the combination of normalization and rectified linear\nfunction leads to inhibited channels, which have small magnitude and contribute\nlittle to the learned feature representation, impeding the generalization\nability of CNNs. Unlike prior arts that simply removed the inhibited channels,\nwe propose to \"wake them up\" during training by designing a novel neural\nbuilding block, termed Channel Equilibrium (CE) block, which enables channels\nat the same layer to contribute equally to the learned representation. We show\nthat CE is able to prevent inhibited channels both empirically and\ntheoretically. CE has several appealing benefits. (1) It can be integrated into\nmany advanced CNN architectures such as ResNet and MobileNet, outperforming\ntheir original networks. (2) CE has an interesting connection with the Nash\nEquilibrium, a well-known solution of a non-cooperative game. (3) Extensive\nexperiments show that CE achieves state-of-the-art performance on various\nchallenging benchmarks such as ImageNet and COCO.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=980&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Convolutional Neural Networks (CNNs) are typically constructed by stacking\nmultiple building blocks, each of which contains a normalization layer such as\nbatch normalization (BN) and a rectified linear function such as ReLU. However,\nthis work shows that the combination of normalization and rectified linear\nfunction leads to inhibited channels, which have small magnitude and contribute\nlittle to the learned feature representation, impeding the generalization\nability of CNNs. Unlike prior arts that simply removed the inhibited channels,\nwe propose to \"wake them up\" during training by designing a novel neural\nbuilding block, termed Channel Equilibrium (CE) block, which enables channels\nat the same layer to contribute equally to the learned representation. We show\nthat CE is able to prevent inhibited channels both empirically and\ntheoretically. CE has several appealing benefits. (1) It can be integrated into\nmany advanced CNN architectures such as ResNet and MobileNet, outperforming\ntheir original networks. (2) CE has an interesting connection with the Nash\nEquilibrium, a well-known solution of a non-cooperative game. (3) Extensive\nexperiments show that CE achieves state-of-the-art performance on various\nchallenging benchmarks such as ImageNet and COCO."}, "authors": ["Wenqi Shao", "Shitao Tang", "Xingang Pan", "Ping Tan", "Xiaogang Wang", "Ping Luo"], "author_detail": {"name": "Ping Luo"}, "author": "Ping Luo", "arxiv_comment": "19 pages, 8 figures", "links": [{"href": "http://arxiv.org/abs/2003.00214v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2003.00214v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2003.00214v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2003.00214v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/2003.05569v1", "guidislink": true, "updated": "2020-03-12T01:53:15Z", "updated_parsed": [2020, 3, 12, 1, 53, 15, 3, 72, 0], "published": "2020-03-12T01:53:15Z", "published_parsed": [2020, 3, 12, 1, 53, 15, 3, 72, 0], "title": "Extended Batch Normalization", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=980&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Extended Batch Normalization"}, "summary": "Batch normalization (BN) has become a standard technique for training the\nmodern deep networks. However, its effectiveness diminishes when the batch size\nbecomes smaller, since the batch statistics estimation becomes inaccurate. That\nhinders batch normalization's usage for 1) training larger model which requires\nsmall batches constrained by memory consumption, 2) training on mobile or\nembedded devices of which the memory resource is limited. In this paper, we\npropose a simple but effective method, called extended batch normalization\n(EBN). For NCHW format feature maps, extended batch normalization computes the\nmean along the (N, H, W) dimensions, as the same as batch normalization, to\nmaintain the advantage of batch normalization. To alleviate the problem caused\nby small batch size, extended batch normalization computes the standard\ndeviation along the (N, C, H, W) dimensions, thus enlarges the number of\nsamples from which the standard deviation is computed. We compare extended\nbatch normalization with batch normalization and group normalization on the\ndatasets of MNIST, CIFAR-10/100, STL-10, and ImageNet, respectively. The\nexperiments show that extended batch normalization alleviates the problem of\nbatch normalization with small batch size while achieving close performances to\nbatch normalization with large batch size.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=980&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Batch normalization (BN) has become a standard technique for training the\nmodern deep networks. However, its effectiveness diminishes when the batch size\nbecomes smaller, since the batch statistics estimation becomes inaccurate. That\nhinders batch normalization's usage for 1) training larger model which requires\nsmall batches constrained by memory consumption, 2) training on mobile or\nembedded devices of which the memory resource is limited. In this paper, we\npropose a simple but effective method, called extended batch normalization\n(EBN). For NCHW format feature maps, extended batch normalization computes the\nmean along the (N, H, W) dimensions, as the same as batch normalization, to\nmaintain the advantage of batch normalization. To alleviate the problem caused\nby small batch size, extended batch normalization computes the standard\ndeviation along the (N, C, H, W) dimensions, thus enlarges the number of\nsamples from which the standard deviation is computed. We compare extended\nbatch normalization with batch normalization and group normalization on the\ndatasets of MNIST, CIFAR-10/100, STL-10, and ImageNet, respectively. The\nexperiments show that extended batch normalization alleviates the problem of\nbatch normalization with small batch size while achieving close performances to\nbatch normalization with large batch size."}, "authors": ["Chunjie Luo", "Jianfeng Zhan", "Lei Wang", "Wanling Gao"], "author_detail": {"name": "Wanling Gao"}, "author": "Wanling Gao", "links": [{"href": "http://arxiv.org/abs/2003.05569v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2003.05569v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2003.05569v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2003.05569v1", "arxiv_comment": null, "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/2003.08109v3", "guidislink": true, "updated": "2020-11-03T08:01:06Z", "updated_parsed": [2020, 11, 3, 8, 1, 6, 1, 308, 0], "published": "2020-03-18T09:16:14Z", "published_parsed": [2020, 3, 18, 9, 16, 14, 2, 78, 0], "title": "Efficient improper learning for online logistic regression", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=980&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Efficient improper learning for online logistic regression"}, "summary": "We consider the setting of online logistic regression and consider the regret\nwith respect to the 2-ball of radius B. It is known (see [Hazan et al., 2014])\nthat any proper algorithm which has logarithmic regret in the number of samples\n(denoted n) necessarily suffers an exponential multiplicative constant in B. In\nthis work, we design an efficient improper algorithm that avoids this\nexponential constant while preserving a logarithmic regret. Indeed, [Foster et\nal., 2018] showed that the lower bound does not apply to improper algorithms\nand proposed a strategy based on exponential weights with prohibitive\ncomputational complexity. Our new algorithm based on regularized empirical risk\nminimization with surrogate losses satisfies a regret scaling as O(B log(Bn))\nwith a per-round time-complexity of order O(d^2).", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=980&max_results=10&sortBy=relevance&sortOrder=descending", "value": "We consider the setting of online logistic regression and consider the regret\nwith respect to the 2-ball of radius B. It is known (see [Hazan et al., 2014])\nthat any proper algorithm which has logarithmic regret in the number of samples\n(denoted n) necessarily suffers an exponential multiplicative constant in B. In\nthis work, we design an efficient improper algorithm that avoids this\nexponential constant while preserving a logarithmic regret. Indeed, [Foster et\nal., 2018] showed that the lower bound does not apply to improper algorithms\nand proposed a strategy based on exponential weights with prohibitive\ncomputational complexity. Our new algorithm based on regularized empirical risk\nminimization with surrogate losses satisfies a regret scaling as O(B log(Bn))\nwith a per-round time-complexity of order O(d^2)."}, "authors": ["Rmi Jzquel", "Pierre Gaillard", "Alessandro Rudi"], "author_detail": {"name": "Alessandro Rudi"}, "author": "Alessandro Rudi", "links": [{"href": "http://arxiv.org/abs/2003.08109v3", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2003.08109v3", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "math.ST", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.ML", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.TH", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2003.08109v3", "affiliation": "SIERRA", "arxiv_url": "http://arxiv.org/abs/2003.08109v3", "arxiv_comment": null, "journal_reference": "Conference on Learning Theory 2020, Jul 2020, Graz, Austria", "doi": null}
{"id": "http://arxiv.org/abs/2003.12327v1", "guidislink": true, "updated": "2020-03-27T11:06:32Z", "updated_parsed": [2020, 3, 27, 11, 6, 32, 4, 87, 0], "published": "2020-03-27T11:06:32Z", "published_parsed": [2020, 3, 27, 11, 6, 32, 4, 87, 0], "title": "An Investigation into the Stochasticity of Batch Whitening", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=980&max_results=10&sortBy=relevance&sortOrder=descending", "value": "An Investigation into the Stochasticity of Batch Whitening"}, "summary": "Batch Normalization (BN) is extensively employed in various network\narchitectures by performing standardization within mini-batches.\n  A full understanding of the process has been a central target in the deep\nlearning communities.\n  Unlike existing works, which usually only analyze the standardization\noperation, this paper investigates the more general Batch Whitening (BW). Our\nwork originates from the observation that while various whitening\ntransformations equivalently improve the conditioning, they show significantly\ndifferent behaviors in discriminative scenarios and training Generative\nAdversarial Networks (GANs).\n  We attribute this phenomenon to the stochasticity that BW introduces.\n  We quantitatively investigate the stochasticity of different whitening\ntransformations and show that it correlates well with the optimization\nbehaviors during training.\n  We also investigate how stochasticity relates to the estimation of population\nstatistics during inference.\n  Based on our analysis, we provide a framework for designing and comparing BW\nalgorithms in different scenarios.\n  Our proposed BW algorithm improves the residual networks by a significant\nmargin on ImageNet classification.\n  Besides, we show that the stochasticity of BW can improve the GAN's\nperformance with, however, the sacrifice of the training stability.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=980&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Batch Normalization (BN) is extensively employed in various network\narchitectures by performing standardization within mini-batches.\n  A full understanding of the process has been a central target in the deep\nlearning communities.\n  Unlike existing works, which usually only analyze the standardization\noperation, this paper investigates the more general Batch Whitening (BW). Our\nwork originates from the observation that while various whitening\ntransformations equivalently improve the conditioning, they show significantly\ndifferent behaviors in discriminative scenarios and training Generative\nAdversarial Networks (GANs).\n  We attribute this phenomenon to the stochasticity that BW introduces.\n  We quantitatively investigate the stochasticity of different whitening\ntransformations and show that it correlates well with the optimization\nbehaviors during training.\n  We also investigate how stochasticity relates to the estimation of population\nstatistics during inference.\n  Based on our analysis, we provide a framework for designing and comparing BW\nalgorithms in different scenarios.\n  Our proposed BW algorithm improves the residual networks by a significant\nmargin on ImageNet classification.\n  Besides, we show that the stochasticity of BW can improve the GAN's\nperformance with, however, the sacrifice of the training stability."}, "authors": ["Lei Huang", "Lei Zhao", "Yi Zhou", "Fan Zhu", "Li Liu", "Ling Shao"], "author_detail": {"name": "Ling Shao"}, "author": "Ling Shao", "arxiv_comment": "Accepted to CVPR 2020. The Code is available at\n  https://github.com/huangleiBuaa/StochasticityBW", "links": [{"href": "http://arxiv.org/abs/2003.12327v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2003.12327v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2003.12327v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2003.12327v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/2004.12381v2", "guidislink": true, "updated": "2020-05-12T01:56:40Z", "updated_parsed": [2020, 5, 12, 1, 56, 40, 1, 133, 0], "published": "2020-04-26T13:46:52Z", "published_parsed": [2020, 4, 26, 13, 46, 52, 6, 117, 0], "title": "Hyperspectral Images Classification Based on Multi-scale Residual\n  Network", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=980&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Hyperspectral Images Classification Based on Multi-scale Residual\n  Network"}, "summary": "Because hyperspectral remote sensing images contain a lot of redundant\ninformation and the data structure is highly non-linear, leading to low\nclassification accuracy of traditional machine learning methods. The latest\nresearch shows that hyperspectral image classification based on deep\nconvolutional neural network has high accuracy. However, when a small amount of\ndata is used for training, the classification accuracy of deep learning methods\nis greatly reduced. In order to solve the problem of low classification\naccuracy of existing algorithms on small samples of hyperspectral images, a\nmulti-scale residual network is proposed. The multi-scale extraction and fusion\nof spatial and spectral features is realized by adding a branch structure into\nthe residual block and using convolution kernels of different sizes in the\nbranch. The spatial and spectral information contained in hyperspectral images\nare fully utilized to improve the classification accuracy. In addition, in\norder to improve the speed and prevent overfitting, the model uses dynamic\nlearning rate, BN and Dropout strategies. The experimental results show that\nthe overall classification accuracy of this method is 99.07% and 99.96%\nrespectively in the data set of Indian Pines and Pavia University, which is\nbetter than other algorithms.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=980&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Because hyperspectral remote sensing images contain a lot of redundant\ninformation and the data structure is highly non-linear, leading to low\nclassification accuracy of traditional machine learning methods. The latest\nresearch shows that hyperspectral image classification based on deep\nconvolutional neural network has high accuracy. However, when a small amount of\ndata is used for training, the classification accuracy of deep learning methods\nis greatly reduced. In order to solve the problem of low classification\naccuracy of existing algorithms on small samples of hyperspectral images, a\nmulti-scale residual network is proposed. The multi-scale extraction and fusion\nof spatial and spectral features is realized by adding a branch structure into\nthe residual block and using convolution kernels of different sizes in the\nbranch. The spatial and spectral information contained in hyperspectral images\nare fully utilized to improve the classification accuracy. In addition, in\norder to improve the speed and prevent overfitting, the model uses dynamic\nlearning rate, BN and Dropout strategies. The experimental results show that\nthe overall classification accuracy of this method is 99.07% and 99.96%\nrespectively in the data set of Indian Pines and Pavia University, which is\nbetter than other algorithms."}, "authors": ["Xiangdong Zhang", "Tengjun Wang", "Yun Yang"], "author_detail": {"name": "Yun Yang"}, "author": "Yun Yang", "links": [{"href": "http://arxiv.org/abs/2004.12381v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2004.12381v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2004.12381v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2004.12381v2", "arxiv_comment": null, "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/2005.06828v1", "guidislink": true, "updated": "2020-05-14T09:16:13Z", "updated_parsed": [2020, 5, 14, 9, 16, 13, 3, 135, 0], "published": "2020-05-14T09:16:13Z", "published_parsed": [2020, 5, 14, 9, 16, 13, 3, 135, 0], "title": "Finet: Using Fine-grained Batch Normalization to Train Light-weight\n  Neural Networks", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=990&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Finet: Using Fine-grained Batch Normalization to Train Light-weight\n  Neural Networks"}, "summary": "To build light-weight network, we propose a new normalization, Fine-grained\nBatch Normalization (FBN). Different from Batch Normalization (BN), which\nnormalizes the final summation of the weighted inputs, FBN normalizes the\nintermediate state of the summation. We propose a novel light-weight network\nbased on FBN, called Finet. At training time, the convolutional layer with FBN\ncan be seen as an inverted bottleneck mechanism. FBN can be fused into\nconvolution at inference time. After fusion, Finet uses the standard\nconvolution with equal channel width, thus makes the inference more efficient.\nOn ImageNet classification dataset, Finet achieves the state-of-art performance\n(65.706% accuracy with 43M FLOPs, and 73.786% accuracy with 303M FLOPs),\nMoreover, experiments show that Finet is more efficient than other state-of-art\nlight-weight networks.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=990&max_results=10&sortBy=relevance&sortOrder=descending", "value": "To build light-weight network, we propose a new normalization, Fine-grained\nBatch Normalization (FBN). Different from Batch Normalization (BN), which\nnormalizes the final summation of the weighted inputs, FBN normalizes the\nintermediate state of the summation. We propose a novel light-weight network\nbased on FBN, called Finet. At training time, the convolutional layer with FBN\ncan be seen as an inverted bottleneck mechanism. FBN can be fused into\nconvolution at inference time. After fusion, Finet uses the standard\nconvolution with equal channel width, thus makes the inference more efficient.\nOn ImageNet classification dataset, Finet achieves the state-of-art performance\n(65.706% accuracy with 43M FLOPs, and 73.786% accuracy with 303M FLOPs),\nMoreover, experiments show that Finet is more efficient than other state-of-art\nlight-weight networks."}, "authors": ["Chunjie Luo", "Jianfeng Zhan", "Lei Wang", "Wanling Gao"], "author_detail": {"name": "Wanling Gao"}, "author": "Wanling Gao", "links": [{"href": "http://arxiv.org/abs/2005.06828v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2005.06828v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.ML", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2005.06828v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2005.06828v1", "arxiv_comment": null, "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/2006.04753v2", "guidislink": true, "updated": "2020-09-10T15:48:29Z", "updated_parsed": [2020, 9, 10, 15, 48, 29, 3, 254, 0], "published": "2020-06-08T17:09:18Z", "published_parsed": [2020, 6, 8, 17, 9, 18, 0, 160, 0], "title": "Approximate learning of high dimensional Bayesian network structures via\n  pruning of Candidate Parent Sets", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=990&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Approximate learning of high dimensional Bayesian network structures via\n  pruning of Candidate Parent Sets"}, "summary": "Score-based algorithms that learn Bayesian Network (BN) structures provide\nsolutions ranging from different levels of approximate learning to exact\nlearning. Approximate solutions exist because exact learning is generally not\napplicable to networks of moderate or higher complexity. In general,\napproximate solutions tend to sacrifice accuracy for speed, where the aim is to\nminimise the loss in accuracy and maximise the gain in speed. While some\napproximate algorithms are optimised to handle thousands of variables, these\nalgorithms may still be unable to learn such high dimensional structures. Some\nof the most efficient score-based algorithms cast the structure learning\nproblem as a combinatorial optimisation of candidate parent sets. This paper\nexplores a strategy towards pruning the size of candidate parent sets, aimed at\nhigh dimensionality problems. The results illustrate how different levels of\npruning affect the learning speed relative to the loss in accuracy in terms of\nmodel fitting, and show that aggressive pruning may be required to produce\napproximate solutions for high complexity problems.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=990&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Score-based algorithms that learn Bayesian Network (BN) structures provide\nsolutions ranging from different levels of approximate learning to exact\nlearning. Approximate solutions exist because exact learning is generally not\napplicable to networks of moderate or higher complexity. In general,\napproximate solutions tend to sacrifice accuracy for speed, where the aim is to\nminimise the loss in accuracy and maximise the gain in speed. While some\napproximate algorithms are optimised to handle thousands of variables, these\nalgorithms may still be unable to learn such high dimensional structures. Some\nof the most efficient score-based algorithms cast the structure learning\nproblem as a combinatorial optimisation of candidate parent sets. This paper\nexplores a strategy towards pruning the size of candidate parent sets, aimed at\nhigh dimensionality problems. The results illustrate how different levels of\npruning affect the learning speed relative to the loss in accuracy in terms of\nmodel fitting, and show that aggressive pruning may be required to produce\napproximate solutions for high complexity problems."}, "authors": ["Zhigao Guo", "Anthony C. Constantinou"], "author_detail": {"name": "Anthony C. Constantinou"}, "author": "Anthony C. Constantinou", "links": [{"title": "doi", "href": "http://dx.doi.org/10.3390/e22101142", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/2006.04753v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2006.04753v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.GR", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2006.04753v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2006.04753v2", "arxiv_comment": null, "journal_reference": null, "doi": "10.3390/e22101142"}
{"id": "http://arxiv.org/abs/2006.09104v1", "guidislink": true, "updated": "2020-06-16T12:26:13Z", "updated_parsed": [2020, 6, 16, 12, 26, 13, 1, 168, 0], "published": "2020-06-16T12:26:13Z", "published_parsed": [2020, 6, 16, 12, 26, 13, 1, 168, 0], "title": "New Interpretations of Normalization Methods in Deep Learning", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=990&max_results=10&sortBy=relevance&sortOrder=descending", "value": "New Interpretations of Normalization Methods in Deep Learning"}, "summary": "In recent years, a variety of normalization methods have been proposed to\nhelp train neural networks, such as batch normalization (BN), layer\nnormalization (LN), weight normalization (WN), group normalization (GN), etc.\nHowever, mathematical tools to analyze all these normalization methods are\nlacking. In this paper, we first propose a lemma to define some necessary\ntools. Then, we use these tools to make a deep analysis on popular\nnormalization methods and obtain the following conclusions: 1) Most of the\nnormalization methods can be interpreted in a unified framework, namely\nnormalizing pre-activations or weights onto a sphere; 2) Since most of the\nexisting normalization methods are scaling invariant, we can conduct\noptimization on a sphere with scaling symmetry removed, which can help\nstabilize the training of network; 3) We prove that training with these\nnormalization methods can make the norm of weights increase, which could cause\nadversarial vulnerability as it amplifies the attack. Finally, a series of\nexperiments are conducted to verify these claims.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=990&max_results=10&sortBy=relevance&sortOrder=descending", "value": "In recent years, a variety of normalization methods have been proposed to\nhelp train neural networks, such as batch normalization (BN), layer\nnormalization (LN), weight normalization (WN), group normalization (GN), etc.\nHowever, mathematical tools to analyze all these normalization methods are\nlacking. In this paper, we first propose a lemma to define some necessary\ntools. Then, we use these tools to make a deep analysis on popular\nnormalization methods and obtain the following conclusions: 1) Most of the\nnormalization methods can be interpreted in a unified framework, namely\nnormalizing pre-activations or weights onto a sphere; 2) Since most of the\nexisting normalization methods are scaling invariant, we can conduct\noptimization on a sphere with scaling symmetry removed, which can help\nstabilize the training of network; 3) We prove that training with these\nnormalization methods can make the norm of weights increase, which could cause\nadversarial vulnerability as it amplifies the attack. Finally, a series of\nexperiments are conducted to verify these claims."}, "authors": ["Jiacheng Sun", "Xiangyong Cao", "Hanwen Liang", "Weiran Huang", "Zewei Chen", "Zhenguo Li"], "author_detail": {"name": "Zhenguo Li"}, "author": "Zhenguo Li", "arxiv_comment": "Accepted by AAAI 2020", "links": [{"href": "http://arxiv.org/abs/2006.09104v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2006.09104v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.ML", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2006.09104v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2006.09104v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/2006.15865v1", "guidislink": true, "updated": "2020-06-29T08:24:57Z", "updated_parsed": [2020, 6, 29, 8, 24, 57, 0, 181, 0], "published": "2020-06-29T08:24:57Z", "published_parsed": [2020, 6, 29, 8, 24, 57, 0, 181, 0], "title": "Propagation for Dynamic Continuous Time Chain Event Graphs", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=990&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Propagation for Dynamic Continuous Time Chain Event Graphs"}, "summary": "Chain Event Graphs (CEGs) are a family of event-based graphical models that\nrepresent context-specific conditional independences typically exhibited by\nasymmetric state space problems. The class of continuous time dynamic CEGs\n(CT-DCEGs) provides a factored representation of longitudinally evolving\ntrajectories of a process in continuous time. Temporal evidence in a CT-DCEG\nintroduces dependence between its transition and holding time distributions. We\npresent a tractable exact inferential scheme analogous to the scheme in\nKj{\\ae}rulff (1992) for discrete Dynamic Bayesian Networks (DBNs) which employs\nstandard junction tree inference by \"unrolling\" the DBN. To enable this scheme,\nwe present an extension of the standard CEG propagation algorithm (Thwaites et\nal., 2008). Interestingly, the CT-DCEG benefits from simplification of its\ngraph on observing compatible evidence while preserving the still relevant\nsymmetries within the asymmetric network. Our results indicate that the CT-DCEG\nis preferred to DBNs and continuous time BNs under contexts involving\nsignificant asymmetry and a natural total ordering of the process evolution.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=990&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Chain Event Graphs (CEGs) are a family of event-based graphical models that\nrepresent context-specific conditional independences typically exhibited by\nasymmetric state space problems. The class of continuous time dynamic CEGs\n(CT-DCEGs) provides a factored representation of longitudinally evolving\ntrajectories of a process in continuous time. Temporal evidence in a CT-DCEG\nintroduces dependence between its transition and holding time distributions. We\npresent a tractable exact inferential scheme analogous to the scheme in\nKj{\\ae}rulff (1992) for discrete Dynamic Bayesian Networks (DBNs) which employs\nstandard junction tree inference by \"unrolling\" the DBN. To enable this scheme,\nwe present an extension of the standard CEG propagation algorithm (Thwaites et\nal., 2008). Interestingly, the CT-DCEG benefits from simplification of its\ngraph on observing compatible evidence while preserving the still relevant\nsymmetries within the asymmetric network. Our results indicate that the CT-DCEG\nis preferred to DBNs and continuous time BNs under contexts involving\nsignificant asymmetry and a natural total ordering of the process evolution."}, "authors": ["Aditi Shenvi", "Jim Q. Smith"], "author_detail": {"name": "Jim Q. Smith"}, "author": "Jim Q. Smith", "links": [{"href": "http://arxiv.org/abs/2006.15865v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2006.15865v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.ML", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2006.15865v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2006.15865v1", "arxiv_comment": null, "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/2007.03938v2", "guidislink": true, "updated": "2020-07-22T02:33:59Z", "updated_parsed": [2020, 7, 22, 2, 33, 59, 2, 204, 0], "published": "2020-07-08T07:44:00Z", "published_parsed": [2020, 7, 8, 7, 44, 0, 2, 190, 0], "title": "Operation-Aware Soft Channel Pruning using Differentiable Masks", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=990&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Operation-Aware Soft Channel Pruning using Differentiable Masks"}, "summary": "We propose a simple but effective data-driven channel pruning algorithm,\nwhich compresses deep neural networks in a differentiable way by exploiting the\ncharacteristics of operations. The proposed approach makes a joint\nconsideration of batch normalization (BN) and rectified linear unit (ReLU) for\nchannel pruning; it estimates how likely the two successive operations\ndeactivate each feature map and prunes the channels with high probabilities. To\nthis end, we learn differentiable masks for individual channels and make soft\ndecisions throughout the optimization procedure, which facilitates to explore\nlarger search space and train more stable networks. The proposed framework\nenables us to identify compressed models via a joint learning of model\nparameters and channel pruning without an extra procedure of fine-tuning. We\nperform extensive experiments and achieve outstanding performance in terms of\nthe accuracy of output networks given the same amount of resources when\ncompared with the state-of-the-art methods.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=990&max_results=10&sortBy=relevance&sortOrder=descending", "value": "We propose a simple but effective data-driven channel pruning algorithm,\nwhich compresses deep neural networks in a differentiable way by exploiting the\ncharacteristics of operations. The proposed approach makes a joint\nconsideration of batch normalization (BN) and rectified linear unit (ReLU) for\nchannel pruning; it estimates how likely the two successive operations\ndeactivate each feature map and prunes the channels with high probabilities. To\nthis end, we learn differentiable masks for individual channels and make soft\ndecisions throughout the optimization procedure, which facilitates to explore\nlarger search space and train more stable networks. The proposed framework\nenables us to identify compressed models via a joint learning of model\nparameters and channel pruning without an extra procedure of fine-tuning. We\nperform extensive experiments and achieve outstanding performance in terms of\nthe accuracy of output networks given the same amount of resources when\ncompared with the state-of-the-art methods."}, "authors": ["Minsoo Kang", "Bohyung Han"], "author_detail": {"name": "Bohyung Han"}, "author": "Bohyung Han", "arxiv_comment": "ICML 2020", "links": [{"href": "http://arxiv.org/abs/2007.03938v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2007.03938v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.ML", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2007.03938v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2007.03938v2", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/2008.07424v1", "guidislink": true, "updated": "2020-08-17T15:49:30Z", "updated_parsed": [2020, 8, 17, 15, 49, 30, 0, 230, 0], "published": "2020-08-17T15:49:30Z", "published_parsed": [2020, 8, 17, 15, 49, 30, 0, 230, 0], "title": "Siloed Federated Learning for Multi-Centric Histopathology Datasets", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=990&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Siloed Federated Learning for Multi-Centric Histopathology Datasets"}, "summary": "While federated learning is a promising approach for training deep learning\nmodels over distributed sensitive datasets, it presents new challenges for\nmachine learning, especially when applied in the medical domain where\nmulti-centric data heterogeneity is common. Building on previous domain\nadaptation works, this paper proposes a novel federated learning approach for\ndeep learning architectures via the introduction of local-statistic batch\nnormalization (BN) layers, resulting in collaboratively-trained, yet\ncenter-specific models. This strategy improves robustness to data heterogeneity\nwhile also reducing the potential for information leaks by not sharing the\ncenter-specific layer activation statistics. We benchmark the proposed method\non the classification of tumorous histopathology image patches extracted from\nthe Camelyon16 and Camelyon17 datasets. We show that our approach compares\nfavorably to previous state-of-the-art methods, especially for transfer\nlearning across datasets.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=990&max_results=10&sortBy=relevance&sortOrder=descending", "value": "While federated learning is a promising approach for training deep learning\nmodels over distributed sensitive datasets, it presents new challenges for\nmachine learning, especially when applied in the medical domain where\nmulti-centric data heterogeneity is common. Building on previous domain\nadaptation works, this paper proposes a novel federated learning approach for\ndeep learning architectures via the introduction of local-statistic batch\nnormalization (BN) layers, resulting in collaboratively-trained, yet\ncenter-specific models. This strategy improves robustness to data heterogeneity\nwhile also reducing the potential for information leaks by not sharing the\ncenter-specific layer activation statistics. We benchmark the proposed method\non the classification of tumorous histopathology image patches extracted from\nthe Camelyon16 and Camelyon17 datasets. We show that our approach compares\nfavorably to previous state-of-the-art methods, especially for transfer\nlearning across datasets."}, "authors": ["Mathieu Andreux", "Jean Ogier du Terrail", "Constance Beguier", "Eric W. Tramel"], "author_detail": {"name": "Eric W. Tramel"}, "author": "Eric W. Tramel", "arxiv_comment": "Accepted to MICCAI 2020 DCL workshop", "links": [{"href": "http://arxiv.org/abs/2008.07424v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2008.07424v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "eess.IV", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2008.07424v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2008.07424v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/2009.05244v1", "guidislink": true, "updated": "2020-09-11T06:07:14Z", "updated_parsed": [2020, 9, 11, 6, 7, 14, 4, 255, 0], "published": "2020-09-11T06:07:14Z", "published_parsed": [2020, 9, 11, 6, 7, 14, 4, 255, 0], "title": "Defending Against Multiple and Unforeseen Adversarial Videos", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=1000&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Defending Against Multiple and Unforeseen Adversarial Videos"}, "summary": "Adversarial examples of deep neural networks have been actively investigated\non image-based classification, segmentation and detection tasks. However,\nadversarial robustness of video models still lacks exploration. While several\nstudies have proposed how to generate adversarial videos, only a handful of\napproaches pertaining to the defense strategies have been published in the\nliterature. Furthermore, these defense methods are limited to a single\nperturbation type and often fail to provide robustness to Lp-bounded attacks\nand physically realizable attacks simultaneously. In this paper, we propose one\nof the first defense solutions against multiple adversarial video types for\nvideo classification. The proposed approach performs adversarial training with\nmultiple types of video adversaries using independent batch normalizations\n(BNs), and recognizes different adversaries by an adversarial video detector.\nDuring inference, a switch module sends an input to a proper batch\nnormalization branch according to the detected attack type. Compared to\nconventional adversarial training, our method exhibits stronger robustness to\nmultiple and even unforeseen adversarial videos and provides higher\nclassification accuracy.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=1000&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Adversarial examples of deep neural networks have been actively investigated\non image-based classification, segmentation and detection tasks. However,\nadversarial robustness of video models still lacks exploration. While several\nstudies have proposed how to generate adversarial videos, only a handful of\napproaches pertaining to the defense strategies have been published in the\nliterature. Furthermore, these defense methods are limited to a single\nperturbation type and often fail to provide robustness to Lp-bounded attacks\nand physically realizable attacks simultaneously. In this paper, we propose one\nof the first defense solutions against multiple adversarial video types for\nvideo classification. The proposed approach performs adversarial training with\nmultiple types of video adversaries using independent batch normalizations\n(BNs), and recognizes different adversaries by an adversarial video detector.\nDuring inference, a switch module sends an input to a proper batch\nnormalization branch according to the detected attack type. Compared to\nconventional adversarial training, our method exhibits stronger robustness to\nmultiple and even unforeseen adversarial videos and provides higher\nclassification accuracy."}, "authors": ["Shao-Yuan Lo", "Vishal M. Patel"], "author_detail": {"name": "Vishal M. Patel"}, "author": "Vishal M. Patel", "links": [{"href": "http://arxiv.org/abs/2009.05244v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2009.05244v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.ML", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2009.05244v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2009.05244v1", "arxiv_comment": null, "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/2010.14022v1", "guidislink": true, "updated": "2020-10-27T02:59:54Z", "updated_parsed": [2020, 10, 27, 2, 59, 54, 1, 301, 0], "published": "2020-10-27T02:59:54Z", "published_parsed": [2020, 10, 27, 2, 59, 54, 1, 301, 0], "title": "ByteCover: Cover Song Identification via Multi-Loss Training", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=1000&max_results=10&sortBy=relevance&sortOrder=descending", "value": "ByteCover: Cover Song Identification via Multi-Loss Training"}, "summary": "We present in this paper ByteCover, which is a new feature learning method\nfor cover song identification (CSI). ByteCover is built based on the classical\nResNet model, and two major improvements are designed to further enhance the\ncapability of the model for CSI. In the first improvement, we introduce the\nintegration of instance normalization (IN) and batch normalization (BN) to\nbuild IBN blocks, which are major components of our ResNet-IBN model. With the\nhelp of the IBN blocks, our CSI model can learn features that are invariant to\nthe changes of musical attributes such as key, tempo, timbre and genre, while\npreserving the version information. In the second improvement, we employ the\nBNNeck method to allow a multi-loss training and encourage our method to\njointly optimize a classification loss and a triplet loss, and by this means,\nthe inter-class discrimination and intra-class compactness of cover songs, can\nbe ensured at the same time. A set of experiments demonstrated the\neffectiveness and efficiency of ByteCover on multiple datasets, and in the\nDa-TACOS dataset, ByteCover outperformed the best competitive system by 20.9\\%.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=1000&max_results=10&sortBy=relevance&sortOrder=descending", "value": "We present in this paper ByteCover, which is a new feature learning method\nfor cover song identification (CSI). ByteCover is built based on the classical\nResNet model, and two major improvements are designed to further enhance the\ncapability of the model for CSI. In the first improvement, we introduce the\nintegration of instance normalization (IN) and batch normalization (BN) to\nbuild IBN blocks, which are major components of our ResNet-IBN model. With the\nhelp of the IBN blocks, our CSI model can learn features that are invariant to\nthe changes of musical attributes such as key, tempo, timbre and genre, while\npreserving the version information. In the second improvement, we employ the\nBNNeck method to allow a multi-loss training and encourage our method to\njointly optimize a classification loss and a triplet loss, and by this means,\nthe inter-class discrimination and intra-class compactness of cover songs, can\nbe ensured at the same time. A set of experiments demonstrated the\neffectiveness and efficiency of ByteCover on multiple datasets, and in the\nDa-TACOS dataset, ByteCover outperformed the best competitive system by 20.9\\%."}, "authors": ["Xingjian Du", "Zhesong Yu", "Bilei Zhu", "Xiaoou Chen", "Zejun Ma"], "author_detail": {"name": "Zejun Ma"}, "author": "Zejun Ma", "links": [{"href": "http://arxiv.org/abs/2010.14022v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2010.14022v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.SD", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.SD", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "eess.AS", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2010.14022v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2010.14022v1", "arxiv_comment": null, "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/2010.15317v1", "guidislink": true, "updated": "2020-10-29T02:18:16Z", "updated_parsed": [2020, 10, 29, 2, 18, 16, 3, 303, 0], "published": "2020-10-29T02:18:16Z", "published_parsed": [2020, 10, 29, 2, 18, 16, 3, 303, 0], "title": "The IQIYI System for Voice Conversion Challenge 2020", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=1000&max_results=10&sortBy=relevance&sortOrder=descending", "value": "The IQIYI System for Voice Conversion Challenge 2020"}, "summary": "This paper presents the IQIYI voice conversion system (T24) for Voice\nConversion 2020. In the competition, each target speaker has 70 sentences. We\nhave built an end-to-end voice conversion system based on PPG. First, the ASR\nacoustic model calculates the BN feature, which represents the content-related\ninformation in the speech. Then the Mel feature is calculated through an\nimproved prosody tacotron model. Finally, the Mel spectrum is converted to wav\nthrough an improved LPCNet. The evaluation results show that this system can\nachieve better voice conversion effects. In the case of using 16k rather than\n24k sampling rate audio, the conversion result is relatively good in\nnaturalness and similarity. Among them, our best results are in the similarity\nevaluation of the Task 2, the 2nd in the ASV-based objective evaluation and the\n5th in the subjective evaluation.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=1000&max_results=10&sortBy=relevance&sortOrder=descending", "value": "This paper presents the IQIYI voice conversion system (T24) for Voice\nConversion 2020. In the competition, each target speaker has 70 sentences. We\nhave built an end-to-end voice conversion system based on PPG. First, the ASR\nacoustic model calculates the BN feature, which represents the content-related\ninformation in the speech. Then the Mel feature is calculated through an\nimproved prosody tacotron model. Finally, the Mel spectrum is converted to wav\nthrough an improved LPCNet. The evaluation results show that this system can\nachieve better voice conversion effects. In the case of using 16k rather than\n24k sampling rate audio, the conversion result is relatively good in\nnaturalness and similarity. Among them, our best results are in the similarity\nevaluation of the Task 2, the 2nd in the ASV-based objective evaluation and the\n5th in the subjective evaluation."}, "authors": ["Wendong Gan", "Haitao Chen", "Yin Yan", "Jianwei Li", "Bolong Wen", "Xueping Xu", "Hai Li"], "author_detail": {"name": "Hai Li"}, "author": "Hai Li", "links": [{"href": "http://arxiv.org/abs/2010.15317v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2010.15317v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.SD", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.SD", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "eess.AS", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2010.15317v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2010.15317v1", "arxiv_comment": null, "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/2011.08096v1", "guidislink": true, "updated": "2020-11-16T16:57:05Z", "updated_parsed": [2020, 11, 16, 16, 57, 5, 0, 321, 0], "published": "2020-11-16T16:57:05Z", "published_parsed": [2020, 11, 16, 16, 57, 5, 0, 321, 0], "title": "The unreasonable effectiveness of Batch-Norm statistics in addressing\n  catastrophic forgetting across medical institutions", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=1000&max_results=10&sortBy=relevance&sortOrder=descending", "value": "The unreasonable effectiveness of Batch-Norm statistics in addressing\n  catastrophic forgetting across medical institutions"}, "summary": "Model brittleness is a primary concern when deploying deep learning models in\nmedical settings owing to inter-institution variations, like patient\ndemographics and intra-institution variation, such as multiple scanner types.\nWhile simply training on the combined datasets is fraught with data privacy\nlimitations, fine-tuning the model on subsequent institutions after training it\non the original institution results in a decrease in performance on the\noriginal dataset, a phenomenon called catastrophic forgetting. In this paper,\nwe investigate trade-off between model refinement and retention of previously\nlearned knowledge and subsequently address catastrophic forgetting for the\nassessment of mammographic breast density. More specifically, we propose a\nsimple yet effective approach, adapting Elastic weight consolidation (EWC)\nusing the global batch normalization (BN) statistics of the original dataset.\nThe results of this study provide guidance for the deployment of clinical deep\nlearning models where continuous learning is needed for domain expansion.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=1000&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Model brittleness is a primary concern when deploying deep learning models in\nmedical settings owing to inter-institution variations, like patient\ndemographics and intra-institution variation, such as multiple scanner types.\nWhile simply training on the combined datasets is fraught with data privacy\nlimitations, fine-tuning the model on subsequent institutions after training it\non the original institution results in a decrease in performance on the\noriginal dataset, a phenomenon called catastrophic forgetting. In this paper,\nwe investigate trade-off between model refinement and retention of previously\nlearned knowledge and subsequently address catastrophic forgetting for the\nassessment of mammographic breast density. More specifically, we propose a\nsimple yet effective approach, adapting Elastic weight consolidation (EWC)\nusing the global batch normalization (BN) statistics of the original dataset.\nThe results of this study provide guidance for the deployment of clinical deep\nlearning models where continuous learning is needed for domain expansion."}, "authors": ["Sharut Gupta", "Praveer Singh", "Ken Chang", "Mehak Aggarwal", "Nishanth Arun", "Liangqiong Qu", "Katharina Hoebel", "Jay Patel", "Mishka Gidwani", "Ashwin Vaswani", "Daniel L Rubin", "Jayashree Kalpathy-Cramer"], "author_detail": {"name": "Jayashree Kalpathy-Cramer"}, "author": "Jayashree Kalpathy-Cramer", "arxiv_comment": "Accepted as oral presentation in Machine Learning for Health (ML4H)\n  at NeurIPS 2020 - Extended Abstract ; 6 pages and 4 figures", "links": [{"href": "http://arxiv.org/abs/2011.08096v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2011.08096v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2011.08096v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2011.08096v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/2011.09020v2", "guidislink": true, "updated": "2020-11-20T08:22:09Z", "updated_parsed": [2020, 11, 20, 8, 22, 9, 4, 325, 0], "published": "2020-11-18T01:11:55Z", "published_parsed": [2020, 11, 18, 1, 11, 55, 2, 323, 0], "title": "FSPN: A New Class of Probabilistic Graphical Model", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=1000&max_results=10&sortBy=relevance&sortOrder=descending", "value": "FSPN: A New Class of Probabilistic Graphical Model"}, "summary": "We introduce factorize sum split product networks (FSPNs), a new class of\nprobabilistic graphical models (PGMs). FSPNs are designed to overcome the\ndrawbacks of existing PGMs in terms of estimation accuracy and inference\nefficiency. Specifically, Bayesian networks (BNs) have low inference speed and\nperformance of tree structured sum product networks(SPNs) significantly\ndegrades in presence of highly correlated variables. FSPNs absorb their\nadvantages by adaptively modeling the joint distribution of variables according\nto their dependence degree, so that one can simultaneously attain the two\ndesirable goals: high estimation accuracy and fast inference speed. We present\nefficient probability inference and structure learning algorithms for FSPNs,\nalong with a theoretical analysis and extensive evaluation evidence. Our\nexperimental results on synthetic and benchmark datasets indicate the\nsuperiority of FSPN over other PGMs.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=1000&max_results=10&sortBy=relevance&sortOrder=descending", "value": "We introduce factorize sum split product networks (FSPNs), a new class of\nprobabilistic graphical models (PGMs). FSPNs are designed to overcome the\ndrawbacks of existing PGMs in terms of estimation accuracy and inference\nefficiency. Specifically, Bayesian networks (BNs) have low inference speed and\nperformance of tree structured sum product networks(SPNs) significantly\ndegrades in presence of highly correlated variables. FSPNs absorb their\nadvantages by adaptively modeling the joint distribution of variables according\nto their dependence degree, so that one can simultaneously attain the two\ndesirable goals: high estimation accuracy and fast inference speed. We present\nefficient probability inference and structure learning algorithms for FSPNs,\nalong with a theoretical analysis and extensive evaluation evidence. Our\nexperimental results on synthetic and benchmark datasets indicate the\nsuperiority of FSPN over other PGMs."}, "authors": ["Ziniu Wu", "Rong Zhu", "Andreas Pfadler", "Yuxing Han", "Jiangneng Li", "Zhengping Qian", "Kai Zeng", "Jingren Zhou"], "author_detail": {"name": "Jingren Zhou"}, "author": "Jingren Zhou", "arxiv_comment": "16 pages", "links": [{"href": "http://arxiv.org/abs/2011.09020v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2011.09020v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2011.09020v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2011.09020v2", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/2012.00204v1", "guidislink": true, "updated": "2020-12-01T01:20:59Z", "updated_parsed": [2020, 12, 1, 1, 20, 59, 1, 336, 0], "published": "2020-12-01T01:20:59Z", "published_parsed": [2020, 12, 1, 1, 20, 59, 1, 336, 0], "title": "How to fine-tune deep neural networks in few-shot learning?", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=1010&max_results=10&sortBy=relevance&sortOrder=descending", "value": "How to fine-tune deep neural networks in few-shot learning?"}, "summary": "Deep learning has been widely used in data-intensive applications. However,\ntraining a deep neural network often requires a large data set. When there is\nnot enough data available for training, the performance of deep learning models\nis even worse than that of shallow networks. It has been proved that few-shot\nlearning can generalize to new tasks with few training samples. Fine-tuning of\na deep model is simple and effective few-shot learning method. However, how to\nfine-tune deep learning models (fine-tune convolution layer or BN layer?) still\nlack deep investigation. Hence, we study how to fine-tune deep models through\nexperimental comparison in this paper. Furthermore, the weight of the models is\nanalyzed to verify the feasibility of the fine-tuning method.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=1010&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Deep learning has been widely used in data-intensive applications. However,\ntraining a deep neural network often requires a large data set. When there is\nnot enough data available for training, the performance of deep learning models\nis even worse than that of shallow networks. It has been proved that few-shot\nlearning can generalize to new tasks with few training samples. Fine-tuning of\na deep model is simple and effective few-shot learning method. However, how to\nfine-tune deep learning models (fine-tune convolution layer or BN layer?) still\nlack deep investigation. Hence, we study how to fine-tune deep models through\nexperimental comparison in this paper. Furthermore, the weight of the models is\nanalyzed to verify the feasibility of the fine-tuning method."}, "authors": ["Peng Peng", "Jiugen Wang"], "author_detail": {"name": "Jiugen Wang"}, "author": "Jiugen Wang", "links": [{"href": "http://arxiv.org/abs/2012.00204v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2012.00204v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2012.00204v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2012.00204v1", "arxiv_comment": null, "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/2012.08408v1", "guidislink": true, "updated": "2020-12-15T16:36:42Z", "updated_parsed": [2020, 12, 15, 16, 36, 42, 1, 350, 0], "published": "2020-12-15T16:36:42Z", "published_parsed": [2020, 12, 15, 16, 36, 42, 1, 350, 0], "title": "SPOC learner's final grade prediction based on a novel sampling batch\n  normalization embedded neural network method", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=1010&max_results=10&sortBy=relevance&sortOrder=descending", "value": "SPOC learner's final grade prediction based on a novel sampling batch\n  normalization embedded neural network method"}, "summary": "Recent years have witnessed the rapid growth of Small Private Online Courses\n(SPOC) which is able to highly customized and personalized to adapt variable\neducational requests, in which machine learning techniques are explored to\nsummarize and predict the learner's performance, mostly focus on the final\ngrade. However, the problem is that the final grade of learners on SPOC is\ngenerally seriously imbalance which handicaps the training of prediction model.\nTo solve this problem, a sampling batch normalization embedded deep neural\nnetwork (SBNEDNN) method is developed in this paper. First, a combined\nindicator is defined to measure the distribution of the data, then a rule is\nestablished to guide the sampling process. Second, the batch normalization (BN)\nmodified layers are embedded into full connected neural network to solve the\ndata imbalanced problem. Experimental results with other three deep learning\nmethods demonstrates the superiority of the proposed method.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=1010&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Recent years have witnessed the rapid growth of Small Private Online Courses\n(SPOC) which is able to highly customized and personalized to adapt variable\neducational requests, in which machine learning techniques are explored to\nsummarize and predict the learner's performance, mostly focus on the final\ngrade. However, the problem is that the final grade of learners on SPOC is\ngenerally seriously imbalance which handicaps the training of prediction model.\nTo solve this problem, a sampling batch normalization embedded deep neural\nnetwork (SBNEDNN) method is developed in this paper. First, a combined\nindicator is defined to measure the distribution of the data, then a rule is\nestablished to guide the sampling process. Second, the batch normalization (BN)\nmodified layers are embedded into full connected neural network to solve the\ndata imbalanced problem. Experimental results with other three deep learning\nmethods demonstrates the superiority of the proposed method."}, "authors": ["Zhuonan Liang", "Ziheng Liu", "Huaze Shi", "Yunlong Chen", "Yanbin Cai", "Yating Liang", "Yafan Feng", "Yuqing Yang", "Jing Zhang", "Peng Fu"], "author_detail": {"name": "Peng Fu"}, "author": "Peng Fu", "arxiv_comment": "11 pages, 5 figures, ICAIS 2021", "links": [{"href": "http://arxiv.org/abs/2012.08408v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2012.08408v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2012.08408v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2012.08408v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/2012.15732v1", "guidislink": true, "updated": "2020-12-28T08:00:56Z", "updated_parsed": [2020, 12, 28, 8, 0, 56, 0, 363, 0], "published": "2020-12-28T08:00:56Z", "published_parsed": [2020, 12, 28, 8, 0, 56, 0, 363, 0], "title": "Improving Unsupervised Domain Adaptation by Reducing Bi-level Feature\n  Redundancy", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=1010&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Improving Unsupervised Domain Adaptation by Reducing Bi-level Feature\n  Redundancy"}, "summary": "Reducing feature redundancy has shown beneficial effects for improving the\naccuracy of deep learning models, thus it is also indispensable for the models\nof unsupervised domain adaptation (UDA). Nevertheless, most recent efforts in\nthe field of UDA ignores this point. Moreover, main schemes realizing this in\ngeneral independent of UDA purely involve a single domain, thus might not be\neffective for cross-domain tasks. In this paper, we emphasize the significance\nof reducing feature redundancy for improving UDA in a bi-level way. For the\nfirst level, we try to ensure compact domain-specific features with a\ntransferable decorrelated normalization module, which preserves specific domain\ninformation whilst easing the side effect of feature redundancy on the sequel\ndomain-invariance. In the second level, domain-invariant feature redundancy\ncaused by domain-shared representation is further mitigated via an alternative\nbrand orthogonality for better generalization. These two novel aspects can be\neasily plugged into any BN-based backbone neural networks. Specifically, simply\napplying them to ResNet50 has achieved competitive performance to the\nstate-of-the-arts on five popular benchmarks. Our code will be available at\nhttps://github.com/dreamkily/gUDA.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=1010&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Reducing feature redundancy has shown beneficial effects for improving the\naccuracy of deep learning models, thus it is also indispensable for the models\nof unsupervised domain adaptation (UDA). Nevertheless, most recent efforts in\nthe field of UDA ignores this point. Moreover, main schemes realizing this in\ngeneral independent of UDA purely involve a single domain, thus might not be\neffective for cross-domain tasks. In this paper, we emphasize the significance\nof reducing feature redundancy for improving UDA in a bi-level way. For the\nfirst level, we try to ensure compact domain-specific features with a\ntransferable decorrelated normalization module, which preserves specific domain\ninformation whilst easing the side effect of feature redundancy on the sequel\ndomain-invariance. In the second level, domain-invariant feature redundancy\ncaused by domain-shared representation is further mitigated via an alternative\nbrand orthogonality for better generalization. These two novel aspects can be\neasily plugged into any BN-based backbone neural networks. Specifically, simply\napplying them to ResNet50 has achieved competitive performance to the\nstate-of-the-arts on five popular benchmarks. Our code will be available at\nhttps://github.com/dreamkily/gUDA."}, "authors": ["Mengzhu Wang", "Xiang Zhang", "Long Lan", "Wei Wang", "Huibin Tan", "Zhigang Luo"], "author_detail": {"name": "Zhigang Luo"}, "author": "Zhigang Luo", "arxiv_comment": "12 pages, 5 figures", "links": [{"href": "http://arxiv.org/abs/2012.15732v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2012.15732v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2012.15732v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2012.15732v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1206.6859v1", "guidislink": true, "updated": "2012-06-27T16:27:12Z", "updated_parsed": [2012, 6, 27, 16, 27, 12, 2, 179, 0], "published": "2012-06-27T16:27:12Z", "published_parsed": [2012, 6, 27, 16, 27, 12, 2, 179, 0], "title": "Propagation of Delays in the National Airspace System", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=1040&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Propagation of Delays in the National Airspace System"}, "summary": "The National Airspace System (NAS) is a large and complex system with\nthousands of interrelated components: administration, control centers,\nairports, airlines, aircraft, passengers, etc. The complexity of the NAS\ncreates many difficulties in management and control. One of the most pressing\nproblems is flight delay. Delay creates high cost to airlines, complaints from\npassengers, and difficulties for airport operations. As demand on the system\nincreases, the delay problem becomes more and more prominent. For this reason,\nit is essential for the Federal Aviation Administration to understand the\ncauses of delay and to find ways to reduce delay. Major contributing factors to\ndelay are congestion at the origin airport, weather, increasing demand, and air\ntraffic management (ATM) decisions such as the Ground Delay Programs (GDP).\nDelay is an inherently stochastic phenomenon. Even if all known causal factors\ncould be accounted for, macro-level national airspace system (NAS) delays could\nnot be predicted with certainty from micro-level aircraft information. This\npaper presents a stochastic model that uses Bayesian Networks (BNs) to model\nthe relationships among different components of aircraft delay and the causal\nfactors that affect delays. A case study on delays of departure flights from\nChicago O'Hare international airport (ORD) to Hartsfield-Jackson Atlanta\nInternational Airport (ATL) reveals how local and system level environmental\nand human-caused factors combine to affect components of delay, and how these\ncomponents contribute to the final arrival delay at the destination airport.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=1040&max_results=10&sortBy=relevance&sortOrder=descending", "value": "The National Airspace System (NAS) is a large and complex system with\nthousands of interrelated components: administration, control centers,\nairports, airlines, aircraft, passengers, etc. The complexity of the NAS\ncreates many difficulties in management and control. One of the most pressing\nproblems is flight delay. Delay creates high cost to airlines, complaints from\npassengers, and difficulties for airport operations. As demand on the system\nincreases, the delay problem becomes more and more prominent. For this reason,\nit is essential for the Federal Aviation Administration to understand the\ncauses of delay and to find ways to reduce delay. Major contributing factors to\ndelay are congestion at the origin airport, weather, increasing demand, and air\ntraffic management (ATM) decisions such as the Ground Delay Programs (GDP).\nDelay is an inherently stochastic phenomenon. Even if all known causal factors\ncould be accounted for, macro-level national airspace system (NAS) delays could\nnot be predicted with certainty from micro-level aircraft information. This\npaper presents a stochastic model that uses Bayesian Networks (BNs) to model\nthe relationships among different components of aircraft delay and the causal\nfactors that affect delays. A case study on delays of departure flights from\nChicago O'Hare international airport (ORD) to Hartsfield-Jackson Atlanta\nInternational Airport (ATL) reveals how local and system level environmental\nand human-caused factors combine to affect components of delay, and how these\ncomponents contribute to the final arrival delay at the destination airport."}, "authors": ["Kathryn Blackmond Laskey", "Ning Xu", "Chun-Hung Chen"], "author_detail": {"name": "Chun-Hung Chen"}, "author": "Chun-Hung Chen", "arxiv_comment": "Appears in Proceedings of the Twenty-Second Conference on Uncertainty\n  in Artificial Intelligence (UAI2006)", "links": [{"href": "http://arxiv.org/abs/1206.6859v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1206.6859v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1206.6859v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1206.6859v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1301.2289v1", "guidislink": true, "updated": "2013-01-10T16:24:58Z", "updated_parsed": [2013, 1, 10, 16, 24, 58, 3, 10, 0], "published": "2013-01-10T16:24:58Z", "published_parsed": [2013, 1, 10, 16, 24, 58, 3, 10, 0], "title": "Exact Inference in Networks with Discrete Children of Continuous Parents", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=1050&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Exact Inference in Networks with Discrete Children of Continuous Parents"}, "summary": "Many real life domains contain a mixture of discrete and continuous variables\nand can be modeled as hybrid Bayesian Networks. Animportant subclass of hybrid\nBNs are conditional linear Gaussian (CLG) networks, where the conditional\ndistribution of the continuous variables given an assignment to the discrete\nvariables is a multivariate Gaussian. Lauritzen's extension to the clique tree\nalgorithm can be used for exact inference in CLG networks. However, many\ndomains also include discrete variables that depend on continuous ones, and CLG\nnetworks do not allow such dependencies to berepresented. No exact inference\nalgorithm has been proposed for these enhanced CLG networks. In this paper, we\ngeneralize Lauritzen's algorithm, providing the first \"exact\" inference\nalgorithm for augmented CLG networks - networks where continuous nodes are\nconditional linear Gaussians but that also allow discrete children ofcontinuous\nparents. Our algorithm is exact in the sense that it computes the exact\ndistributions over the discrete nodes, and the exact first and second moments\nof the continuous ones, up to the accuracy obtained by numerical integration\nused within thealgorithm. When the discrete children are modeled with softmax\nCPDs (as is the case in many real world domains) the approximation of the\ncontinuous distributions using the first two moments is particularly accurate.\nOur algorithm is simple to implement and often comparable in its complexity to\nLauritzen's algorithm. We show empirically that it achieves substantially\nhigher accuracy than previous approximate algorithms.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=1050&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Many real life domains contain a mixture of discrete and continuous variables\nand can be modeled as hybrid Bayesian Networks. Animportant subclass of hybrid\nBNs are conditional linear Gaussian (CLG) networks, where the conditional\ndistribution of the continuous variables given an assignment to the discrete\nvariables is a multivariate Gaussian. Lauritzen's extension to the clique tree\nalgorithm can be used for exact inference in CLG networks. However, many\ndomains also include discrete variables that depend on continuous ones, and CLG\nnetworks do not allow such dependencies to berepresented. No exact inference\nalgorithm has been proposed for these enhanced CLG networks. In this paper, we\ngeneralize Lauritzen's algorithm, providing the first \"exact\" inference\nalgorithm for augmented CLG networks - networks where continuous nodes are\nconditional linear Gaussians but that also allow discrete children ofcontinuous\nparents. Our algorithm is exact in the sense that it computes the exact\ndistributions over the discrete nodes, and the exact first and second moments\nof the continuous ones, up to the accuracy obtained by numerical integration\nused within thealgorithm. When the discrete children are modeled with softmax\nCPDs (as is the case in many real world domains) the approximation of the\ncontinuous distributions using the first two moments is particularly accurate.\nOur algorithm is simple to implement and often comparable in its complexity to\nLauritzen's algorithm. We show empirically that it achieves substantially\nhigher accuracy than previous approximate algorithms."}, "authors": ["Uri Lerner", "Eran Segal", "Daphne Koller"], "author_detail": {"name": "Daphne Koller"}, "author": "Daphne Koller", "arxiv_comment": "Appears in Proceedings of the Seventeenth Conference on Uncertainty\n  in Artificial Intelligence (UAI2001)", "links": [{"href": "http://arxiv.org/abs/1301.2289v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1301.2289v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1301.2289v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1301.2289v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1309.1501v3", "guidislink": true, "updated": "2013-12-10T11:51:39Z", "updated_parsed": [2013, 12, 10, 11, 51, 39, 1, 344, 0], "published": "2013-09-05T22:06:58Z", "published_parsed": [2013, 9, 5, 22, 6, 58, 3, 248, 0], "title": "Improvements to deep convolutional neural networks for LVCSR", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=1050&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Improvements to deep convolutional neural networks for LVCSR"}, "summary": "Deep Convolutional Neural Networks (CNNs) are more powerful than Deep Neural\nNetworks (DNN), as they are able to better reduce spectral variation in the\ninput signal. This has also been confirmed experimentally, with CNNs showing\nimprovements in word error rate (WER) between 4-12% relative compared to DNNs\nacross a variety of LVCSR tasks. In this paper, we describe different methods\nto further improve CNN performance. First, we conduct a deep analysis comparing\nlimited weight sharing and full weight sharing with state-of-the-art features.\nSecond, we apply various pooling strategies that have shown improvements in\ncomputer vision to an LVCSR speech task. Third, we introduce a method to\neffectively incorporate speaker adaptation, namely fMLLR, into log-mel\nfeatures. Fourth, we introduce an effective strategy to use dropout during\nHessian-free sequence training. We find that with these improvements,\nparticularly with fMLLR and dropout, we are able to achieve an additional 2-3%\nrelative improvement in WER on a 50-hour Broadcast News task over our previous\nbest CNN baseline. On a larger 400-hour BN task, we find an additional 4-5%\nrelative improvement over our previous best CNN baseline.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=1050&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Deep Convolutional Neural Networks (CNNs) are more powerful than Deep Neural\nNetworks (DNN), as they are able to better reduce spectral variation in the\ninput signal. This has also been confirmed experimentally, with CNNs showing\nimprovements in word error rate (WER) between 4-12% relative compared to DNNs\nacross a variety of LVCSR tasks. In this paper, we describe different methods\nto further improve CNN performance. First, we conduct a deep analysis comparing\nlimited weight sharing and full weight sharing with state-of-the-art features.\nSecond, we apply various pooling strategies that have shown improvements in\ncomputer vision to an LVCSR speech task. Third, we introduce a method to\neffectively incorporate speaker adaptation, namely fMLLR, into log-mel\nfeatures. Fourth, we introduce an effective strategy to use dropout during\nHessian-free sequence training. We find that with these improvements,\nparticularly with fMLLR and dropout, we are able to achieve an additional 2-3%\nrelative improvement in WER on a 50-hour Broadcast News task over our previous\nbest CNN baseline. On a larger 400-hour BN task, we find an additional 4-5%\nrelative improvement over our previous best CNN baseline."}, "authors": ["Tara N. Sainath", "Brian Kingsbury", "Abdel-rahman Mohamed", "George E. Dahl", "George Saon", "Hagen Soltau", "Tomas Beran", "Aleksandr Y. Aravkin", "Bhuvana Ramabhadran"], "author_detail": {"name": "Bhuvana Ramabhadran"}, "author": "Bhuvana Ramabhadran", "arxiv_comment": "6 pages, 1 figure", "links": [{"href": "http://arxiv.org/abs/1309.1501v3", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1309.1501v3", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.NE", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "math.OC", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.ML", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "65K05, 90C15, 90C90", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1309.1501v3", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1309.1501v3", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1710.07617v3", "guidislink": true, "updated": "2018-11-11T04:30:11Z", "updated_parsed": [2018, 11, 11, 4, 30, 11, 6, 315, 0], "published": "2017-10-20T17:27:51Z", "published_parsed": [2017, 10, 20, 17, 27, 51, 4, 293, 0], "title": "Asymptotically Optimal Resource Block Allocation With Limited Feedback", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=1070&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Asymptotically Optimal Resource Block Allocation With Limited Feedback"}, "summary": "Consider a channel allocation problem over a frequency-selective\nchannel.There are K channels (frequency bands) and N users such that K=bN for\nsome positive integer b. We want to allocate b channels (or resource blocks) to\neach user. Due to the nature of the frequency-selective channel, each user\nconsiders some channels to be better than others. The optimal solution to this\nresource allocation problem can be computed using the Hungarian algorithm.\nHowever, this requires knowledge of the numerical value of all the channel\ngains, which makes this approach impractical for large networks. We suggest a\nsuboptimal approach, that only requires knowing what the M-best channels of\neach user are. We find the minimal value of M such that there exists an\nallocation where all the b channels each user gets are among his M-best. This\nleads to feedback of significantly less than one bit per user per channel. For\na large class of fading distributions, including Rayleigh, Rician, m-Nakagami\nand others, this suboptimal approach leads to both an asymptotically (in K)\noptimal sum-rate and an asymptotically optimal minimal rate. Our\nnon-opportunistic approach achieves (asymptotically) full multiuser diversity\nas well as optimal fairness, by contrast to all other limited feedback\nalgorithms.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=1070&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Consider a channel allocation problem over a frequency-selective\nchannel.There are K channels (frequency bands) and N users such that K=bN for\nsome positive integer b. We want to allocate b channels (or resource blocks) to\neach user. Due to the nature of the frequency-selective channel, each user\nconsiders some channels to be better than others. The optimal solution to this\nresource allocation problem can be computed using the Hungarian algorithm.\nHowever, this requires knowledge of the numerical value of all the channel\ngains, which makes this approach impractical for large networks. We suggest a\nsuboptimal approach, that only requires knowing what the M-best channels of\neach user are. We find the minimal value of M such that there exists an\nallocation where all the b channels each user gets are among his M-best. This\nleads to feedback of significantly less than one bit per user per channel. For\na large class of fading distributions, including Rayleigh, Rician, m-Nakagami\nand others, this suboptimal approach leads to both an asymptotically (in K)\noptimal sum-rate and an asymptotically optimal minimal rate. Our\nnon-opportunistic approach achieves (asymptotically) full multiuser diversity\nas well as optimal fairness, by contrast to all other limited feedback\nalgorithms."}, "authors": ["Ilai Bistritz", "Amir Leshem"], "author_detail": {"name": "Amir Leshem"}, "author": "Amir Leshem", "links": [{"title": "doi", "href": "http://dx.doi.org/10.1109/TWC.2018.2875706", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/1710.07617v3", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1710.07617v3", "rel": "related", "type": "application/pdf"}], "arxiv_comment": "Accepted to IEEE Transactions on Wireless Communications", "arxiv_primary_category": {"term": "cs.IT", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.IT", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "math.IT", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1710.07617v3", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1710.07617v3", "journal_reference": null, "doi": "10.1109/TWC.2018.2875706"}
{"id": "http://arxiv.org/abs/1801.07910v1", "guidislink": true, "updated": "2018-01-24T08:53:55Z", "updated_parsed": [2018, 1, 24, 8, 53, 55, 2, 24, 0], "published": "2018-01-24T08:53:55Z", "published_parsed": [2018, 1, 24, 8, 53, 55, 2, 24, 0], "title": "Waveform Modeling and Generation Using Hierarchical Recurrent Neural\n  Networks for Speech Bandwidth Extension", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=1090&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Waveform Modeling and Generation Using Hierarchical Recurrent Neural\n  Networks for Speech Bandwidth Extension"}, "summary": "This paper presents a waveform modeling and generation method using\nhierarchical recurrent neural networks (HRNN) for speech bandwidth extension\n(BWE). Different from conventional BWE methods which predict spectral\nparameters for reconstructing wideband speech waveforms, this BWE method models\nand predicts waveform samples directly without using vocoders. Inspired by\nSampleRNN which is an unconditional neural audio generator, the HRNN model\nrepresents the distribution of each wideband or high-frequency waveform sample\nconditioned on the input narrowband waveform samples using a neural network\ncomposed of long short-term memory (LSTM) layers and feed-forward (FF) layers.\nThe LSTM layers form a hierarchical structure and each layer operates at a\nspecific temporal resolution to efficiently capture long-span dependencies\nbetween temporal sequences. Furthermore, additional conditions, such as the\nbottleneck (BN) features derived from narrowband speech using a deep neural\nnetwork (DNN)-based state classifier, are employed as auxiliary input to\nfurther improve the quality of generated wideband speech. The experimental\nresults of comparing several waveform modeling methods show that the HRNN-based\nmethod can achieve better speech quality and run-time efficiency than the\ndilated convolutional neural network (DCNN)-based method and the plain\nsample-level recurrent neural network (SRNN)-based method. Our proposed method\nalso outperforms the conventional vocoder-based BWE method using LSTM-RNNs in\nterms of the subjective quality of the reconstructed wideband speech.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=1090&max_results=10&sortBy=relevance&sortOrder=descending", "value": "This paper presents a waveform modeling and generation method using\nhierarchical recurrent neural networks (HRNN) for speech bandwidth extension\n(BWE). Different from conventional BWE methods which predict spectral\nparameters for reconstructing wideband speech waveforms, this BWE method models\nand predicts waveform samples directly without using vocoders. Inspired by\nSampleRNN which is an unconditional neural audio generator, the HRNN model\nrepresents the distribution of each wideband or high-frequency waveform sample\nconditioned on the input narrowband waveform samples using a neural network\ncomposed of long short-term memory (LSTM) layers and feed-forward (FF) layers.\nThe LSTM layers form a hierarchical structure and each layer operates at a\nspecific temporal resolution to efficiently capture long-span dependencies\nbetween temporal sequences. Furthermore, additional conditions, such as the\nbottleneck (BN) features derived from narrowband speech using a deep neural\nnetwork (DNN)-based state classifier, are employed as auxiliary input to\nfurther improve the quality of generated wideband speech. The experimental\nresults of comparing several waveform modeling methods show that the HRNN-based\nmethod can achieve better speech quality and run-time efficiency than the\ndilated convolutional neural network (DCNN)-based method and the plain\nsample-level recurrent neural network (SRNN)-based method. Our proposed method\nalso outperforms the conventional vocoder-based BWE method using LSTM-RNNs in\nterms of the subjective quality of the reconstructed wideband speech."}, "authors": ["Zhen-Hua Ling", "Yang Ai", "Yu Gu", "Li-Rong Dai"], "author_detail": {"name": "Li-Rong Dai"}, "author": "Li-Rong Dai", "links": [{"title": "doi", "href": "http://dx.doi.org/10.1109/TASLP.2018.2798811", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/1801.07910v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1801.07910v1", "rel": "related", "type": "application/pdf"}], "arxiv_comment": "Accepted by IEEE Transactions on Audio, Speech and Language\n  Processing", "arxiv_primary_category": {"term": "cs.SD", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.SD", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "eess.AS", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1801.07910v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1801.07910v1", "journal_reference": null, "doi": "10.1109/TASLP.2018.2798811"}
{"id": "http://arxiv.org/abs/1806.02455v2", "guidislink": true, "updated": "2018-06-08T00:36:05Z", "updated_parsed": [2018, 6, 8, 0, 36, 5, 4, 159, 0], "published": "2018-06-06T23:12:02Z", "published_parsed": [2018, 6, 6, 23, 12, 2, 2, 157, 0], "title": "MEBN-RM: A Mapping between Multi-Entity Bayesian Network and Relational\n  Model", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=1090&max_results=10&sortBy=relevance&sortOrder=descending", "value": "MEBN-RM: A Mapping between Multi-Entity Bayesian Network and Relational\n  Model"}, "summary": "Multi-Entity Bayesian Network (MEBN) is a knowledge representation formalism\ncombining Bayesian Networks (BN) with First-Order Logic (FOL). MEBN has\nsufficient expressive power for general-purpose knowledge representation and\nreasoning. Developing a MEBN model to support a given application is a\nchallenge, requiring definition of entities, relationships, random variables,\nconditional dependence relationships, and probability distributions. When\navailable, data can be invaluable both to improve performance and to streamline\ndevelopment. By far the most common format for available data is the relational\ndatabase (RDB). Relational databases describe and organize data according to\nthe Relational Model (RM). Developing a MEBN model from data stored in an RDB\ntherefore requires mapping between the two formalisms. This paper presents\nMEBN-RM, a set of mapping rules between key elements of MEBN and RM. We\nidentify links between the two languages (RM and MEBN) and define four levels\nof mapping from elements of RM to elements of MEBN. These definitions are\nimplemented in the MEBN-RM algorithm, which converts a relational schema in RM\nto a partial MEBN model. Through this research, the software has been released\nas a MEBN-RM open-source software tool. The method is illustrated through two\nexample use cases using MEBN-RM to develop MEBN models: a Critical\nInfrastructure Defense System and a Smart Manufacturing System.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=1090&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Multi-Entity Bayesian Network (MEBN) is a knowledge representation formalism\ncombining Bayesian Networks (BN) with First-Order Logic (FOL). MEBN has\nsufficient expressive power for general-purpose knowledge representation and\nreasoning. Developing a MEBN model to support a given application is a\nchallenge, requiring definition of entities, relationships, random variables,\nconditional dependence relationships, and probability distributions. When\navailable, data can be invaluable both to improve performance and to streamline\ndevelopment. By far the most common format for available data is the relational\ndatabase (RDB). Relational databases describe and organize data according to\nthe Relational Model (RM). Developing a MEBN model from data stored in an RDB\ntherefore requires mapping between the two formalisms. This paper presents\nMEBN-RM, a set of mapping rules between key elements of MEBN and RM. We\nidentify links between the two languages (RM and MEBN) and define four levels\nof mapping from elements of RM to elements of MEBN. These definitions are\nimplemented in the MEBN-RM algorithm, which converts a relational schema in RM\nto a partial MEBN model. Through this research, the software has been released\nas a MEBN-RM open-source software tool. The method is illustrated through two\nexample use cases using MEBN-RM to develop MEBN models: a Critical\nInfrastructure Defense System and a Smart Manufacturing System."}, "authors": ["Cheol Young Park", "Kathryn Blackmond Laskey"], "author_detail": {"name": "Kathryn Blackmond Laskey"}, "author": "Kathryn Blackmond Laskey", "links": [{"title": "doi", "href": "http://dx.doi.org/10.3390/app9091743", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/1806.02455v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1806.02455v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.ML", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1806.02455v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1806.02455v2", "arxiv_comment": null, "journal_reference": "Applied Sciences 2019,9", "doi": "10.3390/app9091743"}
{"id": "http://arxiv.org/abs/1806.06157v3", "guidislink": true, "updated": "2018-09-20T08:59:32Z", "updated_parsed": [2018, 9, 20, 8, 59, 32, 3, 263, 0], "published": "2018-06-16T00:33:50Z", "published_parsed": [2018, 6, 16, 0, 33, 50, 5, 167, 0], "title": "Object Level Visual Reasoning in Videos", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=1090&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Object Level Visual Reasoning in Videos"}, "summary": "Human activity recognition is typically addressed by detecting key concepts\nlike global and local motion, features related to object classes present in the\nscene, as well as features related to the global context. The next open\nchallenges in activity recognition require a level of understanding that pushes\nbeyond this and call for models with capabilities for fine distinction and\ndetailed comprehension of interactions between actors and objects in a scene.\nWe propose a model capable of learning to reason about semantically meaningful\nspatiotemporal interactions in videos. The key to our approach is a choice of\nperforming this reasoning at the object level through the integration of state\nof the art object detection networks. This allows the model to learn detailed\nspatial interactions that exist at a semantic, object-interaction relevant\nlevel. We evaluate our method on three standard datasets (Twenty-BN\nSomething-Something, VLOG and EPIC Kitchens) and achieve state of the art\nresults on all of them. Finally, we show visualizations of the interactions\nlearned by the model, which illustrate object classes and their interactions\ncorresponding to different activity classes.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=1090&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Human activity recognition is typically addressed by detecting key concepts\nlike global and local motion, features related to object classes present in the\nscene, as well as features related to the global context. The next open\nchallenges in activity recognition require a level of understanding that pushes\nbeyond this and call for models with capabilities for fine distinction and\ndetailed comprehension of interactions between actors and objects in a scene.\nWe propose a model capable of learning to reason about semantically meaningful\nspatiotemporal interactions in videos. The key to our approach is a choice of\nperforming this reasoning at the object level through the integration of state\nof the art object detection networks. This allows the model to learn detailed\nspatial interactions that exist at a semantic, object-interaction relevant\nlevel. We evaluate our method on three standard datasets (Twenty-BN\nSomething-Something, VLOG and EPIC Kitchens) and achieve state of the art\nresults on all of them. Finally, we show visualizations of the interactions\nlearned by the model, which illustrate object classes and their interactions\ncorresponding to different activity classes."}, "authors": ["Fabien Baradel", "Natalia Neverova", "Christian Wolf", "Julien Mille", "Greg Mori"], "author_detail": {"name": "Greg Mori"}, "author": "Greg Mori", "arxiv_comment": "Accepted at ECCV 2018 - long version (16 pages + ref)", "links": [{"href": "http://arxiv.org/abs/1806.06157v3", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1806.06157v3", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1806.06157v3", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1806.06157v3", "journal_reference": "ECCV 2018", "doi": null}
{"id": "http://arxiv.org/abs/1812.10307v1", "guidislink": true, "updated": "2018-12-26T13:33:45Z", "updated_parsed": [2018, 12, 26, 13, 33, 45, 2, 360, 0], "published": "2018-12-26T13:33:45Z", "published_parsed": [2018, 12, 26, 13, 33, 45, 2, 360, 0], "title": "Greening Big Data Networks: The Impact of Veracity", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=1100&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Greening Big Data Networks: The Impact of Veracity"}, "summary": "The continuous increase in big data applications, in number and types,\ncreates new challenges that should be tackled by the green ICT community. Big\ndata is mainly characterized by 4 Vs volume, variety, velocity, and veracity.\nEach V poses a number of challenges that have implications on the energy\nefficiency of the underlying networks carrying the big data. Addressing the\nveracity of the data is a more serious challenge to data scientists, since they\nneed to distinguish between the meaningful data and the dirty data. In this\narticle, we investigate the impact of big data veracity on greening IP by\ndeveloping a Mixed Integer Linear Programming, MILP, model that encapsulates\nthe distinctive features of veracity. In our analyses, the big data network was\ngreened by cleansing the raw big data before processing and then progressively\nprocessing the cleansed big data at strategic locations, dubbed processing\nnodes, PNs. The PNs are built into the network along the path from the sources\nto the centralized datacenters. At each PN, the cleansed data was processed and\nsmaller volume of useful information was extracted progressively, thereby,\nreducing the network power consumption. Furthermore, a backup for the cleansed\ndata was stored in an optimally selected Backup Node, BN. We evaluated the\nnetwork power saving that can be achieved by a green big data network compared\nto the classical non-progressive approach. We obtained up to 52 percent network\npower savings, on average, in the green big data approach compared to the\nclassical approach.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=1100&max_results=10&sortBy=relevance&sortOrder=descending", "value": "The continuous increase in big data applications, in number and types,\ncreates new challenges that should be tackled by the green ICT community. Big\ndata is mainly characterized by 4 Vs volume, variety, velocity, and veracity.\nEach V poses a number of challenges that have implications on the energy\nefficiency of the underlying networks carrying the big data. Addressing the\nveracity of the data is a more serious challenge to data scientists, since they\nneed to distinguish between the meaningful data and the dirty data. In this\narticle, we investigate the impact of big data veracity on greening IP by\ndeveloping a Mixed Integer Linear Programming, MILP, model that encapsulates\nthe distinctive features of veracity. In our analyses, the big data network was\ngreened by cleansing the raw big data before processing and then progressively\nprocessing the cleansed big data at strategic locations, dubbed processing\nnodes, PNs. The PNs are built into the network along the path from the sources\nto the centralized datacenters. At each PN, the cleansed data was processed and\nsmaller volume of useful information was extracted progressively, thereby,\nreducing the network power consumption. Furthermore, a backup for the cleansed\ndata was stored in an optimally selected Backup Node, BN. We evaluated the\nnetwork power saving that can be achieved by a green big data network compared\nto the classical non-progressive approach. We obtained up to 52 percent network\npower savings, on average, in the green big data approach compared to the\nclassical approach."}, "authors": ["Ali M. Al-Salim", "Taisir E. H. El-Gorashi", "Ahmed Q. Lawey", "Jaafar M. H. Elmirghani"], "author_detail": {"name": "Jaafar M. H. Elmirghani"}, "author": "Jaafar M. H. Elmirghani", "links": [{"href": "http://arxiv.org/abs/1812.10307v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1812.10307v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.NI", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.NI", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1812.10307v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1812.10307v1", "arxiv_comment": null, "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1907.12608v3", "guidislink": true, "updated": "2020-07-02T17:03:26Z", "updated_parsed": [2020, 7, 2, 17, 3, 26, 3, 184, 0], "published": "2019-07-29T19:24:40Z", "published_parsed": [2019, 7, 29, 19, 24, 40, 0, 210, 0], "title": "Deep Gradient Boosting -- Layer-wise Input Normalization of Neural\n  Networks", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=1110&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Deep Gradient Boosting -- Layer-wise Input Normalization of Neural\n  Networks"}, "summary": "Stochastic gradient descent (SGD) has been the dominant optimization method\nfor training deep neural networks due to its many desirable properties. One of\nthe more remarkable and least understood quality of SGD is that it generalizes\nrelatively well on unseen data even when the neural network has millions of\nparameters. We hypothesize that in certain cases it is desirable to relax its\nintrinsic generalization properties and introduce an extension of SGD called\ndeep gradient boosting (DGB). The key idea of DGB is that back-propagated\ngradients inferred using the chain rule can be viewed as pseudo-residual\ntargets of a gradient boosting problem. Thus at each layer of a neural network\nthe weight update is calculated by solving the corresponding boosting problem\nusing a linear base learner. The resulting weight update formula can also be\nviewed as a normalization procedure of the data that arrives at each layer\nduring the forward pass. When implemented as a separate input normalization\nlayer (INN) the new architecture shows improved performance on image\nrecognition tasks when compared to the same architecture without normalization\nlayers. As opposed to batch normalization (BN), INN has no learnable parameters\nhowever it matches its performance on CIFAR10 and ImageNet classification\ntasks.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=1110&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Stochastic gradient descent (SGD) has been the dominant optimization method\nfor training deep neural networks due to its many desirable properties. One of\nthe more remarkable and least understood quality of SGD is that it generalizes\nrelatively well on unseen data even when the neural network has millions of\nparameters. We hypothesize that in certain cases it is desirable to relax its\nintrinsic generalization properties and introduce an extension of SGD called\ndeep gradient boosting (DGB). The key idea of DGB is that back-propagated\ngradients inferred using the chain rule can be viewed as pseudo-residual\ntargets of a gradient boosting problem. Thus at each layer of a neural network\nthe weight update is calculated by solving the corresponding boosting problem\nusing a linear base learner. The resulting weight update formula can also be\nviewed as a normalization procedure of the data that arrives at each layer\nduring the forward pass. When implemented as a separate input normalization\nlayer (INN) the new architecture shows improved performance on image\nrecognition tasks when compared to the same architecture without normalization\nlayers. As opposed to batch normalization (BN), INN has no learnable parameters\nhowever it matches its performance on CIFAR10 and ImageNet classification\ntasks."}, "authors": ["Erhan Bilal"], "author_detail": {"name": "Erhan Bilal"}, "author": "Erhan Bilal", "arxiv_comment": "Solving the pseudo-inverse with SVD and splitting this into two\n  separate papers. There are too many changes to just update this version", "links": [{"href": "http://arxiv.org/abs/1907.12608v3", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1907.12608v3", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.ML", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1907.12608v3", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1907.12608v3", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1912.05845v3", "guidislink": true, "updated": "2020-05-09T09:27:12Z", "updated_parsed": [2020, 5, 9, 9, 27, 12, 5, 130, 0], "published": "2019-12-12T09:28:24Z", "published_parsed": [2019, 12, 12, 9, 28, 24, 3, 346, 0], "title": "Local Context Normalization: Revisiting Local Normalization", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=1110&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Local Context Normalization: Revisiting Local Normalization"}, "summary": "Normalization layers have been shown to improve convergence in deep neural\nnetworks, and even add useful inductive biases. In many vision applications the\nlocal spatial context of the features is important, but most common\nnormalization schemes including Group Normalization (GN), Instance\nNormalization (IN), and Layer Normalization (LN) normalize over the entire\nspatial dimension of a feature. This can wash out important signals and degrade\nperformance. For example, in applications that use satellite imagery, input\nimages can be arbitrarily large; consequently, it is nonsensical to normalize\nover the entire area. Positional Normalization (PN), on the other hand, only\nnormalizes over a single spatial position at a time. A natural compromise is to\nnormalize features by local context, while also taking into account group level\ninformation. In this paper, we propose Local Context Normalization (LCN): a\nnormalization layer where every feature is normalized based on a window around\nit and the filters in its group. We propose an algorithmic solution to make LCN\nefficient for arbitrary window sizes, even if every point in the image has a\nunique window. LCN outperforms its Batch Normalization (BN), GN, IN, and LN\ncounterparts for object detection, semantic segmentation, and instance\nsegmentation applications in several benchmark datasets, while keeping\nperformance independent of the batch size and facilitating transfer learning.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=1110&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Normalization layers have been shown to improve convergence in deep neural\nnetworks, and even add useful inductive biases. In many vision applications the\nlocal spatial context of the features is important, but most common\nnormalization schemes including Group Normalization (GN), Instance\nNormalization (IN), and Layer Normalization (LN) normalize over the entire\nspatial dimension of a feature. This can wash out important signals and degrade\nperformance. For example, in applications that use satellite imagery, input\nimages can be arbitrarily large; consequently, it is nonsensical to normalize\nover the entire area. Positional Normalization (PN), on the other hand, only\nnormalizes over a single spatial position at a time. A natural compromise is to\nnormalize features by local context, while also taking into account group level\ninformation. In this paper, we propose Local Context Normalization (LCN): a\nnormalization layer where every feature is normalized based on a window around\nit and the filters in its group. We propose an algorithmic solution to make LCN\nefficient for arbitrary window sizes, even if every point in the image has a\nunique window. LCN outperforms its Batch Normalization (BN), GN, IN, and LN\ncounterparts for object detection, semantic segmentation, and instance\nsegmentation applications in several benchmark datasets, while keeping\nperformance independent of the batch size and facilitating transfer learning."}, "authors": ["Anthony Ortiz", "Caleb Robinson", "Dan Morris", "Olac Fuentes", "Christopher Kiekintveld", "Md Mahmudulla Hassan", "Nebojsa Jojic"], "author_detail": {"name": "Nebojsa Jojic"}, "author": "Nebojsa Jojic", "arxiv_comment": "Accepted as a CVPR 2020 oral paper. arXiv admin note: text overlap\n  with arXiv:1803.08494 by other authors", "links": [{"href": "http://arxiv.org/abs/1912.05845v3", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1912.05845v3", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1912.05845v3", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1912.05845v3", "journal_reference": "CVPR 2020", "doi": null}
{"id": "http://arxiv.org/abs/1912.10178v1", "guidislink": true, "updated": "2019-12-21T02:11:47Z", "updated_parsed": [2019, 12, 21, 2, 11, 47, 5, 355, 0], "published": "2019-12-21T02:11:47Z", "published_parsed": [2019, 12, 21, 2, 11, 47, 5, 355, 0], "title": "DBP: Discrimination Based Block-Level Pruning for Deep Model\n  Acceleration", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=1110&max_results=10&sortBy=relevance&sortOrder=descending", "value": "DBP: Discrimination Based Block-Level Pruning for Deep Model\n  Acceleration"}, "summary": "Neural network pruning is one of the most popular methods of accelerating the\ninference of deep convolutional neural networks (CNNs). The dominant pruning\nmethods, filter-level pruning methods, evaluate their performance through the\nreduction ratio of computations and deem that a higher reduction ratio of\ncomputations is equivalent to a higher acceleration ratio in terms of inference\ntime. However, we argue that they are not equivalent if parallel computing is\nconsidered. Given that filter-level pruning only prunes filters in layers and\ncomputations in a layer usually run in parallel, most computations reduced by\nfilter-level pruning usually run in parallel with the un-reduced ones. Thus,\nthe acceleration ratio of filter-level pruning is limited. To get a higher\nacceleration ratio, it is better to prune redundant layers because computations\nof different layers cannot run in parallel. In this paper, we propose our\nDiscrimination based Block-level Pruning method (DBP). Specifically, DBP takes\na sequence of consecutive layers (e.g., Conv-BN-ReLu) as a block and removes\nredundant blocks according to the discrimination of their output features. As a\nresult, DBP achieves a considerable acceleration ratio by reducing the depth of\nCNNs. Extensive experiments show that DBP has surpassed state-of-the-art\nfilter-level pruning methods in both accuracy and acceleration ratio. Our code\nwill be made available soon.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=1110&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Neural network pruning is one of the most popular methods of accelerating the\ninference of deep convolutional neural networks (CNNs). The dominant pruning\nmethods, filter-level pruning methods, evaluate their performance through the\nreduction ratio of computations and deem that a higher reduction ratio of\ncomputations is equivalent to a higher acceleration ratio in terms of inference\ntime. However, we argue that they are not equivalent if parallel computing is\nconsidered. Given that filter-level pruning only prunes filters in layers and\ncomputations in a layer usually run in parallel, most computations reduced by\nfilter-level pruning usually run in parallel with the un-reduced ones. Thus,\nthe acceleration ratio of filter-level pruning is limited. To get a higher\nacceleration ratio, it is better to prune redundant layers because computations\nof different layers cannot run in parallel. In this paper, we propose our\nDiscrimination based Block-level Pruning method (DBP). Specifically, DBP takes\na sequence of consecutive layers (e.g., Conv-BN-ReLu) as a block and removes\nredundant blocks according to the discrimination of their output features. As a\nresult, DBP achieves a considerable acceleration ratio by reducing the depth of\nCNNs. Extensive experiments show that DBP has surpassed state-of-the-art\nfilter-level pruning methods in both accuracy and acceleration ratio. Our code\nwill be made available soon."}, "authors": ["Wenxiao Wang", "Shuai Zhao", "Minghao Chen", "Jinming Hu", "Deng Cai", "Haifeng Liu"], "author_detail": {"name": "Haifeng Liu"}, "author": "Haifeng Liu", "arxiv_comment": "9 pages, 5 figures", "links": [{"href": "http://arxiv.org/abs/1912.10178v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1912.10178v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1912.10178v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1912.10178v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/2003.08761v2", "guidislink": true, "updated": "2020-03-20T14:58:40Z", "updated_parsed": [2020, 3, 20, 14, 58, 40, 4, 80, 0], "published": "2020-03-19T13:23:40Z", "published_parsed": [2020, 3, 19, 13, 23, 40, 3, 79, 0], "title": "Exemplar Normalization for Learning Deep Representation", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=1120&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Exemplar Normalization for Learning Deep Representation"}, "summary": "Normalization techniques are important in different advanced neural networks\nand different tasks. This work investigates a novel dynamic\nlearning-to-normalize (L2N) problem by proposing Exemplar Normalization (EN),\nwhich is able to learn different normalization methods for different\nconvolutional layers and image samples of a deep network. EN significantly\nimproves flexibility of the recently proposed switchable normalization (SN),\nwhich solves a static L2N problem by linearly combining several normalizers in\neach normalization layer (the combination is the same for all samples). Instead\nof directly employing a multi-layer perceptron (MLP) to learn data-dependent\nparameters as conditional batch normalization (cBN) did, the internal\narchitecture of EN is carefully designed to stabilize its optimization, leading\nto many appealing benefits. (1) EN enables different convolutional layers,\nimage samples, categories, benchmarks, and tasks to use different normalization\nmethods, shedding light on analyzing them in a holistic view. (2) EN is\neffective for various network architectures and tasks. (3) It could replace any\nnormalization layers in a deep network and still produce stable model training.\nExtensive experiments demonstrate the effectiveness of EN in a wide spectrum of\ntasks including image recognition, noisy label learning, and semantic\nsegmentation. For example, by replacing BN in the ordinary ResNet50,\nimprovement produced by EN is 300% more than that of SN on both ImageNet and\nthe noisy WebVision dataset.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=1120&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Normalization techniques are important in different advanced neural networks\nand different tasks. This work investigates a novel dynamic\nlearning-to-normalize (L2N) problem by proposing Exemplar Normalization (EN),\nwhich is able to learn different normalization methods for different\nconvolutional layers and image samples of a deep network. EN significantly\nimproves flexibility of the recently proposed switchable normalization (SN),\nwhich solves a static L2N problem by linearly combining several normalizers in\neach normalization layer (the combination is the same for all samples). Instead\nof directly employing a multi-layer perceptron (MLP) to learn data-dependent\nparameters as conditional batch normalization (cBN) did, the internal\narchitecture of EN is carefully designed to stabilize its optimization, leading\nto many appealing benefits. (1) EN enables different convolutional layers,\nimage samples, categories, benchmarks, and tasks to use different normalization\nmethods, shedding light on analyzing them in a holistic view. (2) EN is\neffective for various network architectures and tasks. (3) It could replace any\nnormalization layers in a deep network and still produce stable model training.\nExtensive experiments demonstrate the effectiveness of EN in a wide spectrum of\ntasks including image recognition, noisy label learning, and semantic\nsegmentation. For example, by replacing BN in the ordinary ResNet50,\nimprovement produced by EN is 300% more than that of SN on both ImageNet and\nthe noisy WebVision dataset."}, "authors": ["Ruimao Zhang", "Zhanglin Peng", "Lingyun Wu", "Zhen Li", "Ping Luo"], "author_detail": {"name": "Ping Luo"}, "author": "Ping Luo", "arxiv_comment": "Accepted by CVPR2020, normalization methods, image classification", "links": [{"href": "http://arxiv.org/abs/2003.08761v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2003.08761v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2003.08761v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2003.08761v2", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/2004.01461v2", "guidislink": true, "updated": "2020-04-08T03:40:44Z", "updated_parsed": [2020, 4, 8, 3, 40, 44, 2, 99, 0], "published": "2020-04-03T10:25:00Z", "published_parsed": [2020, 4, 3, 10, 25, 0, 4, 94, 0], "title": "Gradient Centralization: A New Optimization Technique for Deep Neural\n  Networks", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=1120&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Gradient Centralization: A New Optimization Technique for Deep Neural\n  Networks"}, "summary": "Optimization techniques are of great importance to effectively and\nefficiently train a deep neural network (DNN). It has been shown that using the\nfirst and second order statistics (e.g., mean and variance) to perform Z-score\nstandardization on network activations or weight vectors, such as batch\nnormalization (BN) and weight standardization (WS), can improve the training\nperformance. Different from these existing methods that mostly operate on\nactivations or weights, we present a new optimization technique, namely\ngradient centralization (GC), which operates directly on gradients by\ncentralizing the gradient vectors to have zero mean. GC can be viewed as a\nprojected gradient descent method with a constrained loss function. We show\nthat GC can regularize both the weight space and output feature space so that\nit can boost the generalization performance of DNNs. Moreover, GC improves the\nLipschitzness of the loss function and its gradient so that the training\nprocess becomes more efficient and stable. GC is very simple to implement and\ncan be easily embedded into existing gradient based DNN optimizers with only\none line of code. It can also be directly used to fine-tune the pre-trained\nDNNs. Our experiments on various applications, including general image\nclassification, fine-grained image classification, detection and segmentation,\ndemonstrate that GC can consistently improve the performance of DNN learning.\nThe code of GC can be found at\nhttps://github.com/Yonghongwei/Gradient-Centralization.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=1120&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Optimization techniques are of great importance to effectively and\nefficiently train a deep neural network (DNN). It has been shown that using the\nfirst and second order statistics (e.g., mean and variance) to perform Z-score\nstandardization on network activations or weight vectors, such as batch\nnormalization (BN) and weight standardization (WS), can improve the training\nperformance. Different from these existing methods that mostly operate on\nactivations or weights, we present a new optimization technique, namely\ngradient centralization (GC), which operates directly on gradients by\ncentralizing the gradient vectors to have zero mean. GC can be viewed as a\nprojected gradient descent method with a constrained loss function. We show\nthat GC can regularize both the weight space and output feature space so that\nit can boost the generalization performance of DNNs. Moreover, GC improves the\nLipschitzness of the loss function and its gradient so that the training\nprocess becomes more efficient and stable. GC is very simple to implement and\ncan be easily embedded into existing gradient based DNN optimizers with only\none line of code. It can also be directly used to fine-tune the pre-trained\nDNNs. Our experiments on various applications, including general image\nclassification, fine-grained image classification, detection and segmentation,\ndemonstrate that GC can consistently improve the performance of DNN learning.\nThe code of GC can be found at\nhttps://github.com/Yonghongwei/Gradient-Centralization."}, "authors": ["Hongwei Yong", "Jianqiang Huang", "Xiansheng Hua", "Lei Zhang"], "author_detail": {"name": "Lei Zhang"}, "author": "Lei Zhang", "arxiv_comment": "20 pages, 7 figures, conference", "links": [{"href": "http://arxiv.org/abs/2004.01461v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2004.01461v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2004.01461v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2004.01461v2", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/2004.04571v2", "guidislink": true, "updated": "2020-06-05T14:16:49Z", "updated_parsed": [2020, 6, 5, 14, 16, 49, 4, 157, 0], "published": "2020-04-09T14:44:11Z", "published_parsed": [2020, 4, 9, 14, 44, 11, 3, 100, 0], "title": "Learning Bayesian Networks that enable full propagation of evidence", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=1130&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Learning Bayesian Networks that enable full propagation of evidence"}, "summary": "This paper builds on recent developments in Bayesian network (BN) structure\nlearning under the controversial assumption that the input variables are\ndependent. This assumption can be viewed as a learning constraint geared\ntowards cases where the input variables are known or assumed to be dependent.\nIt addresses the problem of learning multiple disjoint subgraphs that do not\nenable full propagation of evidence. This problem is highly prevalent in cases\nwhere the sample size of the input data is low with respect to the\ndimensionality of the model, which is often the case when working with real\ndata. The paper presents a novel hybrid structure learning algorithm, called\nSaiyanH, that addresses this issue. The results show that this constraint helps\nthe algorithm to estimate the number of true edges with higher accuracy\ncompared to the state-of-the-art. Out of the 13 algorithms investigated, the\nresults rank SaiyanH 4th in reconstructing the true DAG, with accuracy scores\nlower by 8.1% (F1), 10.2% (BSF), and 19.5% (SHD) compared to the top ranked\nalgorithm, and higher by 75.5% (F1), 118% (BSF), and 4.3% (SHD) compared to the\nbottom ranked algorithm. Overall, the results suggest that the proposed\nalgorithm discovers satisfactorily accurate connected DAGs in cases where other\nalgorithms produce multiple disjoint subgraphs that often underfit the true\ngraph.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=1130&max_results=10&sortBy=relevance&sortOrder=descending", "value": "This paper builds on recent developments in Bayesian network (BN) structure\nlearning under the controversial assumption that the input variables are\ndependent. This assumption can be viewed as a learning constraint geared\ntowards cases where the input variables are known or assumed to be dependent.\nIt addresses the problem of learning multiple disjoint subgraphs that do not\nenable full propagation of evidence. This problem is highly prevalent in cases\nwhere the sample size of the input data is low with respect to the\ndimensionality of the model, which is often the case when working with real\ndata. The paper presents a novel hybrid structure learning algorithm, called\nSaiyanH, that addresses this issue. The results show that this constraint helps\nthe algorithm to estimate the number of true edges with higher accuracy\ncompared to the state-of-the-art. Out of the 13 algorithms investigated, the\nresults rank SaiyanH 4th in reconstructing the true DAG, with accuracy scores\nlower by 8.1% (F1), 10.2% (BSF), and 19.5% (SHD) compared to the top ranked\nalgorithm, and higher by 75.5% (F1), 118% (BSF), and 4.3% (SHD) compared to the\nbottom ranked algorithm. Overall, the results suggest that the proposed\nalgorithm discovers satisfactorily accurate connected DAGs in cases where other\nalgorithms produce multiple disjoint subgraphs that often underfit the true\ngraph."}, "authors": ["Anthony Constantinou"], "author_detail": {"name": "Anthony Constantinou"}, "author": "Anthony Constantinou", "links": [{"href": "http://arxiv.org/abs/2004.04571v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2004.04571v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.ML", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2004.04571v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2004.04571v2", "arxiv_comment": null, "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/2011.08609v1", "guidislink": true, "updated": "2020-11-17T13:07:51Z", "updated_parsed": [2020, 11, 17, 13, 7, 51, 1, 322, 0], "published": "2020-11-17T13:07:51Z", "published_parsed": [2020, 11, 17, 13, 7, 51, 1, 322, 0], "title": "Accent and Speaker Disentanglement in Many-to-many Voice Conversion", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=1140&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Accent and Speaker Disentanglement in Many-to-many Voice Conversion"}, "summary": "This paper proposes an interesting voice and accent joint conversion\napproach, which can convert an arbitrary source speaker's voice to a target\nspeaker with non-native accent. This problem is challenging as each target\nspeaker only has training data in native accent and we need to disentangle\naccent and speaker information in the conversion model training and re-combine\nthem in the conversion stage. In our recognition-synthesis conversion\nframework, we manage to solve this problem by two proposed tricks. First, we\nuse accent-dependent speech recognizers to obtain bottleneck features for\ndifferent accented speakers. This aims to wipe out other factors beyond the\nlinguistic information in the BN features for conversion model training.\nSecond, we propose to use adversarial training to better disentangle the\nspeaker and accent information in our encoder-decoder based conversion model.\nSpecifically, we plug an auxiliary speaker classifier to the encoder, trained\nwith an adversarial loss to wipe out speaker information from the encoder\noutput. Experiments show that our approach is superior to the baseline. The\nproposed tricks are quite effective in improving accentedness and audio quality\nand speaker similarity are well maintained.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=1140&max_results=10&sortBy=relevance&sortOrder=descending", "value": "This paper proposes an interesting voice and accent joint conversion\napproach, which can convert an arbitrary source speaker's voice to a target\nspeaker with non-native accent. This problem is challenging as each target\nspeaker only has training data in native accent and we need to disentangle\naccent and speaker information in the conversion model training and re-combine\nthem in the conversion stage. In our recognition-synthesis conversion\nframework, we manage to solve this problem by two proposed tricks. First, we\nuse accent-dependent speech recognizers to obtain bottleneck features for\ndifferent accented speakers. This aims to wipe out other factors beyond the\nlinguistic information in the BN features for conversion model training.\nSecond, we propose to use adversarial training to better disentangle the\nspeaker and accent information in our encoder-decoder based conversion model.\nSpecifically, we plug an auxiliary speaker classifier to the encoder, trained\nwith an adversarial loss to wipe out speaker information from the encoder\noutput. Experiments show that our approach is superior to the baseline. The\nproposed tricks are quite effective in improving accentedness and audio quality\nand speaker similarity are well maintained."}, "authors": ["Zhichao Wang", "Wenshuo Ge", "Xiong Wang", "Shan Yang", "Wendong Gan", "Haitao Chen", "Hai Li", "Lei Xie", "Xiulin Li"], "author_detail": {"name": "Xiulin Li"}, "author": "Xiulin Li", "arxiv_comment": "Accepted to ISCSLP2021", "links": [{"href": "http://arxiv.org/abs/2011.08609v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2011.08609v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.SD", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.SD", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "eess.AS", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2011.08609v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2011.08609v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/2101.06663v1", "guidislink": true, "updated": "2021-01-17T13:04:06Z", "updated_parsed": [2021, 1, 17, 13, 4, 6, 6, 17, 0], "published": "2021-01-17T13:04:06Z", "published_parsed": [2021, 1, 17, 13, 4, 6, 6, 17, 0], "title": "Separable Batch Normalization for Robust Facial Landmark Localization\n  with Cross-protocol Network Training", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=1140&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Separable Batch Normalization for Robust Facial Landmark Localization\n  with Cross-protocol Network Training"}, "summary": "A big, diverse and balanced training data is the key to the success of deep\nneural network training. However, existing publicly available datasets used in\nfacial landmark localization are usually much smaller than those for other\ncomputer vision tasks. A small dataset without diverse and balanced training\nsamples cannot support the training of a deep network effectively. To address\nthe above issues, this paper presents a novel Separable Batch Normalization\n(SepBN) module with a Cross-protocol Network Training (CNT) strategy for robust\nfacial landmark localization. Different from the standard BN layer that uses\nall the training data to calculate a single set of parameters, SepBN considers\nthat the samples of a training dataset may belong to different sub-domains.\nAccordingly, the proposed SepBN module uses multiple sets of parameters, each\ncorresponding to a specific sub-domain. However, the selection of an\nappropriate branch in the inference stage remains a challenging task because\nthe sub-domain of a test sample is unknown. To mitigate this difficulty, we\npropose a novel attention mechanism that assigns different weights to each\nbranch for automatic selection in an effective style. As a further innovation,\nthe proposed CNT strategy trains a network using multiple datasets having\ndifferent facial landmark annotation systems, boosting the performance and\nenhancing the generalization capacity of the trained network. The experimental\nresults obtained on several well-known datasets demonstrate the effectiveness\nof the proposed method.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=1140&max_results=10&sortBy=relevance&sortOrder=descending", "value": "A big, diverse and balanced training data is the key to the success of deep\nneural network training. However, existing publicly available datasets used in\nfacial landmark localization are usually much smaller than those for other\ncomputer vision tasks. A small dataset without diverse and balanced training\nsamples cannot support the training of a deep network effectively. To address\nthe above issues, this paper presents a novel Separable Batch Normalization\n(SepBN) module with a Cross-protocol Network Training (CNT) strategy for robust\nfacial landmark localization. Different from the standard BN layer that uses\nall the training data to calculate a single set of parameters, SepBN considers\nthat the samples of a training dataset may belong to different sub-domains.\nAccordingly, the proposed SepBN module uses multiple sets of parameters, each\ncorresponding to a specific sub-domain. However, the selection of an\nappropriate branch in the inference stage remains a challenging task because\nthe sub-domain of a test sample is unknown. To mitigate this difficulty, we\npropose a novel attention mechanism that assigns different weights to each\nbranch for automatic selection in an effective style. As a further innovation,\nthe proposed CNT strategy trains a network using multiple datasets having\ndifferent facial landmark annotation systems, boosting the performance and\nenhancing the generalization capacity of the trained network. The experimental\nresults obtained on several well-known datasets demonstrate the effectiveness\nof the proposed method."}, "authors": ["Shuangping Jin", "Zhenhua Feng", "Wankou Yang", "Josef Kittler"], "author_detail": {"name": "Josef Kittler"}, "author": "Josef Kittler", "arxiv_comment": "10 pages,6 figures", "links": [{"href": "http://arxiv.org/abs/2101.06663v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2101.06663v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2101.06663v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2101.06663v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1808.05578v1", "guidislink": true, "updated": "2018-08-16T16:48:56Z", "updated_parsed": [2018, 8, 16, 16, 48, 56, 3, 228, 0], "published": "2018-08-16T16:48:56Z", "published_parsed": [2018, 8, 16, 16, 48, 56, 3, 228, 0], "title": "LARNN: Linear Attention Recurrent Neural Network", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=1190&max_results=10&sortBy=relevance&sortOrder=descending", "value": "LARNN: Linear Attention Recurrent Neural Network"}, "summary": "The Linear Attention Recurrent Neural Network (LARNN) is a recurrent\nattention module derived from the Long Short-Term Memory (LSTM) cell and ideas\nfrom the consciousness Recurrent Neural Network (RNN). Yes, it LARNNs. The\nLARNN uses attention on its past cell state values for a limited window size\n$k$. The formulas are also derived from the Batch Normalized LSTM (BN-LSTM)\ncell and the Transformer Network for its Multi-Head Attention Mechanism. The\nMulti-Head Attention Mechanism is used inside the cell such that it can query\nits own $k$ past values with the attention window. This has the effect of\naugmenting the rank of the tensor with the attention mechanism, such that the\ncell can perform complex queries to question its previous inner memories, which\nshould augment the long short-term effect of the memory. With a clever trick,\nthe LARNN cell with attention can be easily used inside a loop on the cell\nstate, just like how any other Recurrent Neural Network (RNN) cell can be\nlooped linearly through time series. This is due to the fact that its state,\nwhich is looped upon throughout time steps within time series, stores the inner\nstates in a \"first in, first out\" queue which contains the $k$ most recent\nstates and on which it is easily possible to add static positional encoding\nwhen the queue is represented as a tensor. This neural architecture yields\nbetter results than the vanilla LSTM cells. It can obtain results of 91.92% for\nthe test accuracy, compared to the previously attained 91.65% using vanilla\nLSTM cells. Note that this is not to compare to other research, where up to\n93.35% is obtained, but costly using 18 LSTM cells rather than with 2 to 3\ncells as analyzed here. Finally, an interesting discovery is made, such that\nadding activation within the multi-head attention mechanism's linear layers can\nyield better results in the context researched hereto.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=1190&max_results=10&sortBy=relevance&sortOrder=descending", "value": "The Linear Attention Recurrent Neural Network (LARNN) is a recurrent\nattention module derived from the Long Short-Term Memory (LSTM) cell and ideas\nfrom the consciousness Recurrent Neural Network (RNN). Yes, it LARNNs. The\nLARNN uses attention on its past cell state values for a limited window size\n$k$. The formulas are also derived from the Batch Normalized LSTM (BN-LSTM)\ncell and the Transformer Network for its Multi-Head Attention Mechanism. The\nMulti-Head Attention Mechanism is used inside the cell such that it can query\nits own $k$ past values with the attention window. This has the effect of\naugmenting the rank of the tensor with the attention mechanism, such that the\ncell can perform complex queries to question its previous inner memories, which\nshould augment the long short-term effect of the memory. With a clever trick,\nthe LARNN cell with attention can be easily used inside a loop on the cell\nstate, just like how any other Recurrent Neural Network (RNN) cell can be\nlooped linearly through time series. This is due to the fact that its state,\nwhich is looped upon throughout time steps within time series, stores the inner\nstates in a \"first in, first out\" queue which contains the $k$ most recent\nstates and on which it is easily possible to add static positional encoding\nwhen the queue is represented as a tensor. This neural architecture yields\nbetter results than the vanilla LSTM cells. It can obtain results of 91.92% for\nthe test accuracy, compared to the previously attained 91.65% using vanilla\nLSTM cells. Note that this is not to compare to other research, where up to\n93.35% is obtained, but costly using 18 LSTM cells rather than with 2 to 3\ncells as analyzed here. Finally, an interesting discovery is made, such that\nadding activation within the multi-head attention mechanism's linear layers can\nyield better results in the context researched hereto."}, "authors": ["Guillaume Chevalier"], "author_detail": {"name": "Guillaume Chevalier"}, "author": "Guillaume Chevalier", "arxiv_comment": "14 pages, 10 figures", "links": [{"href": "http://arxiv.org/abs/1808.05578v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1808.05578v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.CC", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.ML", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1808.05578v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1808.05578v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1903.00705v1", "guidislink": true, "updated": "2019-03-02T14:10:46Z", "updated_parsed": [2019, 3, 2, 14, 10, 46, 5, 61, 0], "published": "2019-03-02T14:10:46Z", "published_parsed": [2019, 3, 2, 14, 10, 46, 5, 61, 0], "title": "Deep Optimization model for Screen Content Image Quality Assessment\n  using Neural Networks", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=1190&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Deep Optimization model for Screen Content Image Quality Assessment\n  using Neural Networks"}, "summary": "In this paper, we propose a novel quadratic optimized model based on the deep\nconvolutional neural network (QODCNN) for full-reference and no-reference\nscreen content image (SCI) quality assessment. Unlike traditional CNN methods\ntaking all image patches as training data and using average quality pooling,\nour model is optimized to obtain a more effective model including three steps.\nIn the first step, an end-to-end deep CNN is trained to preliminarily predict\nthe image visual quality, and batch normalized (BN) layers and l2\nregularization are employed to improve the speed and performance of network\nfitting. For second step, the pretrained model is fine-tuned to achieve better\nperformance under analysis of the raw training data. An adaptive weighting\nmethod is proposed in the third step to fuse local quality inspired by the\nperceptual property of the human visual system (HVS) that the HVS is sensitive\nto image patches containing texture and edge information. The novelty of our\nalgorithm can be concluded as follows: 1) with the consideration of correlation\nbetween local quality and subjective differential mean opinion score (DMOS),\nthe Euclidean distance is utilized to measure effectiveness of image patches,\nand the pretrained model is fine-tuned with more effective training data; 2) an\nadaptive pooling approach is employed to fuse patch quality of textual and\npictorial regions, whose feature only extracted from distorted images owns\nstrong noise robust and effects on both FR and NR IQA; 3) Considering the\ncharacteristics of SCIs, a deep and valid network architecture is designed for\nboth NR and FR visual quality evaluation of SCIs. Experimental results verify\nthat our model outperforms both current no-reference and full-reference image\nquality assessment methods on the benchmark screen content image quality\nassessment database (SIQAD).", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=1190&max_results=10&sortBy=relevance&sortOrder=descending", "value": "In this paper, we propose a novel quadratic optimized model based on the deep\nconvolutional neural network (QODCNN) for full-reference and no-reference\nscreen content image (SCI) quality assessment. Unlike traditional CNN methods\ntaking all image patches as training data and using average quality pooling,\nour model is optimized to obtain a more effective model including three steps.\nIn the first step, an end-to-end deep CNN is trained to preliminarily predict\nthe image visual quality, and batch normalized (BN) layers and l2\nregularization are employed to improve the speed and performance of network\nfitting. For second step, the pretrained model is fine-tuned to achieve better\nperformance under analysis of the raw training data. An adaptive weighting\nmethod is proposed in the third step to fuse local quality inspired by the\nperceptual property of the human visual system (HVS) that the HVS is sensitive\nto image patches containing texture and edge information. The novelty of our\nalgorithm can be concluded as follows: 1) with the consideration of correlation\nbetween local quality and subjective differential mean opinion score (DMOS),\nthe Euclidean distance is utilized to measure effectiveness of image patches,\nand the pretrained model is fine-tuned with more effective training data; 2) an\nadaptive pooling approach is employed to fuse patch quality of textual and\npictorial regions, whose feature only extracted from distorted images owns\nstrong noise robust and effects on both FR and NR IQA; 3) Considering the\ncharacteristics of SCIs, a deep and valid network architecture is designed for\nboth NR and FR visual quality evaluation of SCIs. Experimental results verify\nthat our model outperforms both current no-reference and full-reference image\nquality assessment methods on the benchmark screen content image quality\nassessment database (SIQAD)."}, "authors": ["Xuhao Jiang", "Liquan Shen", "Guorui Feng", "Liangwei Yu", "Ping An"], "author_detail": {"name": "Ping An"}, "author": "Ping An", "arxiv_comment": "12pages, 9 figures", "links": [{"href": "http://arxiv.org/abs/1903.00705v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1903.00705v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1903.00705v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1903.00705v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1907.04406v2", "guidislink": true, "updated": "2019-10-20T18:46:29Z", "updated_parsed": [2019, 10, 20, 18, 46, 29, 6, 293, 0], "published": "2019-07-09T20:42:48Z", "published_parsed": [2019, 7, 9, 20, 42, 48, 1, 190, 0], "title": "Faster provable sieving algorithms for the Shortest Vector Problem and\n  the Closest Vector Problem on lattices in $\\ell_p$ norm", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=1200&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Faster provable sieving algorithms for the Shortest Vector Problem and\n  the Closest Vector Problem on lattices in $\\ell_p$ norm"}, "summary": "In this paper we give provable sieving algorithms for the Shortest Vector\nProblem (SVP) and the Closest Vector Problem (CVP) on lattices in $\\ell_p$ norm\nfor $1\\leq p\\leq\\infty$. The running time we get is better than existing\nprovable sieving algorithms, except the Discrete Gaussian based algorithm by\nAggarwal et al. [2015], but this algorithm works only for the Euclidean norm.\nWe build on the randomized sieving framework of Ajtai, Kumar and Sivakumar\n[2001,2002], where they used a sieving sub-routine that runs in time quadratic\nin the number of sampled vectors. We give a new sieving procedure that works\nfor all $\\ell_p$ norm and runs in time linear in the number of sampled vectors.\nThe main idea is to divide the space (hyperball) into sub-regions (hypercubes)\nsuch that each vector can be mapped efficiently to a sub-region. This is an\nextension of the sieving technique in Aggarwal and Mukhopadhyay [2018], where\nit has been used only for the $\\ell_{\\infty}$ norm. Prior to these works Blomer\nand Naewe [2009] generalised the AKS algorithm and our analysis shows that\ntheir algorithm can achieve a time complexity of $2^{3.849n+o(n)}$, while our\nalgorithm has a running time $2^{2.751n+o(n)}$.\n  We further modify our linear sieving technique and introduce a mixed sieving\nprocedure. At first a point is mapped to a hypercube within a ball and then\nwithin each hypercube we perform a quadratic seve like AKS. This improves the\nrunning time, specially in the $\\ell_2$ norm, where we achieve a time\ncomplexity of $2^{2.25n+o(n)}$, while the List Sieve Birthday algorithm [Pujol\nan Stehle, 2009] has a running time $2^{2.465n+o(n)}$ in the same norm. We also\nadopt our sieving techniques to approximation algorithms for SVP and CVP in\n$\\ell_p$ norm and achieve a running time of $2^{2.001n+o(n)}$, while algorithms\nlike [BN, 2009] have a time complexity of $2^{3.169n+o(n)}$.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=1200&max_results=10&sortBy=relevance&sortOrder=descending", "value": "In this paper we give provable sieving algorithms for the Shortest Vector\nProblem (SVP) and the Closest Vector Problem (CVP) on lattices in $\\ell_p$ norm\nfor $1\\leq p\\leq\\infty$. The running time we get is better than existing\nprovable sieving algorithms, except the Discrete Gaussian based algorithm by\nAggarwal et al. [2015], but this algorithm works only for the Euclidean norm.\nWe build on the randomized sieving framework of Ajtai, Kumar and Sivakumar\n[2001,2002], where they used a sieving sub-routine that runs in time quadratic\nin the number of sampled vectors. We give a new sieving procedure that works\nfor all $\\ell_p$ norm and runs in time linear in the number of sampled vectors.\nThe main idea is to divide the space (hyperball) into sub-regions (hypercubes)\nsuch that each vector can be mapped efficiently to a sub-region. This is an\nextension of the sieving technique in Aggarwal and Mukhopadhyay [2018], where\nit has been used only for the $\\ell_{\\infty}$ norm. Prior to these works Blomer\nand Naewe [2009] generalised the AKS algorithm and our analysis shows that\ntheir algorithm can achieve a time complexity of $2^{3.849n+o(n)}$, while our\nalgorithm has a running time $2^{2.751n+o(n)}$.\n  We further modify our linear sieving technique and introduce a mixed sieving\nprocedure. At first a point is mapped to a hypercube within a ball and then\nwithin each hypercube we perform a quadratic seve like AKS. This improves the\nrunning time, specially in the $\\ell_2$ norm, where we achieve a time\ncomplexity of $2^{2.25n+o(n)}$, while the List Sieve Birthday algorithm [Pujol\nan Stehle, 2009] has a running time $2^{2.465n+o(n)}$ in the same norm. We also\nadopt our sieving techniques to approximation algorithms for SVP and CVP in\n$\\ell_p$ norm and achieve a running time of $2^{2.001n+o(n)}$, while algorithms\nlike [BN, 2009] have a time complexity of $2^{3.169n+o(n)}$."}, "authors": ["Priyanka Mukhopadhyay"], "author_detail": {"name": "Priyanka Mukhopadhyay"}, "author": "Priyanka Mukhopadhyay", "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:1801.02358", "links": [{"href": "http://arxiv.org/abs/1907.04406v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1907.04406v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.DS", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.DS", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1907.04406v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1907.04406v2", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/1908.04265v1", "guidislink": true, "updated": "2019-07-22T11:07:55Z", "updated_parsed": [2019, 7, 22, 11, 7, 55, 0, 203, 0], "published": "2019-07-22T11:07:55Z", "published_parsed": [2019, 7, 22, 11, 7, 55, 0, 203, 0], "title": "Recursion, Probability, Convolution and Classification for Computations", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=1200&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Recursion, Probability, Convolution and Classification for Computations"}, "summary": "The main motivation of this work was practical, to offer computationally and\ntheoretical scalable ways to structuring large classes of computation. It\nstarted from attempts to optimize R code for machine learning/artificial\nintelligence algorithms for huge data sets, that due to their size, should be\nhandled into an incremental (online) fashion.\n  Our target are large classes of relational (attribute based), mathematical\n(index based) or graph computations. We wanted to use powerful computation\nrepresentations that emerged in AI (artificial intelligence)/ML (machine\nlearning) as BN (Bayesian networks) and CNN (convolution neural networks). For\nthe classes of computation addressed by us, and for our HPC (high performance\ncomputing) needs, the current solutions for translating computations into such\nrepresentation need to be extended.\n  Our results show that the classes of computation targeted by us, could be\ntree-structured, and a probability distribution (defining a DBN, i.e. Dynamic\nBayesian Network) associated with it. More ever, this DBN may be viewed as a\nrecursive CNN (Convolution Neural Network). Within this tree-like structure,\nclassification in classes with size bounded (by a parameterizable may be\nperformed.\n  These results are at the core of very powerful, yet highly practically\nalgorithms for restructuring and parallelizing the computations. The\nmathematical background required for an in depth presentation and exposing the\nfull generality of our approach) is the subject of a subsequent paper. In this\npaper, we work in an limited (but important) framework that could be understood\nwith rudiments of linear algebra and graph theory. The focus is in\napplicability, most of this paper discuss the usefulness of our approach for\nsolving hard compilation problems related to automatic parallelism.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=1200&max_results=10&sortBy=relevance&sortOrder=descending", "value": "The main motivation of this work was practical, to offer computationally and\ntheoretical scalable ways to structuring large classes of computation. It\nstarted from attempts to optimize R code for machine learning/artificial\nintelligence algorithms for huge data sets, that due to their size, should be\nhandled into an incremental (online) fashion.\n  Our target are large classes of relational (attribute based), mathematical\n(index based) or graph computations. We wanted to use powerful computation\nrepresentations that emerged in AI (artificial intelligence)/ML (machine\nlearning) as BN (Bayesian networks) and CNN (convolution neural networks). For\nthe classes of computation addressed by us, and for our HPC (high performance\ncomputing) needs, the current solutions for translating computations into such\nrepresentation need to be extended.\n  Our results show that the classes of computation targeted by us, could be\ntree-structured, and a probability distribution (defining a DBN, i.e. Dynamic\nBayesian Network) associated with it. More ever, this DBN may be viewed as a\nrecursive CNN (Convolution Neural Network). Within this tree-like structure,\nclassification in classes with size bounded (by a parameterizable may be\nperformed.\n  These results are at the core of very powerful, yet highly practically\nalgorithms for restructuring and parallelizing the computations. The\nmathematical background required for an in depth presentation and exposing the\nfull generality of our approach) is the subject of a subsequent paper. In this\npaper, we work in an limited (but important) framework that could be understood\nwith rudiments of linear algebra and graph theory. The focus is in\napplicability, most of this paper discuss the usefulness of our approach for\nsolving hard compilation problems related to automatic parallelism."}, "authors": ["Mircea Namolaru", "Thierry Goubier"], "author_detail": {"name": "Thierry Goubier"}, "author": "Thierry Goubier", "arxiv_comment": "17 pages, revised version of a NIPS 2019 submission (with the same\n  title), combined with a revised (rejected) PLDI 2019 submission (with the\n  title \"Rubik's cube and Bayesian Networks\")", "links": [{"href": "http://arxiv.org/abs/1908.04265v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1908.04265v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.PL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.PL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1908.04265v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1908.04265v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/2008.10148v1", "guidislink": true, "updated": "2020-08-24T01:19:40Z", "updated_parsed": [2020, 8, 24, 1, 19, 40, 0, 237, 0], "published": "2020-08-24T01:19:40Z", "published_parsed": [2020, 8, 24, 1, 19, 40, 0, 237, 0], "title": "Drive Safe: Cognitive-Behavioral Mining for Intelligent Transportation\n  Cyber-Physical System", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=1210&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Drive Safe: Cognitive-Behavioral Mining for Intelligent Transportation\n  Cyber-Physical System"}, "summary": "This paper presents a cognitive behavioral-based driver mood repairment\nplatform in intelligent transportation cyber-physical systems (IT-CPS) for road\nsafety. In particular, we propose a driving safety platform for distracted\ndrivers, namely \\emph{drive safe}, in IT-CPS. The proposed platform recognizes\nthe distracting activities of the drivers as well as their emotions for mood\nrepair. Further, we develop a prototype of the proposed drive safe platform to\nestablish proof-of-concept (PoC) for the road safety in IT-CPS. In the\ndeveloped driving safety platform, we employ five AI and statistical-based\nmodels to infer a vehicle driver's cognitive-behavioral mining to ensure safe\ndriving during the drive. Especially, capsule network (CN), maximum likelihood\n(ML), convolutional neural network (CNN), Apriori algorithm, and Bayesian\nnetwork (BN) are deployed for driver activity recognition, environmental\nfeature extraction, mood recognition, sequential pattern mining, and content\nrecommendation for affective mood repairment of the driver, respectively.\nBesides, we develop a communication module to interact with the systems in\nIT-CPS asynchronously. Thus, the developed drive safe PoC can guide the vehicle\ndrivers when they are distracted from driving due to the cognitive-behavioral\nfactors. Finally, we have performed a qualitative evaluation to measure the\nusability and effectiveness of the developed drive safe platform. We observe\nthat the P-value is 0.0041 (i.e., < 0.05) in the ANOVA test. Moreover, the\nconfidence interval analysis also shows significant gains in prevalence value\nwhich is around 0.93 for a 95% confidence level. The aforementioned statistical\nresults indicate high reliability in terms of driver's safety and mental state.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=1210&max_results=10&sortBy=relevance&sortOrder=descending", "value": "This paper presents a cognitive behavioral-based driver mood repairment\nplatform in intelligent transportation cyber-physical systems (IT-CPS) for road\nsafety. In particular, we propose a driving safety platform for distracted\ndrivers, namely \\emph{drive safe}, in IT-CPS. The proposed platform recognizes\nthe distracting activities of the drivers as well as their emotions for mood\nrepair. Further, we develop a prototype of the proposed drive safe platform to\nestablish proof-of-concept (PoC) for the road safety in IT-CPS. In the\ndeveloped driving safety platform, we employ five AI and statistical-based\nmodels to infer a vehicle driver's cognitive-behavioral mining to ensure safe\ndriving during the drive. Especially, capsule network (CN), maximum likelihood\n(ML), convolutional neural network (CNN), Apriori algorithm, and Bayesian\nnetwork (BN) are deployed for driver activity recognition, environmental\nfeature extraction, mood recognition, sequential pattern mining, and content\nrecommendation for affective mood repairment of the driver, respectively.\nBesides, we develop a communication module to interact with the systems in\nIT-CPS asynchronously. Thus, the developed drive safe PoC can guide the vehicle\ndrivers when they are distracted from driving due to the cognitive-behavioral\nfactors. Finally, we have performed a qualitative evaluation to measure the\nusability and effectiveness of the developed drive safe platform. We observe\nthat the P-value is 0.0041 (i.e., < 0.05) in the ANOVA test. Moreover, the\nconfidence interval analysis also shows significant gains in prevalence value\nwhich is around 0.93 for a 95% confidence level. The aforementioned statistical\nresults indicate high reliability in terms of driver's safety and mental state."}, "authors": ["Md. Shirajum Munir", "Sarder Fakhrul Abedin", "Ki Tae Kim", "Do Hyeon Kim", "Md. Golam Rabiul Alam", "Choong Seon Hong"], "author_detail": {"name": "Choong Seon Hong"}, "author": "Choong Seon Hong", "arxiv_comment": "Submitted to IEEE Transactions on Intelligent Transportation Systems,\n  Special Issue on Technologies for risk mitigation and support of impaired\n  drivers", "links": [{"href": "http://arxiv.org/abs/2008.10148v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2008.10148v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.HC", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.HC", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.SY", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "eess.SY", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2008.10148v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2008.10148v1", "journal_reference": null, "doi": null}
{"id": "http://arxiv.org/abs/2012.09058v1", "guidislink": true, "updated": "2020-12-16T16:23:40Z", "updated_parsed": [2020, 12, 16, 16, 23, 40, 2, 351, 0], "published": "2020-12-16T16:23:40Z", "published_parsed": [2020, 12, 16, 16, 23, 40, 2, 351, 0], "title": "Towards Recognizing New Semantic Concepts in New Visual Domains", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=1210&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Towards Recognizing New Semantic Concepts in New Visual Domains"}, "summary": "Deep learning models heavily rely on large scale annotated datasets for\ntraining. Unfortunately, datasets cannot capture the infinite variability of\nthe real world, thus neural networks are inherently limited by the restricted\nvisual and semantic information contained in their training set. In this\nthesis, we argue that it is crucial to design deep architectures that can\noperate in previously unseen visual domains and recognize novel semantic\nconcepts. In the first part of the thesis, we describe different solutions to\nenable deep models to generalize to new visual domains, by transferring\nknowledge from a labeled source domain(s) to a domain (target) where no labeled\ndata are available. We will show how variants of batch-normalization (BN) can\nbe applied to different scenarios, from domain adaptation when source and\ntarget are mixtures of multiple latent domains, to domain generalization,\ncontinuous domain adaptation, and predictive domain adaptation, where\ninformation about the target domain is available only in the form of metadata.\nIn the second part of the thesis, we show how to extend the knowledge of a\npretrained deep model to new semantic concepts, without access to the original\ntraining set. We address the scenarios of sequential multi-task learning, using\ntransformed task-specific binary masks, open-world recognition, with end-to-end\ntraining and enforced clustering, and incremental class learning in semantic\nsegmentation, where we highlight and address the problem of the semantic shift\nof the background class. In the final part, we tackle a more challenging\nproblem: given images of multiple domains and semantic categories (with their\nattributes), how to build a model that recognizes images of unseen concepts in\nunseen domains? We also propose an approach based on domain and semantic mixing\nof inputs and features, which is a first, promising step towards solving this\nproblem.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bn&id_list=&start=1210&max_results=10&sortBy=relevance&sortOrder=descending", "value": "Deep learning models heavily rely on large scale annotated datasets for\ntraining. Unfortunately, datasets cannot capture the infinite variability of\nthe real world, thus neural networks are inherently limited by the restricted\nvisual and semantic information contained in their training set. In this\nthesis, we argue that it is crucial to design deep architectures that can\noperate in previously unseen visual domains and recognize novel semantic\nconcepts. In the first part of the thesis, we describe different solutions to\nenable deep models to generalize to new visual domains, by transferring\nknowledge from a labeled source domain(s) to a domain (target) where no labeled\ndata are available. We will show how variants of batch-normalization (BN) can\nbe applied to different scenarios, from domain adaptation when source and\ntarget are mixtures of multiple latent domains, to domain generalization,\ncontinuous domain adaptation, and predictive domain adaptation, where\ninformation about the target domain is available only in the form of metadata.\nIn the second part of the thesis, we show how to extend the knowledge of a\npretrained deep model to new semantic concepts, without access to the original\ntraining set. We address the scenarios of sequential multi-task learning, using\ntransformed task-specific binary masks, open-world recognition, with end-to-end\ntraining and enforced clustering, and incremental class learning in semantic\nsegmentation, where we highlight and address the problem of the semantic shift\nof the background class. In the final part, we tackle a more challenging\nproblem: given images of multiple domains and semantic categories (with their\nattributes), how to build a model that recognizes images of unseen concepts in\nunseen domains? We also propose an approach based on domain and semantic mixing\nof inputs and features, which is a first, promising step towards solving this\nproblem."}, "authors": ["Massimiliano Mancini"], "author_detail": {"name": "Massimiliano Mancini"}, "author": "Massimiliano Mancini", "arxiv_comment": "Ph.D. thesis. Sapienza University of Rome (2020)", "links": [{"href": "http://arxiv.org/abs/2012.09058v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2012.09058v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/2012.09058v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/2012.09058v1", "journal_reference": null, "doi": null}
