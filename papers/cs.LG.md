
|Title|Category|URL|Author|Published Date|
| :--- | :--- | :--- | :--- | :--- |
|Word level Script Identification from Bangla and Devanagri Handwritten  Texts mixed with Roman Script|cs.LG|http://arxiv.org/abs/1002.4007v1|Ram Sarkar et al.|2010-02-21T19:48:16Z|
|On Stacked Denoising Autoencoder based Pre-training of ANN for Isolated  Handwritten Bengali Numerals Dataset Recognition|cs.LG|http://arxiv.org/abs/1812.05758v1|Al Mehdi Saadat Chowdhury et al.|2018-12-14T02:01:13Z|
|F0 Modeling In Hmm-Based Speech Synthesis System Using Deep Belief  Network|cs.LG|http://arxiv.org/abs/1502.05213v1|Sankar Mukherjee et al.|2015-02-18T13:15:13Z|
|Towards Understanding Regularization in Batch Normalization|cs.LG|http://arxiv.org/abs/1809.00846v4|Ping Luo et al.|2018-09-04T09:01:10Z|
|MimicNorm: Weight Mean and Last BN Layer Mimic the Dynamic of Batch  Normalization|cs.LG|http://arxiv.org/abs/2010.09278v2|Wen Fei et al.|2020-10-19T07:42:41Z|
|Single-bit-per-weight deep convolutional neural networks without  batch-normalization layers for embedded systems|cs.LG|http://arxiv.org/abs/1907.06916v2|Mark D. McDonnell et al.|2019-07-16T09:42:02Z|
|An Internal Covariate Shift Bounding Algorithm for Deep Neural Networks  by Unitizing Layers' Outputs|cs.LG|http://arxiv.org/abs/2001.02814v1|You Huang et al.|2020-01-09T02:35:58Z|
|How Does BN Increase Collapsed Neural Network Filters?|cs.LG|http://arxiv.org/abs/2001.11216v2|Sheng Zhou et al.|2020-01-30T09:00:08Z|
|Diminishing Batch Normalization|cs.LG|http://arxiv.org/abs/1705.08011v2|Yintai Ma et al.|2017-05-22T21:31:10Z|
|Bayesian Network Based Label Correlation Analysis For Multi-label  Classifier Chain|cs.LG|http://arxiv.org/abs/1908.02172v1|Ran Wang et al.|2019-08-06T14:07:18Z|
|Comparing Bayesian Network Classifiers|cs.LG|http://arxiv.org/abs/1301.6684v1|Jie Cheng et al.|2013-01-23T15:57:14Z|
|Generalized Batch Normalization: Towards Accelerating Deep Neural  Networks|cs.LG|http://arxiv.org/abs/1812.03271v1|Xiaoyong Yuan et al.|2018-12-08T06:53:48Z|
|Continual Learning with Extended Kronecker-factored Approximate  Curvature|cs.LG|http://arxiv.org/abs/2004.07507v1|Janghyeon Lee et al.|2020-04-16T07:58:47Z|
|Efficient Conversion of Bayesian Network Learning into Quadratic  Unconstrained Binary Optimization|cs.LG|http://arxiv.org/abs/2006.06926v2|Yuta Shikuri|2020-06-12T03:19:48Z|
|Explicit regularization and implicit bias in deep network classifiers  trained with the square loss|cs.LG|http://arxiv.org/abs/2101.00072v1|Tomaso Poggio et al.|2020-12-31T21:07:56Z|
|BN-invariant sharpness regularizes the training model to better  generalization|cs.LG|http://arxiv.org/abs/2101.02944v1|Mingyang Yi et al.|2021-01-08T10:23:24Z|
|Latent Tree Models and Approximate Inference in Bayesian Networks|cs.LG|http://arxiv.org/abs/1401.3429v1|Yi Wang et al.|2014-01-15T04:46:37Z|
|Understanding Batch Normalization|cs.LG|http://arxiv.org/abs/1806.02375v4|Johan Bjorck et al.|2018-06-01T03:57:56Z|
|Training Faster by Separating Modes of Variation in Batch-normalized  Models|cs.LG|http://arxiv.org/abs/1806.02892v2|Mahdi M. Kalayeh et al.|2018-06-07T20:41:09Z|
|Double Forward Propagation for Memorized Batch Normalization|cs.LG|http://arxiv.org/abs/2010.04947v1|Yong Guo et al.|2020-10-10T08:48:41Z|
|Instance Enhancement Batch Normalization: an Adaptive Regulator of Batch  Noise|cs.LG|http://arxiv.org/abs/1908.04008v2|Senwei Liang et al.|2019-08-12T05:42:09Z|
|A spherical analysis of Adam with Batch Normalization|cs.LG|http://arxiv.org/abs/2006.13382v2|Simon Roburin et al.|2020-06-23T23:29:51Z|
|Optimal Quantization for Batch Normalization in Neural Network  Deployments and Beyond|cs.LG|http://arxiv.org/abs/2008.13128v1|Dachao Lin et al.|2020-08-30T09:33:29Z|
|Momentum^2 Teacher: Momentum Teacher with Momentum Statistics for  Self-Supervised Learning|cs.LG|http://arxiv.org/abs/2101.07525v1|Zeming Li et al.|2021-01-19T09:27:03Z|
|Exponential Moving Average Normalization for Self-supervised and  Semi-supervised Learning|cs.LG|http://arxiv.org/abs/2101.08482v1|Zhaowei Cai et al.|2021-01-21T07:45:37Z|
|Information-theoretic limits of Bayesian network structure learning|cs.LG|http://arxiv.org/abs/1601.07460v4|Asish Ghoshal et al.|2016-01-27T17:41:05Z|
|L1-Norm Batch Normalization for Efficient Training of Deep Neural  Networks|cs.LG|http://arxiv.org/abs/1802.09769v1|Shuang Wu et al.|2018-02-27T08:29:16Z|
|An Exponential Learning Rate Schedule for Deep Learning|cs.LG|http://arxiv.org/abs/1910.07454v3|Zhiyuan Li et al.|2019-10-16T16:22:58Z|
|Learning Successor States and Goal-Dependent Values: A Mathematical  Viewpoint|cs.LG|http://arxiv.org/abs/2101.07123v1|Léonard Blier et al.|2021-01-18T15:33:26Z|
|Batch Group Normalization|cs.LG|http://arxiv.org/abs/2012.02782v2|Xiao-Yun Zhou et al.|2020-12-04T18:57:52Z|
|Mode Normalization|cs.LG|http://arxiv.org/abs/1810.05466v1|Lucas Deecke et al.|2018-10-12T12:10:10Z|
|Stochastic Normalizations as Bayesian Learning|cs.LG|http://arxiv.org/abs/1811.00639v1|Alexander Shekhovtsov et al.|2018-11-01T21:30:39Z|
|Analysis on Gradient Propagation in Batch Normalized Residual Networks|cs.LG|http://arxiv.org/abs/1812.00342v1|Abhishek Panigrahi et al.|2018-12-02T06:41:28Z|
|Supervised Feature Selection for Diagnosis of Coronary Artery Disease  Based on Genetic Algorithm|cs.LG|http://arxiv.org/abs/1305.6046v1|Sidahmed Mokeddem et al.|2013-05-26T18:16:52Z|
|Generative Model for Heterogeneous Inference|cs.LG|http://arxiv.org/abs/1804.09858v1|Honggang Zhou et al.|2018-04-26T02:28:34Z|
|Cognitive Learning of Statistical Primary Patterns via Bayesian Network|cs.LG|http://arxiv.org/abs/1409.7930v5|Weijia Han et al.|2014-09-28T16:36:06Z|
|Batch Normalization Sampling|cs.LG|http://arxiv.org/abs/1810.10962v2|Zhaodong Chen et al.|2018-10-25T16:31:49Z|
|Filter Response Normalization Layer: Eliminating Batch Dependence in the  Training of Deep Neural Networks|cs.LG|http://arxiv.org/abs/1911.09737v2|Saurabh Singh et al.|2019-11-21T20:32:04Z|
|Batch Normalization Increases Adversarial Vulnerability: Disentangling  Usefulness and Robustness of Model Features|cs.LG|http://arxiv.org/abs/2010.03316v1|Philipp Benz et al.|2020-10-07T10:24:33Z|
|On the Number of Samples Needed to Learn the Correct Structure of a  Bayesian Network|cs.LG|http://arxiv.org/abs/1206.6862v1|Or Zuk et al.|2012-06-27T16:28:06Z|
|Estimating Well-Performing Bayesian Networks using Bernoulli Mixtures|cs.LG|http://arxiv.org/abs/1301.2280v1|Geoff A. Jarrad|2013-01-10T16:24:19Z|
|Fast Learning of Relational Dependency Networks|cs.LG|http://arxiv.org/abs/1410.7835v2|Oliver Schulte et al.|2014-10-28T23:14:56Z|
|Riemannian approach to batch normalization|cs.LG|http://arxiv.org/abs/1709.09603v3|Minhyung Cho et al.|2017-09-27T16:18:00Z|
|Understanding the Disharmony between Dropout and Batch Normalization by  Variance Shift|cs.LG|http://arxiv.org/abs/1801.05134v1|Xiang Li et al.|2018-01-16T06:47:59Z|
|A Quantitative Analysis of the Effect of Batch Normalization on Gradient  Descent|cs.LG|http://arxiv.org/abs/1810.00122v2|Yongqiang Cai et al.|2018-09-29T00:50:21Z|
|Bayesian Networks based Hybrid Quantum-Classical Machine Learning  Approach to Elucidate Gene Regulatory Pathways|cs.LG|http://arxiv.org/abs/1901.10557v1|Radhakrishnan Balu et al.|2019-01-23T13:21:08Z|
|Optimize TSK Fuzzy Systems for Classification Problems: Mini-Batch  Gradient Descent with Uniform Regularization and Batch Normalization|cs.LG|http://arxiv.org/abs/1908.00636v3|Yuqi Cui et al.|2019-08-01T21:28:46Z|
|Bayesian network structure learning with causal effects in the presence  of latent variables|cs.LG|http://arxiv.org/abs/2005.14381v2|Kiattikun Chobtham et al.|2020-05-29T04:42:28Z|
|On Resource-Efficient Bayesian Network Classifiers and Deep Neural  Networks|cs.LG|http://arxiv.org/abs/2010.11773v1|Wolfgang Roth et al.|2020-10-22T14:47:55Z|
|A Fast Algorithm for Heart Disease Prediction using Bayesian Network  Model|cs.LG|http://arxiv.org/abs/2012.09429v1|Mistura Muibideen et al.|2020-12-17T07:42:40Z|
|Towards Accelerating Training of Batch Normalization: A Manifold  Perspective|cs.LG|http://arxiv.org/abs/2101.02916v1|Mingyang Yi et al.|2021-01-08T08:53:07Z|
|Effective and Efficient Dropout for Deep Convolutional Neural Networks|cs.LG|http://arxiv.org/abs/1904.03392v5|Shaofeng Cai et al.|2019-04-06T09:17:51Z|
|On Batch Orthogonalization Layers|cs.LG|http://arxiv.org/abs/1812.03049v1|Blanchette et al.|2018-12-07T14:53:03Z|
|On Local Optima in Learning Bayesian Networks|cs.LG|http://arxiv.org/abs/1212.2500v1|Jens D. Nielsen et al.|2012-10-19T15:07:12Z|
|Streaming Normalization: Towards Simpler and More Biologically-plausible  Normalizations for Online and Recurrent Learning|cs.LG|http://arxiv.org/abs/1610.06160v1|Qianli Liao et al.|2016-10-19T19:34:48Z|
|Parallel Implementation of Efficient Search Schemes for the Inference of  Cancer Progression Models|cs.LG|http://arxiv.org/abs/1703.03038v1|Daniele Ramazzotti et al.|2017-03-08T21:29:52Z|
|Learning the structure of Bayesian Networks: A quantitative assessment  of the effect of different algorithmic schemes|cs.LG|http://arxiv.org/abs/1704.08676v2|Stefano Beretta et al.|2017-04-27T17:40:22Z|
|Dynamic Sparse Graph for Efficient Deep Learning|cs.LG|http://arxiv.org/abs/1810.00859v2|Liu Liu et al.|2018-10-01T17:55:43Z|
|Theoretical Analysis of Auto Rate-Tuning by Batch Normalization|cs.LG|http://arxiv.org/abs/1812.03981v1|Sanjeev Arora et al.|2018-12-10T18:58:12Z|
|Evaluating structure learning algorithms with a balanced scoring  function|cs.LG|http://arxiv.org/abs/1905.12666v3|Anthony C. Constantinou|2019-05-29T18:23:17Z|
|Mean Spectral Normalization of Deep Neural Networks for Embedded  Automation|cs.LG|http://arxiv.org/abs/1907.04003v1|Anand Krishnamoorthy Subramanian et al.|2019-07-09T06:24:22Z|
|SNDCNN: Self-normalizing deep CNNs with scaled exponential linear units  for speech recognition|cs.LG|http://arxiv.org/abs/1910.01992v3|Zhen Huang et al.|2019-10-04T15:31:48Z|
|User-Oriented Multi-Task Federated Deep Learning for Mobile Edge  Computing|cs.LG|http://arxiv.org/abs/2007.09236v1|Jed Mills et al.|2020-07-17T21:11:01Z|
|Large-scale empirical validation of Bayesian Network structure learning  algorithms with noisy data|cs.LG|http://arxiv.org/abs/2005.09020v2|Anthony C. Constantinou et al.|2020-05-18T18:40:09Z|
|Efficient and Scalable Structure Learning for Bayesian Networks:  Algorithms and Applications|cs.LG|http://arxiv.org/abs/2012.03540v1|Rong Zhu et al.|2020-12-07T09:11:08Z|
|Ordering-Based Search: A Simple and Effective Algorithm for Learning  Bayesian Networks|cs.LG|http://arxiv.org/abs/1207.1429v1|Marc Teyssier et al.|2012-07-04T16:31:04Z|
|Efficient computational strategies to learn the structure of  probabilistic graphical models of cumulative phenomena|cs.LG|http://arxiv.org/abs/1703.03074v4|Daniele Ramazzotti et al.|2017-03-08T23:50:19Z|
|Scalable Methods for 8-bit Training of Neural Networks|cs.LG|http://arxiv.org/abs/1805.11046v3|Ron Banner et al.|2018-05-25T15:20:37Z|
|Order and Chaos: NTK views on DNN Normalization, Checkerboard and  Boundary Artifacts|cs.LG|http://arxiv.org/abs/1907.05715v2|Arthur Jacot et al.|2019-07-11T10:55:39Z|
|Training High-Performance and Large-Scale Deep Neural Networks with Full  8-bit Integers|cs.LG|http://arxiv.org/abs/1909.02384v2|Yukuan Yang et al.|2019-09-05T13:17:38Z|
|FQ-Conv: Fully Quantized Convolution for Efficient and Accurate  Inference|cs.LG|http://arxiv.org/abs/1912.09356v1|Bram-Ernst Verhoef et al.|2019-12-19T16:39:45Z|
|A Batch Normalized Inference Network Keeps the KL Vanishing Away|cs.LG|http://arxiv.org/abs/2004.12585v2|Qile Zhu et al.|2020-04-27T05:20:01Z|
|Group Whitening: Balancing Learning Efficiency and Representational  Capacity|cs.LG|http://arxiv.org/abs/2009.13333v3|Lei Huang et al.|2020-09-28T14:00:07Z|
|Robust Semi-Supervised Learning with Out of Distribution Data|cs.LG|http://arxiv.org/abs/2010.03658v2|Xujiang Zhao et al.|2020-10-07T21:18:46Z|
|How Important is Weight Symmetry in Backpropagation?|cs.LG|http://arxiv.org/abs/1510.05067v4|Qianli Liao et al.|2015-10-17T03:49:05Z|
|Probabilistic Relational Model Benchmark Generation|cs.LG|http://arxiv.org/abs/1603.00709v1|Mouna Ben Ishak et al.|2016-03-02T13:46:31Z|
|Revisit Fuzzy Neural Network: Demystifying Batch Normalization and ReLU  with Generalized Hamming Network|cs.LG|http://arxiv.org/abs/1710.10328v1|Lixin Fan|2017-10-27T20:48:57Z|
|Normalization of Neural Networks using Analytic Variance Propagation|cs.LG|http://arxiv.org/abs/1803.10560v1|Alexander Shekhovtsov et al.|2018-03-28T12:37:27Z|
|Classification from Positive, Unlabeled and Biased Negative Data|cs.LG|http://arxiv.org/abs/1810.00846v2|Yu-Guan Hsieh et al.|2018-10-01T17:38:58Z|
|Asymmetric Valleys: Beyond Sharp and Flat Local Minima|cs.LG|http://arxiv.org/abs/1902.00744v2|Haowei He et al.|2019-02-02T16:14:52Z|
|Split Batch Normalization: Improving Semi-Supervised Learning under  Domain Shift|cs.LG|http://arxiv.org/abs/1904.03515v1|Michał Zając et al.|2019-04-06T19:10:05Z|
|MixPath: A Unified Approach for One-shot Neural Architecture Search|cs.LG|http://arxiv.org/abs/2001.05887v3|Xiangxiang Chu et al.|2020-01-16T15:24:26Z|
|Efficient improper learning for online logistic regression|cs.LG|http://arxiv.org/abs/2003.08109v3|Rémi Jézéquel et al.|2020-03-18T09:16:14Z|
|Finet: Using Fine-grained Batch Normalization to Train Light-weight  Neural Networks|cs.LG|http://arxiv.org/abs/2005.06828v1|Chunjie Luo et al.|2020-05-14T09:16:13Z|
|New Interpretations of Normalization Methods in Deep Learning|cs.LG|http://arxiv.org/abs/2006.09104v1|Jiacheng Sun et al.|2020-06-16T12:26:13Z|
|Operation-Aware Soft Channel Pruning using Differentiable Masks|cs.LG|http://arxiv.org/abs/2007.03938v2|Minsoo Kang et al.|2020-07-08T07:44:00Z|
|Defending Against Multiple and Unforeseen Adversarial Videos|cs.LG|http://arxiv.org/abs/2009.05244v1|Shao-Yuan Lo et al.|2020-09-11T06:07:14Z|
|The unreasonable effectiveness of Batch-Norm statistics in addressing  catastrophic forgetting across medical institutions|cs.LG|http://arxiv.org/abs/2011.08096v1|Sharut Gupta et al.|2020-11-16T16:57:05Z|
|How to fine-tune deep neural networks in few-shot learning?|cs.LG|http://arxiv.org/abs/2012.00204v1|Peng Peng et al.|2020-12-01T01:20:59Z|
|Improving Unsupervised Domain Adaptation by Reducing Bi-level Feature  Redundancy|cs.LG|http://arxiv.org/abs/2012.15732v1|Mengzhu Wang et al.|2020-12-28T08:00:56Z|
|Improvements to deep convolutional neural networks for LVCSR|cs.LG|http://arxiv.org/abs/1309.1501v3|Tara N. Sainath et al.|2013-09-05T22:06:58Z|
|MEBN-RM: A Mapping between Multi-Entity Bayesian Network and Relational  Model|cs.LG|http://arxiv.org/abs/1806.02455v2|Cheol Young Park et al.|2018-06-06T23:12:02Z|
|Deep Gradient Boosting -- Layer-wise Input Normalization of Neural  Networks|cs.LG|http://arxiv.org/abs/1907.12608v3|Erhan Bilal|2019-07-29T19:24:40Z|
|Learning Bayesian Networks that enable full propagation of evidence|cs.LG|http://arxiv.org/abs/2004.04571v2|Anthony Constantinou|2020-04-09T14:44:11Z|
|LARNN: Linear Attention Recurrent Neural Network|cs.LG|http://arxiv.org/abs/1808.05578v1|Guillaume Chevalier|2018-08-16T16:48:56Z|
